{"posts":[{"title":"MongoDB 入门","text":"MongoDB 入门数据库shell12345678910111213# 查询所有数据库show databases; | show dsb;# 创建/切换到指定的数据库。如果数据库不存在任何集合，使用 `show dbs` 将不会展示该数据库 use [database]; # 创建(如果不存在)/切换到 test 数据库 use test;# 查询当前所在的数据库db# 删除当前数据库db.dropDatabase(); 集合shell1234567891011121314# 查询数据库中的所有集合show collections; | show tables;# 显式创建集合db.createCollection(&quot;[集合名称]&quot;); # 创建一个集合名称为 user 的集合 db.createCollection(&quot;user&quot;); # 通过新增一条文档记录，隐式创建一个集合。比如：往 user 集合中插入一条 name 为 wuhunyu 的文档，将会隐式地创建 user 集合 db.user.insert({name: &quot;wuhunyu&quot;});# 删除一个集合db.[集合].drop(); # 删除一个名称为 user 的集合 db.user.drop(); 文档插入shell12345678910111213141516# 单行插入db.[集合名称].insert([文档]) # 往 user 集合中插入一条 name 为 wuhunyu 的文档 db.user.insert({name: &quot;wuhunyu&quot;});# 批量插入db.[集合名称].insertMany([文档1, 文档2...]); # 批量插入 db.user.insertMany([{_id: 1, name: &quot;1号&quot;}, {name: &quot;2号&quot;, age: 25}, {name: &quot;3号&quot;, bir: &quot;2023-12-16&quot;}]); # 使用 insert 也可以批量插入 db.user.insert([{_id: 1, name: &quot;1号&quot;}, {name: &quot;2号&quot;, age: 25}, {name: &quot;3号&quot;, bir: &quot;2023-12-16&quot;}]); # js 脚本插入for (let i = 0; i &lt; 10; i++) { db.user.insert({_id: i, name: i + &quot;号&quot;, age: i + 20});} 删除shell123456789101112131415# 删除集合全部的文档记录db.[集合].remove({}); # 删除 user 集合中的全部数据 db.user.remove({}); # 删除 user 集合中的 _id 为 1 的文档记录 db.user.remove({_id: 1}); # 如果是 mongo 自动生成的 id，需要使用 ObjectId(&quot;[id]&quot;) 的方式指定 _id db.user.remove({_id: ObjectId(&quot;657d08e9cb5c0571e33ee6b4&quot;)}); # 删除 user 集合中的 name 为 &quot;1号&quot; 的文档记录 db.user.remove({name: &quot;1号&quot;}); # 删除一条记录db.[集合].remove({}, {justOne: true}); # 删除 user 集合中 age 字段为 20 的第一条记录 db.user.remove({age: 20}, {justOne: true}); 修改shell12345678910111213141516171819# 修改# 查询条件 可以类比成 where 条件# 修改值 可以类比成 set# multi 表示是否批量更新，默认为 false，即为只更新找到的第一条记录# upsert 表示查询条件查询不存在记录时，是否新增，默认为 falsedb.[集合].update( {[查询条件]}, {[修改值]}, { multi: &lt;boolean&gt;, upsert: &lt;boolean&gt;, }); # 修改 user 集合中第一条 age 为 20 的文档记录。这种修改方式类似于删除之后再新增，修改之后的字段只会保留 _id 和 age 字段 db.user.update({age: 20}, {age: 25}); # 修改 user 集合中第一条 age 为 20 的文档记录。这种修改方式类似于部分修改，只修改指定的字段 db.user.update({age: 20}, {$set: {age: 25}}); # 修改 user 集合中的所有 age 为 20 的文档记录 db.user.update({age: 20}, {$set: {age: 25}}, {multi: true}); 查询shell1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 查询全部db.[集合].find(); # 查询 user 集合中的全部文档记录 db.user.find(); # 格式化显示，对于较长的文档记录，可以使用 .pretty() 来格式化展示 db.user.find().pretty(); # 操作符$gt &gt;$lt &lt;$gte &gt;=$lte &lt;=$ne !=# 条件查询db.[集合].find({[查询条件]}); # and 条件查询 # 查询 user 集合中，age 等于 25 的记录 db.user.find({age: 25}); # 查询 user 集合中，age 大于等于 25 的记录 db.user.find({age: {$gte: 25}}); # 查询 user 集合中，age 大于等于 25 且 小于等于 30 的记录 db.user.find({age: {$gte: 25, $lte: 30}}); # 错误写法：后一个 age 条件会覆盖掉前一个查询条件，等价于 `db.user.find({age: 30})` db.user.find({age: 25, age: 30}); # or 条件查询 # 查询 user 集合中，age 大于等于 25 或者 name 为 &quot;1号&quot; 的记录 db.user.find({$or: [{age: {$gte: 25}}, {name: &quot;1号&quot;}]}); # and 和 or 的联合查询 # 查询 user 集合中，age大于等于 25 或者 name 为 &quot;1号&quot; 且 age 等于 20 的记录 db.user.find({$or: [{age: {$gte: 25}}, {name: &quot;1号&quot;, age: 20}]});# 数组结构。在 mongo 中，数组结构的查询和和普通的查询也是一样的# 比如，user 集合中一条文档记录有一个字段是 likes# { &quot;_id&quot; : ObjectId(&quot;657d1dfacb5c0571e33ee6c3&quot;), &quot;name&quot; : &quot;test&quot;, &quot;likes&quot; : [ &quot;看电视&quot;, &quot;看书&quot;, &quot;打篮球&quot; ] }# 如果要查询，likes 中包含了 &quot;看书&quot; 的记录db.user.find({likes: &quot;看书&quot;});# 模糊查询。mongo 中并没有像是 `%`, `_` 的标识符，需要通过正则表达式的方式来实现模糊查询# 查询 user 集合中，name 包含 &quot;号&quot; 的记录db.user.find({name: /号/}).pretty();# 排序db.[集合].find().sort({[排序条件]}); # 1: 升序; -1: 降序 # 查询 user 集合中全部的记录，按 age 倒序，name 顺序 排序 db.user.find().sort({age: -1, name: 1}); # 分页# skip 偏移量，从 0 开始# limit 最大条数db.[集合].find().skip([skip]).limit([limit]); # 查询 user 集合全部的记录，只保留前 5 条记录，如果 skip 为 0，可以省略 skip，等价于 `db.user.find()limit(5);` db.user.find().skip(0).limit(5); # 统计数据量db.[集合].count(); # 统计 user 集合的中数据量 db.user.count(); # 统计 user 集合中，age 小于等于 25 的记录数量 db.user.find({age: {$lte: 25}}).count();# 返回指定的字段db.[集合].find([查询条件], [映射字段]); # 查询 user 集合中的 name 字段。其中，1 表示显示，0 表示不显示 db.user.find({}, {name: 1}); $type$type 操作符是基于BSON类型来检索集合中匹配的数据类型，并返回结果 类型 数字 备注 Double 1 String 2 Object 3 Array 4 Binary data 5 Undefined 6 已废弃。 Object id 7 Boolean 8 Date 9 Null 10 Regular Expression 11 JavaScript 13 Symbol 14 JavaScript (with scope) 15 32-bit integer 16 Timestamp 17 64-bit integer 18 Min key 255 Query with -1. Max key 127 shell123# 查询 user 集合中，name 为字符串类型的文档记录，等价于 `db.user.find({name: {$type: 2}});`db.user.find({name: {$type: &quot;string&quot;}});# 值得一提的是，mongo 中的整形的默认类型是 Double 索引shell12345678910111213141516171819# 查询集合下的所有索引db.[集合].getIndexes(); # 查询 user 集合下的所有索引 db.user.getIndexes(); # 创建一个缩影db.[索引].craeteIndex([索引字段], [索引配置]); # 创建一个名称为 idx_name_age 的索引，索引字段为 (name, age), 都为升序排列 db.user.createIndex({name: 1, age: 1}, {name: &quot;idx_name_age&quot;}); # 删除集合下的全部索引db.[集合].dropIndexes(); # 删除 user 集合下的所有索引(除了 id 索引) db.user.dropIndexes();# 删除指定索引db.[集合].dropIndex([索引名称]); # 删除 user 集合的 idx_name_age 索引 db.user.dropIndex(&quot;idx_name_age&quot;); 聚合管道 $project：修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。 $match：用于过滤数据，只输出符合条件的文档。$match使用MongoDB的标准查询操作。 $limit：用来限制MongoDB聚合管道返回的文档数。 $skip：在聚合管道中跳过指定数量的文档，并返回余下的文档。 $unwind：将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。 $group：将集合中的文档分组，可用于统计结果。 $sort：将输入文档排序后输出。 $geoNear：输出接近某一地理位置的有序文档。 聚合函数 表达式 描述 $sum 计算总和。 $avg 计算平均值 $min 获取集合中所有文档对应值得最小值。 $max 获取集合中所有文档对应值得最大值。 $push 将值加入一个数组中，不会判断是否有重复的值。 $addToSet 将值加入一个数组中，会判断是否有重复的值，若相同的值在数组中已经存在了，则不加入。 $first 根据资源文档的排序获取第一个文档数据。 $last 根据资源文档的排序获取最后一个文档数据 测试数据shell1234567891011121314151617181920212223242526db.mycol.insert([ { title: 'MongoDB Overview', description: 'MongoDB is no sql database', by_user: 'runoob.com', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 100 }, { title: 'NoSQL Overview', description: 'No sql database is very fast', by_user: 'runoob.com', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 10 }, { title: 'Neo4j Overview', description: 'Neo4j is no sql database', by_user: 'Neo4j', url: 'http://www.neo4j.com', tags: ['neo4j', 'database', 'NoSQL'], likes: 750 }]); 示例操作shell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149# 管道操作# $project 映射 title 和 by_user 字段db.mycol.aggregate([ { $project: { _id: 0, title: 1, by_user: 1 } }]);# $match 过滤 likes &gt;= 100, tags 包含 mongo 的记录db.mycol.aggregate([ { $match: { likes: { $gte: 100 }, tags: &quot;mongodb&quot; } }, { $project: { _id: 0, title: 1, by_user: 1 } }]);# 分页db.mycol.aggregate([ { $match: { likes: { $gte: 100 }, tags: &quot;mongodb&quot; } }, { $project: { _id: 0, title: 1, by_user: 1 } }, { $skip: 0 }, { $limit: 10 }]);# $unwind 数组拆分db.mycol.aggregate([ { $match: { likes: { $gte: 100 }, tags: &quot;mongodb&quot; } }, { $unwind: &quot;$tags&quot; }, { $project: { _id: 0, title: 1, by_user: 1, tags: 1 } }, { $skip: 0 }, { $limit: 10 }]);# $sort 排序db.mycol.aggregate([ { $match: { likes: { $gte: 100 }, tags: &quot;mongodb&quot; } }, { $unwind: &quot;$tags&quot; }, { $project: { _id: 0, title: 1, by_user: 1, tags: 1 } }, { $sort: { title: 1, tags: 1 } }, { $skip: 0 }, { $limit: 10 }]);# $group 分组db.mycol.aggregate([ { $match: { likes: { $gte: 100 }, tags: &quot;database&quot; } }, { $group: { _id: { author: &quot;$by_user&quot;, url: &quot;$url&quot; }, minLikes: { $min: &quot;$likes&quot; }, maxLikes: { $max: &quot;$likes&quot; }, avgLikes: { $avg: &quot;$likes&quot; }, count: { $sum: 1 }, sum: { $sum: &quot;$likes&quot; } } }]).pretty();","link":"/mongodb/MongoDB%20%E5%85%A5%E9%97%A8/index.html"},{"title":"GitHub Actions 简单使用","text":"GitHub Actions 是什么官方文档 是这么介绍的 在 GitHub Actions 的仓库中自动化、自定义和执行软件开发工作流程。 您可以发现、创建和共享操作以执行您喜欢的任何作业（包括 CI/CD），并将操作合并到完全自定义的工作流程中。 我的理解可以理解为一个有条件（比如push事件，也可以是 cron 表达式定义的一个定时任务）触发的工作流任务。而这个工作流任务是可以由我们开发者自定义的，执行这些任务的环境由 GitHub 提供，一般是 ubuntu 环境。通过这个环境，我们可以执行一些自定义脚本，亦或是调用其他开发者发布的插件来实现一些开发者想要的功能 GitHub Actions 可以做什么传统 CI/CD 能做的编译，打包，发布，使用 GitHub Actions 肯定也是能实现的 这里，我想使用两个我自己的案例来演示说明，我使用 GitHub Actions 做了些什么 个人博客编译，打包，发布 作为拉取 docker-hub 官方镜像的中转工具 前置准备工作如何自定义工作流任务 只需要在仓库存在 .github/workflows/*.yml，在满足触发条件之后，就会被自动执行 这个配置文件可以有多个，执行的时候是并行的，但也可以定义工作流的依赖关系实现串行化 关于执行环境 可以自定义选择 Linux，Windows 和 macOS，具体的版本可以访问这里查看 Linux 环境已经自带了 Docker 环境，无需重复安装 关于插件 GitHub Actions 的插件分为官方和非官方的，其中官方的插件地址都是 https://github.com/actions 开头的 比如检出仓库代码的插件 checkout 比如第三方部署到 GitHub Pages 的插件 peaceiris/actions-gh-pages 可以在 GitHub 市场 中寻找符合自己需求的插件 个人博客编译，打包，发布.github/workflows/deploy.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172name: Deploy My Hexo Blogon: push: branches: - mainjobs: build: runs-on: ubuntu-latest # 权限控制 permissions: contents: read pages: write id-token: write steps: # 检出仓库代码 - name: Checkout Repository uses: actions/checkout@v4 # 设置 Node.js 版本 - name: Setup Node.js uses: actions/setup-node@v4 with: node-version: '20.9.0' # 缓存 npm 依赖 - name: Cache npm dependencies uses: actions/cache@v2 with: path: ~/.npm key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}-v2 restore-keys: | ${{ runner.os }}-node- # 安装 Hexo CLI - name: Install Hexo CLI run: npm install hexo-cli -g # 安装 npm 依赖 - name: Install npm dependencies run: | npm install --prefix themes/icarus npm install # 生成静态文件 - name: Generate static files run: | hexo clean hexo g # 部署到云服务器 - name: Deploy to Cloud Server uses: easingthemes/ssh-deploy@main with: SSH_PRIVATE_KEY: ${{ secrets.PRIVATE_KEY }} ARGS: &quot;-avz --delete&quot; SOURCE: ${{ secrets.SOURCE }} REMOTE_HOST: ${{ secrets.HOST }} REMOTE_PORT: ${{ secrets.PORT }} REMOTE_USER: ${{ secrets.USER }} TARGET: ${{ secrets.TARGET }} # 部署到 wuhunyu.github.io - name: Deploy to Github Pages uses: peaceiris/actions-gh-pages@v4 with: personal_token: ${{ secrets.ACCESS_TOKEN }} external_repository: ${{ secrets.PAGE_REPO }} publish_branch: ${{ secrets.PAGE_BRAN }} publish_dir: ${{ secrets.SOURCE }} commit_message: ${{ github.event.head_commit.message }} 简单说明一下这个 actions 的内容 actions 的名称是 Deploy My Hexo Blog 当仓库的 main 分支有 push 事件发生时自动触发 工作流的执行环境是 ubuntu-latest 其次是关于权限控制的说明 最后按 steps 的声明顺序，任务会被依次执行，如果中途出现了错误，则会直接中断后续的任务。每个任务的作用在注释中已经写了，就不再多说明了。值得注意的是，其中有 ${{ }} 的符号，这是从当前上下文取值的占位符，其中 secrets 是由开发者自定义的环境变量，具体可以在每个仓库的 settings/secrets/actions 进行设置 action 运行的控制台像这样，按执行顺序列出了每个任务的日志 作为拉取 docker-hub 官方镜像的中转工具 前些天在掘金上看到了有人已经实现了，功能要比我这个更加健壮，地址如下 docker_image_pusher 下面展示的这个工具是我个人使用，每次拉取镜像也不会太大，像是磁盘不够的情况暂时还不需要考虑 .github/workflows/docker-hub.yml 123456789101112131415161718192021222324252627282930313233name: Pull and Push Docker Imageson: push: branches: - mainjobs: deploy: runs-on: ubuntu-latest steps: # 检出仓库代码 - name: Checkout Repository uses: actions/checkout@v4 # 安装 jq 以处理 JSON 文件 - name: Install jq run: sudo apt-get install -y jq # 登录到阿里云容器镜像服务 - name: Log in to Alibaba Cloud Registry run: echo &quot;${{ secrets.ALIYUN_REGISTRY_PASSWORD }}&quot; | docker login -u &quot;${{ secrets.ALIYUN_REGISTRY_USERNAME }}&quot; registry.cn-guangzhou.aliyuncs.com --password-stdin # 拉取和推送 Docker 镜像 - name: Pull and Push Docker images env: ALIYUN_NAMESPACE: ${{ secrets.ALIYUN_REGISTRY_NAMESPACE }} run: | for image in $(jq -r '.[]' images.json); do echo &quot;Processing image: ${image}&quot; docker pull ${image} aliyun_image=&quot;registry.cn-guangzhou.aliyuncs.com/${{ env.ALIYUN_NAMESPACE }}/${image##*/}&quot; docker tag ${image} $aliyun_image docker push ${aliyun_image} done 同样也是当仓库的 main 分支有 push 事件发生时自动触发 之后登陆阿里云的 docker hub 镜像 读取仓库中的 images.json 配置文件，形如 1234[ &quot;redis&quot;, &quot;nginx:latest&quot;] 在容器内 pull 镜像之后再 push 给阿里云的 docker hub 这里利用 actions 的容器可访问外网的能力将 docker 镜像中转到内网可访问的镜像站 其他除了以上两个案例，GitHub 有一个叫 Releases 的概念，也可以借由 GitHub Actions 来打包 .github/workflows/buildAndRelease.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344name: Build And Releaseon: push: tags: - 'v*.*.*'jobs: build: runs-on: ubuntu-latest steps: - name: Checkout Code uses: actions/checkout@v4 - name: Set Up JDK 21 uses: actions/setup-java@v4 with: java-version: '21' distribution: 'adopt' - name: Build with Maven run: mvn clean package -DskipTests - name: Create Release id: create_release uses: actions/create-release@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: tag_name: ${{ github.ref }} release_name: Release ${{ github.ref }} draft: false prerelease: false - name: Upload JAR to Release uses: actions/upload-release-asset@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: upload_url: ${{ steps.create_release.outputs.upload_url }} asset_path: ${{ secrets.ASSET_PATH }} asset_name: ${{ secrets.ASSET_NAME }} asset_content_type: application/java-archive Releases 基于 tag，所以需要 actions 的触发条件可以设置为 tag 被 push 时 效果如下 总结 GitHub Actions 是 GitHub 免费开放给开发者的自动化工作流 基于 GitHub Actions，可以解放部分项目本地编译，部署的服务器资源。我借用这个功能实现了几个小功能，包括个人博客的编译部署，docker hub 镜像中转工具 最后如果借用 GitHub Actions 的容器资源来做爬虫节点是不是也挺好，官方给的服务器资源配置也还不错，可以查看这个 引用资源 GitHub Actions 文档 用于公共存储库的 GitHub 托管的标准运行器 checkout peaceiris/actions-gh-pages GitHub 市场 docker-hub docker_image_pusher starter","link":"/tools/github-actions-example/index.html"},{"title":"MyBatis 缓存","text":"MyBatis 缓存MyBatis 有两级缓存，其中一级缓存默认开启，二级缓存需要手动开启。不过 MyBatis 的缓存很容易失效，所以建议保持默认配置只开启一级缓存即可。本文将以案例的形式来介绍 MyBatis 的缓存 本文所使用的例子完整示例存放在 GitHub 中 MyBatis 的缓存分为 一级缓存 和 二级缓存一级缓存一级缓存 默认开启，同一个 SqlSession 对象共享一个缓存 失效场景 没有使用同一个 SqlSession 发生了任意修改类操作 SqlSession commit 或者手动调用了 SqlSession 的 clearCache() 方法 localCacheScope 属性配置为 STATEMENT 二级缓存二级缓存需要手动开启，同一个 namespace 共享一个缓存，也可以使用配置的方法使得多个 namespace 共享一个缓存 失效场景 发生了任意修改类操作，会清空所有的二级缓存。当然，一级缓存也会被清空 如果使用 MyBatisPlus 默认提供的查询语句，没有在 Mapper 接口上配置 @CacheNamespaceRef 也会导致二级缓存不生效 SqlSession commit 或者手动调用了 SqlSession 的 clearCache() 方法 mapper 映射文件没有配置 cache 标签 一级缓存示例项目工程 MyBatis 核心配置 xml12345678910&lt;settings&gt; &lt;!-- 二级缓存关闭 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;false&quot;/&gt; &lt;!-- 一级缓存配置为 session 级别，简单理解就是开启一级缓存 --&gt; &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt; &lt;!-- 开启下划线转驼峰 --&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt; &lt;!-- 日志实现 --&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;SLF4J&quot;/&gt;&lt;/settings&gt; logback 日志配置 xml12345678910111213141516171819202122232425262728&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;configuration scan=&quot;false&quot;&gt; &lt;contextName&gt;mybatis&lt;/contextName&gt; &lt;appender name=&quot;CONSOLE&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;Pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%logger{50}] - %msg%n&lt;/Pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name=&quot;org.springframework&quot; level=&quot;WARN&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt; &lt;/logger&gt; &lt;logger name=&quot;org.mybatis&quot; level=&quot;DEBUG&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt; &lt;/logger&gt; &lt;!-- mapper 层开启 debug 日志 --&gt; &lt;logger name=&quot;top.wuhunyu.mybatis.cache.mapper&quot; level=&quot;DEBUG&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt; &lt;/logger&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;CONSOLE&quot;/&gt; &lt;/root&gt; &lt;/configuration&gt; User 实体 Java1234567891011121314151617181920212223@Data@NoArgsConstructor@AllArgsConstructorpublic class User implements Serializable { private static final long serialVersionUID = -2716722547404828511L; /** * 用户id */ private Long id; /** * 用户名称 */ private String userName; /** * 年龄 */ private Integer age;} UserMapper Java12345678910111213141516171819public interface UserMapper { /** * 根据用户id查询用户信息 * * @param id 用户id 非空 * @return 用户实体对象 */ User findUserById(@Param(&quot;id&quot;) Long id); /** * 根据用户id修改用户信息 * * @param user 用户实体对象 */ void updateUserById(@Param(&quot;user&quot;) User user); } UserMapper.xml 映射文件 xml1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;top.wuhunyu.mybatis.cache.mapper.UserMapper&quot;&gt; &lt;select id=&quot;findUserById&quot; resultType=&quot;top.wuhunyu.mybatis.cache.domain.User&quot;&gt; select u.id, u.user_name, u.age from `user` u where u.id = #{id} &lt;/select&gt; &lt;update id=&quot;updateUserById&quot;&gt; update `user` set user_name = #{user.userName}, age = #{user.age} where id = #{user.id} &lt;/update&gt;&lt;/mapper&gt; 一级缓存生效演示同一个 SqlSession 同一个 Mapper 对象分别查询同一条 sql 时 Java123456789101112try ( // 关闭自动提交事务 SqlSession sqlSession = sqlSessionFactory.openSession(false);) { UserMapper userMapper = sqlSession.getMapper(UserMapper.class); User user1 = userMapper.findUserById(1L); log.info(&quot;user1: {}&quot;, user1); User user2 = userMapper.findUserById(1L); log.info(&quot;user2: {}&quot;, user2); sqlSession.commit(); Assert.assertSame(user1, user2);} 日志打印情况如下 图中只打印了一次 sql，表示第二次查询并没有真的发起 sql 查询，而是从一级缓存中直接获取。由于一级缓存可以理解为一个 Map，并不存在序列化操作，因此直接比较 user1 和 user2 的内存地址也是相同的 同一个 SqlSession 不同 Mapper 对象分别查询同一条 sql 时 Java12345678910111213try ( // 关闭自动提交事务 SqlSession sqlSession = sqlSessionFactory.openSession(false);) { UserMapper userMapper1 = sqlSession.getMapper(UserMapper.class); UserMapper userMapper2 = sqlSession.getMapper(UserMapper.class); User user1 = userMapper1.findUserById(1L); log.info(&quot;user1: {}&quot;, user1); User user2 = userMapper2.findUserById(1L); log.info(&quot;user2: {}&quot;, user2); sqlSession.commit(); Assert.assertSame(user1, user2);} 日志打印情况如下 图中只打印了一次 sql，表示一级缓存生效 一级缓存失效演示没有使用同一个 SqlSession Java123456789101112131415try ( // 关闭自动提交事务 SqlSession sqlSession1 = sqlSessionFactory.openSession(false); SqlSession sqlSession2 = sqlSessionFactory.openSession(false);) { UserMapper userMapper1 = sqlSession1.getMapper(UserMapper.class); UserMapper userMapper2 = sqlSession2.getMapper(UserMapper.class); User user1 = userMapper1.findUserById(1L); log.info(&quot;user1: {}&quot;, user1); sqlSession1.commit(); User user2 = userMapper2.findUserById(1L); log.info(&quot;user2: {}&quot;, user2); sqlSession2.commit(); Assert.assertNotSame(user1, user2);} 日志打印情况如下 图中分别打印了两次 sql，user1 和 user2 的内存地址也不一样了，表示一级缓存失效了 发生了任意修改类操作 Java12345678910111213try ( // 关闭自动提交事务 SqlSession sqlSession = sqlSessionFactory.openSession(false);) { UserMapper userMapper = sqlSession.getMapper(UserMapper.class); User user1 = userMapper.findUserById(1L); log.info(&quot;user1: {}&quot;, user1); userMapper.updateUserById(new User(2L, &quot;李四-2&quot;, 21)); User user2 = userMapper.findUserById(1L); log.info(&quot;user2: {}&quot;, user2); sqlSession.commit(); Assert.assertNotSame(user1, user2);} 日志打印情况如下 同样打印了两次查询 sql，这是由于 updateUserById 发生了修改操作导致的缓存失效 SqlSession commit 或者手动调用了 SqlSession 的 clearCache() 方法 Java12345678910111213try ( // 关闭自动提交事务 SqlSession sqlSession = sqlSessionFactory.openSession(false);) { UserMapper userMapper = sqlSession.getMapper(UserMapper.class); User user1 = userMapper.findUserById(1L); log.info(&quot;user1: {}&quot;, user1); sqlSession.clearCache(); User user2 = userMapper.findUserById(1L); log.info(&quot;user2: {}&quot;, user2); sqlSession.commit(); Assert.assertNotSame(user1, user2);} 日志打印情况如下 由于手动触发了 clearCache，导致一级缓存失效了 localCacheScope 属性配置为 STATEMENT 修改 localCacheScope 的值为 STATEMENT 拿之前一级缓存生效的例子 Java123456789101112try ( // 关闭自动提交事务 SqlSession sqlSession = sqlSessionFactory.openSession(false);) { UserMapper userMapper = sqlSession.getMapper(UserMapper.class); User user1 = userMapper.findUserById(1L); log.info(&quot;user1: {}&quot;, user1); User user2 = userMapper.findUserById(1L); log.info(&quot;user2: {}&quot;, user2); sqlSession.commit(); Assert.assertNotSame(user1, user2);} 日志打印情况如下 localCacheScope 为 STATEMENT 时，表示缓存作用于 STATEMENT，而一个 STATEMENT 就是一条 sql，也就表示一级缓存失效了 二级缓存示例项目工程 application.yml 配置 yaml1234567891011121314151617181920212223242526spring: application: name: mybatis-plus datasource: druid: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/test username: root password: 123456server: port: 10000# mybatis-plus 配置mybatis-plus: configuration: # 开启下划线转驼峰 map-underscore-to-camel-case: true # 日志实现 log-impl: org.apache.ibatis.logging.slf4j.Slf4jImpl # 二级缓存开启 cache-enabled: true # 一级缓存配置为 session 级别，简单理解就是开启一级缓存 local-cache-scope: session # mapper.xml 位置 mapper-locations: classpath*:/top/wuhunyu.mybatis/plus/cache/mapper/**/*Mapper.xml 主启动类配置 mapper 扫描路径 切换成了 MybatisPlus 之后，先看看一级缓存是否还生效 一级缓存生效演示 同一个 SqlSession，使用 Mybatis 原生 sql 写法 Java1234User user1 = userMapper.findUserById(1L);log.info(&quot;user1: {}&quot;, user1);User user2 = userMapper.findUserById(1L);log.info(&quot;user2: {}&quot;, user2); 日志打印情况如下 可以看到，查询的 sql 语句只被执行了一次，日志还打印了缓存被击中的概率为 0.5，说明第二次查询确实从缓存中获取成功了 另外，日志有一条 WARN。这是由于二级缓存的结果会序列化保存的缘故 通过 debug 可以观察到两个 user 对象的 hash 值并不相同，表示不是同一个对象，但确实走了二级缓存，这是由于二级缓存反序列导致的 这条 WARN 日志就是针对序列化可能产生 bug 而打印的警告，详情可以通过日志给出的链接自行了解 此处以及之后的例子都不对这个 WARN 日志进行处理 同一个 SqlSession，使用 MybatisPlus api 的写法在 UserMapper 上加上 @CacheNamespaceRef(UserMapper.class) 注解 Java12345678910111213141516171819@CacheNamespaceRef(UserMapper.class)public interface UserMapper extends BaseMapper&lt;User&gt; { /** * 根据用户id查询用户信息 * * @param id 用户id 非空 * @return 用户实体对象 */ User findUserById(@Param(&quot;id&quot;) Long id); /** * 根据用户id修改用户信息 * * @param user 用户实体对象 */ void updateUserById(@Param(&quot;user&quot;) User user);} 测试用例如下 Java1234User user1 = userMapper.selectById(1L);log.info(&quot;user1: {}&quot;, user1);User user2 = userMapper.selectById(1L);log.info(&quot;user2: {}&quot;, user2); 日志打印情况如下 能够观察到走了二级缓存 不同 SqlSession，使用 MybatisPlus api 的写法 Java12345678910User user1 = userMapper.selectById(1L);log.info(&quot;user1: {}&quot;, user1);ExecutorService executor = Executors.newSingleThreadExecutor();CountDownLatch countDownLatch = new CountDownLatch(1);executor.execute(() -&gt; { User user2 = userMapper.selectById(1L); log.info(&quot;user2: {}&quot;, user2); countDownLatch.countDown();});countDownLatch.await(); 日志打印情况如下 也是没有问题的 二级缓存失效演示 发生了任意修改类操作 Java12345678910111213User user1 = userMapper.selectById(1L);log.info(&quot;user1: {}&quot;, user1);userMapper.updateUserById(new User(2L, &quot;张三&quot;, 21));ExecutorService executor = Executors.newSingleThreadExecutor();CountDownLatch countDownLatch = new CountDownLatch(1);AtomicReference&lt;User&gt; userAtomicReference = new AtomicReference&lt;&gt;();executor.execute(() -&gt; { User user2 = userMapper.selectById(1L); log.info(&quot;user2: {}&quot;, user2); userAtomicReference.set(user2); countDownLatch.countDown();});countDownLatch.await(); 日志打印情况如下 可以看到二级缓存的集中率都是 0.0，表示二级缓存没有被击中 如果使用 MyBatisPlus 默认提供的查询语句，没有在 Mapper 接口上配置 @CacheNamespaceRef Java1234User user1 = userMapper.selectById(1L);log.info(&quot;user1: {}&quot;, user1);User user2 = userMapper.selectById(1L);log.info(&quot;user2: {}&quot;, user2); 日志打印情况如下 发生了两次 sql 查询，说明缓存失效 mapper 映射文件没有配置 cache 标签 Java1234User user1 = userMapper.findUserById(1L);log.info(&quot;user1: {}&quot;, user1);User user2 = userMapper.findUserById(1L);log.info(&quot;user2: {}&quot;, user2); 日志打印情况如下","link":"/mybatis/mybatis-cache/index.html"},{"title":"JetBrains 全家桶破解","text":"前排说明 破解原理基于 Gitee 上的一个开源插件 ja-netfilter，博主写这篇文章的目的并非倡导破解，该插件博主并未参与开发，仅仅作为一个使用者 截止到 2024年04月06日，博主目前使用的版本如下，实测有效可用，但并非是最新版本，至于其他版本是否支持，博主并未测试 ide 使用版本 IntelliJ IDEA 2023.3.6 WebStorm 2023.2.6 GoLand 2023.2.6 DataGrip 2023.2.3 破解需要 ja-netfilter 插件，以及当前还可用的激活码(需要激活第一次，以后便不需要了) 插件作者的使用教程可以参考这篇文章，本篇文章是基于插件作者的文章以及实际安装过程写给博主自己用的 文中仅仅介绍如何破解 JetBrains 家族的软件 演示环境基于博主的 MacOS 操作步骤 下载破解插件 ja-netfilter，文件的名称是 ja-netfilter-2022.2.0.zip 解压后的文件结构如下，ja-netfilter.jar所在的磁盘路径需要记下来（比如在我的电脑上是 /Users/wuhunyu/plugins/ja-netfilter/ja-netfilter.jar） 123456789101112131415.├── README.pdf├── config│ ├── dns.conf│ ├── native.conf│ ├── power.conf│ └── url.conf├── ja-netfilter.jar├── plugins│ ├── dns.jar│ ├── hideme.jar│ ├── native.jar│ ├── power.jar│ └── url.jar└── sha1sum.txt 修改 config/dns.conf 配置文件，替换成以下内容即可 12[DNS]EQUAL,jetbrains.com 修改 config/url.conf 配置文件，替换成以下内容即可 12[URL]PREFIX,https://account.jetbrains.com/lservice/rpc/validateKey.action 修改 JetBrains 各个 ide 的配置文件，配置文件的后缀都是 .vmoptions ide 对应的 vmoptions 配置文件名称 可能存在的路径 IntelliJ IDEA idea.vmoptions /Users/[用户名]/Library/Application Support/JetBrains/IntelliJIdea[具体版本号]/idea.vmoptions WebStorm webstorm.vmoptions /Users/[用户名]/Library/Application Support/JetBrains/WebStorm[具体版本号]/webstorm.vmoptions GoLand goland.vmoptions /Users/[用户名]/Library/Application Support/JetBrains/GoLand[具体版本号]/goland.vmoptions DataGrip datagrip.vmoptions /Users/[用户名]/Library/Application Support/JetBrains/DataGrip[具体版本号]/datagrip.vmoptions 其他方式：又或者此刻你还能打开 ide 正常使用，通过在 ide 中找到 Help -&gt; Edit Custom VM Options 也能在 ide 中打开此配置文件 在 .vmoptions 中加入以下配置 1234# 激活-javaagent:[ja-netfilter.jar 所在的绝对路径]--add-opens=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED--add-opens=java.base/jdk.internal.org.objectweb.asm.tree=ALL-UNNAMED 以我的电脑上ja-netfilter.jar 所在的绝对路径是 /Users/wuhunyu/plugins/ja-netfilter/ja-netfilter.jar 为例，最后需要添加的配置如下 1234# 激活-javaagent:/Users/wuhunyu/plugins/ja-netfilter/ja-netfilter.jar--add-opens=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED--add-opens=java.base/jdk.internal.org.objectweb.asm.tree=ALL-UNNAMED 启动 ide，输入 Activation code 激活，后续即便现在所使用的激活码失效，也不需要再次激活 关于激活码，ja-netfilter 插件作者的文章中提到了热心大佬的key。博主自己试过好几个倒是都无效 博主目前的解决方案是在taobao购买了一个激活码用于第一次激活","link":"/tools/jetbrains-active/index.html"},{"title":"搭建自己的 docker-hub 仓库","text":"前言之前写了一篇通过 GitHub Actions 的容器作为中转拉取 docker-hub 博客，不过使用的还是阿里云的镜像仓库 GitHub 上有推荐自建仓库的开源项目，比如 registry-mirror 网上虽然有不少活雷锋把自己的仓库开源出来，不过不少是限速的，拉取小镜像还好，一旦需要拉取像是 elasticsearch 这样大的镜像，又可能会因为拉取速度太慢导致多次重试，最后失败 这次，我也来试试自建仓库。自建的 docker 仓库，都是基于官方的一个镜像 registry 下面简单说说自建仓库的过程 资源准备 域名和证书 这个倒不是必须的，但 Docker 默认不允许非 HTTPS 方式推送镜像的 镜像 httpd 这里选择的是 httpd:alpine，这个镜像主要用来生成 http 认证文件 镜像 registry 这里选择的是 registry:latest，搭建自己 docker 仓库就靠它了 编写配置http 认证文件12345# 把命令中的 username 和 password 换成自己的用户名称和密码即可，后续 docker login 时需要通过这个凭证来登陆自己搭建好的 docker 仓库&gt; docker run --rm \\ --entrypoint htpasswd \\ httpd:alpine \\ -Bbn username password &gt; auth/nginx.htpasswd config.yml这个配置文件是 registry 的配置文件，默认位置在 /etc/docker/registry/config.yml 另外，值得注意的是，保存在仓库中的镜像默认存放在 /var/lib/registry 目录下 此外，http.tls 还配置了公私钥，比如我的域名是 docker.wuhunyu.top 以下是示例配置 1234567891011121314151617181920212223242526272829303132333435version: 0.1log: accesslog: disabled: true level: warn formatter: text fields: service: registry environment: stagingstorage: delete: enabled: true cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registryauth: htpasswd: realm: basic-realm path: /etc/docker/registry/auth/nginx.htpasswdhttp: addr: :443 host: https://docker.wuhunyu.top headers: X-Content-Type-Options: [nosniff] http2: disabled: false tls: certificate: /etc/docker/registry/ssl/docker.wuhunyu.top.pem key: /etc/docker/registry/ssl/docker.wuhunyu.top.keyhealth: storagedriver: enabled: true interval: 10sthreshold: 3 docker-compose 配置脚本123456789101112131415161718version: '3'services: registry: container_name: docker-registry image: registry:latest restart: always ports: - &quot;443:443&quot; volumes: - /home/wuhunyu/registry/config.yml:/etc/docker/registry/config.yml:ro - /home/wuhunyu/registry/repository:/var/lib/registry - /home/wuhunyu/registry/auth/nginx.htpasswd:/etc/docker/registry/auth/nginx.htpasswd:ro - /home/wuhunyu/cert/docker.wuhunyu.top.pem:/etc/docker/registry/ssl/docker.wuhunyu.top.pem:ro - /home/wuhunyu/cert/docker.wuhunyu.top.key:/etc/docker/registry/ssl/docker.wuhunyu.top.key:rovolumes: docker-registry: 分别指定了 registry 的配置文件，暴露仓库镜像数据卷，指定 http 认证文件以及公私钥文件 配置完毕之后，我的目录结构如下 123456registry├── auth│ └── nginx.htpasswd├── config.yml├── docker-registry.yml└── repository 启动通过 docker-compose -f docker-registry.yml up -d 完成自建仓库的部署 1234[+] Building 0.0s (0/0) docker:default[+] Running 2/2 ✔ Network registry_default Created 0.1s ✔ Container docker-registry Started 0.0s 通过 docker ps | grep docker-registry 可以观察到容器是否正常运行 13e3d14457946 registry:latest &quot;/entrypoint.sh /etc…&quot; 2 minutes ago Up 2 minutes 5000/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp docker-registry 登录仓库1&gt; docker login docker.wuhunyu.top 对需要上传的镜像打标签12# 这里以 openjdk:21-jdk-slim 为例&gt; docker tag openjdk:21-jdk-slim docker.wuhunyu.top/openjdk:21-jdk-slim 推送镜像到自建仓库12# 这里以 openjdk:21-jdk-slim 为例&gt; docker push docker.wuhunyu.top/openjdk:21-jdk-slim 退出登录1&gt; docker logout docker.wuhunyu.top 最后自建的仓库服务我放在自己的云服务器上，最所周知，带宽资源才是最昂贵的，阿里云对流入的流量我在上传镜像的时候感觉还行，上传近 1G 的镜像，能在一分钟内跑完，到了拉取镜像就开始不好了，1G 的镜像拉取了 5 分钟都还没结束，这个和我的带宽有关，我的带宽只有 3 Mbps 最后总结为自建的 docker 仓库还不好用，主要是我不想为网络资源额外付费 引用资源 registry-mirror 私有仓库 私有仓库高级配置","link":"/tools/make-self-docker-hub/index.html"},{"title":"自组折叠自行车","text":"前言前几个月对折叠自行车比较感兴趣,通过视频,网络好友,配件卖家等渠道了解到了一些自组折叠自行车的相关知识 写一篇博客记录分享一下 这部分知识和组装折叠自行车强相关 知识比较偏向于理论,并没有实践来验证 强烈建议想要了解自组折叠车的朋友转到附表-配件简图先认识一下各个配件的样子 什么是折叠自行车折叠自行车肯定是要能折叠的,不过也因为折叠属性,我目前没见过大轮径的折叠车 折叠也分多种,两折叠比较常见,一般在大梁的位置可以折叠一次;也有三折叠,在一些小众名牌中 只要能折叠,是自行车.我认为它被称作折叠自行车就是没有毛病的 下面我着重介绍的折叠自行车是指带变速的类型 以下两张图并非我的组装成果,来自山东某位网友 术语碳纤维碳纤维是一种新工艺,其主要的成份是碳材料.制作好的成品一般具有不错的滤震能力,且重量相比于铝制来说也要轻不少,刚性也不错 碳纤维在自行车领域在我看来是一种高级材料,你可以将车身的许多配件都找到它的制品.比如碳车架,碳前叉,碳车把,碳坐管,碳轮组,甚至碳坐垫,碳脚踏和碳盘片 碳纤维的特点是轻和滤震,也具有不错的强度,但它比较脆,不像金属一样具有一定的延展性,受到强力的挤压会导致碳制零件断裂报废 另外一点就是,碳纤维技术虽然在90年代就被发明了,但在工业化的现在,其成本依然高于钢制铝制材料 钛合金钛制品在很多领域都具有一种特殊的光环.比如钛制水杯 基于钛金属本身的一些特性(耐腐蚀,重量相对钢制品较轻,强度高),在一小部分人群中,会追求钛车架 但钛本身滤震性并不如碳,基于它的成本和碳都不是普通人愿意接受的价格,不推荐普通人接触这个东西 刚性 指的是自行车车架和各个部件在受力时抵抗形变（弯曲、扭曲）的能力 以上发言来自 ai Gemini 2.5 pro 什么是刚性呢?我简单理解为稳定性 折叠车由于需要折叠,整个车架并不是一体的,比起传统的一体式车架,更需要考虑在暴力骑行中是否会出现车身晃动甚至散架的问题 折叠车的刚性和车架的结构和材质关系比较大 比如,大行 P8为了增加稳定性,在中轴和碗组的位置安装了一根叫马甲线的线材 比如,蚂蚁腿车架使用了铬钼钢作为车架材料 整车品牌 虽然主要介绍的是组装,但了解一下整车也是很有必要的 有一些不那么爱折腾的朋友,有一种途径是买整车然后再替换部分配件 换下来的旧配件可以放闲鱼上卖.事实上也不少人是这么做的 大行(DAHON)这是个美国的品牌,在国内的折叠车领域也算得上是一把手了,比较有名的型号是大行 P8 只看线上电商平台的渲染图,我更喜欢大行的另一个车型大行 K3 Plus 我在线下试骑过这个,但车店只有一辆二手的车型,车子骑起来特别软,刚性很差,导致我放弃了这个车型 燕鸥(Tern)这是牌子是从大行出来的员工新创立的牌子 配件按重要性来排序,各个大件我会这样排序 车架 变速套件 内外胎 坐管 头管 刹车 轮组 车把 坐垫 其它 车架 折叠车的车架一般都是带前叉的 自行车圈子有一句调侃的话是 穷玩车架,富玩轮 车架作为联通自行车各个部件的中间人,地位就像是电脑中的主板一样.有些人狠狠加料,有些人觉得够用就行 前面有说到,有部分朋友是买的整车然后改装,然后闲鱼出售旧的配件 既然有出售配件的,当然也有平台车车架的.你也可以选择在闲鱼蹲一辆大行的拆车车架 不过,二手产品的稳定性不那么好保证,如果有洁癖还是选择新购入吧 车架方面,国内比较知名一些的车架平台有风行,科瑞斯(Crius),犀牛(EIOSIX) 这里面,名声最大的是风行,这个品牌早期是给整车品牌大行做车架代工的.所以也有人会说风行的车架是盗版的 车架也有几种不同的大类,我比较熟悉的有 K架 Y架 海豚架 蚂蚁架(蚂蚁腿) 以上都是风行的几款车架,你更喜欢哪一款呢 K架作为大行P8经典车型的同款车架,算是老大哥了 Y架多一条横梁,是这四款车架中刚性最好的 什么是刚性呢?我简单理解为稳定性 折叠车由于折叠的特性,稳定性不如一体式车架 对于大体重的人来说,骑折叠车或许不是一个好选择 海豚架是最有特点的一个车架,整个车身像是一个已经跃出海面的海豚 蚂蚁腿是我个人认为最漂亮的一款车架,电商平台把它归类到了复古风格的车架 材质再说说车架的材质 风行的上述四个车架,除了蚂蚁腿是铬钼钢,其它的三款都是铝合金材质 在自行车领域,材质比较多为铝合金材质,这个材质在工业化的现在易加工成本比较低,有较好的稳定性 如果资金富裕,可以尝试进军碳纤维领域.富玩轮的轮说的就是碳纤维的轮组 碳纤维有很好的避震性能,它本身也具有不错的刚性,更重要的是,相比于铝,碳纤维可以大大降低整车的重量 还有钢架.一般而言,钢材是比较重的,折叠车为了加强车架的刚性,有些车型会选择铬钼钢作为车架材质,比如风行的蚂蚁腿车架 最后还有一种很小众的车架,钛金属车架.这个材质的车架成本和高级碳纤维应该是大差不差的.都是有钱人的玩具.据车友自述,钛架的避震性能很差,而且重,普通玩家不建议入手 尺码 先下一个结论,折叠车并没有尺码这个说法,至少我没没见到同一款车型可以选择不同尺码的折叠车 尺码在公路车,山地车,瓜车中出现的比较多 下图是瓜车 骁风真挚 的车架尺码 但折叠车也不是完全没有尺码,只不过折叠车的尺码不同于公路车的尺码 比如风行蚂蚁腿这个车架,有 FGD2018 和 FGD1618 两种 FGD2018 安装 20 英寸和 21 英寸的轮组,一般安装 20 英寸 FGD1618 安装 16 英寸和 17 英寸的轮组,一般安装 16 英寸 这个尺码会直接影响你的其它配件 变速套件 分类套件分为大中小大类 它们包含的配件不一样,但具体包含了哪些配件没有统一的规定,甚至每个店家对于小中大套件的定义也是不一样的 购买的时候需要注意套件中包含的配件 下面给出一个示例,不一定标准 套件 配件 小套件 前拨 + 后拨 + 变速器 + 链条 + 飞轮 中套件 小套件 + 曲柄 + 牙盘 + 中轴 大套件 中套件 + 刹车碟片 + 刹把 自行车套件逃不过的一个品牌是禧玛诺(SHIMANO),国内还有顺泰,蓝图等厂商 不过,据广大网友评价的结论,禧玛诺依旧还是第一梯队 接下来只简单说说禧玛诺 在公路车领域,禧玛诺有 CLARIS SORA TIAGRA 105 ULTEGRA DURA-ACE 这六个等级,品质级别从低到高,普通骑友的套件天花板是105.理由是105摸到了ULTEGRA的绝大多数体验,但性价比比ULTEGRA高很多,被称为专业入门的套件 作为折叠车,我觉得上TIAGRA就行了 如果你去自行车店购买自行车,店家宣称自己的车是禧玛诺的变速套件.希望你能询问老板自行车的哪些配件是禧玛诺,以及使用的是那个级别的禧玛诺套件 为了有一个直观的感觉,我贴一个大概的价格 价格表来源为淘宝,时间为 2025.09.14 禧玛诺套件 价格 / ¥ TIAGRA 4700 小套件(中腿后拨 + 变速器 + 链条 + 飞轮) 410 TIAGRA 4700 中套件(中腿后拨 + 变速器 + 链条 + 飞轮 + 曲柄 + 牙盘(1 x 10速) + 中轴) 880 105 R7120 小套件(前拨 + 后拨 + 手变 + 夹器) 1,720 105 R7120 中套件(前拨 + 后拨 + 手变 + 夹器 + 链条 + 飞轮 + 曲柄 + 牙盘(2 x 12速)) 2,350 内外胎其实主要讲的是外胎,内胎可讲的不多 内胎我知道的内胎材质有两种 传统的几丁质材料 和 TPU塑料材质 它们两的主要区别是 对比参数 几丁质 TPU 重量 重 很轻 气密性 好 很差 稳定性 好 很差 外胎先说说品牌 世文(SCHWALBE): 在防刺方面做的比较出色. 大行P8 用到的大苹果胎(Big Apple)就出自这个平台 德国马牌: 相对比较综合一些,防刺和竞速方面都不错 对于公路车而言,为了竞速,公路车的外胎通常会做的比较细,且胎面较薄,轮胎的胎压也会比较高,这么做是为了降低轮胎的滚阻,让自行车的气动性更佳 对于山地车而言,情况就不一样了,它们的轮胎一般又大又宽,胎面有很明显的花纹用来增加与地面的摩擦力,这么做是为了适应不同的路面环境 公路车为了速度,轮胎会做的细窄且胎压较高,但代价是公路车的轮胎更适合在路况良好的路面,对于路况差的路面来说,公路车的轮胎往往更容易出现爆胎,避震性差的问题 山地车为了适应性,轮胎做的又大又宽,代价是轮胎更重,滚阻更高 对于折叠车如何选择,我想这因人而异 但就像是在公路车和山地车之间,还有一种瓜车一样,多数折叠车的轮胎选择既不会像公路车那样激进,也不会像是山地车那样被要求适应复杂路况.所以我的选择是世文的马拉松plus系列 坐管 坐管是 ai 强烈推荐我换成碳纤维的配件之一,理由是它能低成本避震 不过说是低成本,其实也是相对于碳前叉和碳车架来说 不过说到碳坐管,除了它避震和重量轻的优点,也有个缺点,碳纤维坐管更容易出现滑落的问题.所以你可能要加配双层坐管夹以及增加摩擦性的止滑剂 个人推荐折叠车使用铝坐管即可 头管 折叠车的头管也是可以折叠的 折叠方式按照折叠方式可以分为两种内折和外折 据说外折头管会更不容易磕碰到车架 拆卸方式按照拆卸方式,可以分为快拆,T型和双钉 快拆: 上图左一,左二,右三,右一都是快拆,意思是顶部有一个折叠扣可以徒手取下车把 T型: 这个了解的不多 双钉: 需要借助内六角扳手才能取下来车把,与它的安装和拆卸麻烦相比,稳定性要比快拆头管好不少 形状主要有两种形状,一种是上图这样弯弯的,叫鹅颈头管 还有一种更常见的直立头管 高度折叠自行车的坐管是很长的,可以自由调节,但头管的高度是不可变的,建议有条件可以先去一些自行车线下店(比如大行,迪卡侬等等)实际体验一下它们的整车并观察其头管的长度,寻找合适自己的高度 与头管一样,车把的长度我也是推荐在线下实际体验之后再确定 刹车圈刹 共享单车的刹车使用的就是这个 相对于碟刹来说,无论是购买成本还是维护成本都更低,缺点是刹车的制动性能一般都是不如碟刹的 碟刹圈刹的刹车位置在轮组的轮圈上,碟刹的刹车位置在轮组轴承的碟片上 碟刹也分多种 线拉碟刹车的制动力通过拉动钢线来传导,有小部分人认为线拉碟调教得好甚至不逊于油碟 油碟刹车的制动力通过压迫线管中的油来传导 除了安装难度高,维护成本高,算是自行车领域刹车的王者 油碟刹车会比圈刹制动更短,意味着捏刹车不能一下捏死,不然在快速行驶时容易轮胎抱死摔车 推荐折叠车上推荐使用禧玛诺的 MT200油压碟刹 + 禧玛诺RT56油刹碟片 至于为什么,问就是大家都这么选,性价比和功用性在折叠车上都拉满了,甚至性能过剩 网上有传言说,折叠车上油刹维护成本高,因为折叠更容易导致油管漏油 关于这个问题,我咨询过一个五年骑友,他自组的折叠车都是油压碟刹,我想问题应该没那么大 轮组轮组我了解的不多 需要注意的是,根据车架的尺寸(20英寸,16英寸比较常见)以及刹车系统(圈刹或者碟刹)的不同,选择的轮组也是不同的 有一个有意思的轮组叫星芒轮组(下图左) 说是辐条的形状像是一个五芒星一样,辐条的减少可见降低风阻,但也一定程度上降低轮组的可靠性 另一个问题是星芒轮组会更贵一些 车把 和坐管一样, ai 认为这是低成本换碳车把,可以在避震和重量上带来很大收益的配件 我的建议是有钱可以上碳纤车把,预算吃紧还是用铝的吧 坐垫 很多坐垫都做了中空处理,女士我不清楚.对于男士而言,骑行的姿势使得前列腺一直处于压迫状态 有过长时间骑车经验的男士小伙伴,应该都会感觉到下体发麻甚至短时间内没有知觉 骑行圈处理这个问题主要是四大措施 坐垫.像是图中的坐垫一样中间留空,减少压迫 骑行裤.貌似是在裆部加了一层垫子 铁锭.多骑多练,让身体慢慢适应 换避震性能好的配件.比如碳车架,碳前叉甚至避震前叉等 其它脚踏脚踏按材质分类,有普通塑料的,尼龙塑料的,金属的 我个人比较推荐尼龙塑料,金属虽然质感好,但折叠之后容易剐蹭到车身导致掉漆 还有一种是快拆类型的脚踏,也就是说脚踏可以徒手拆卸下来 不过这种脚踏会更重一些 B柱前转换座 如果安装的是圈刹,就不需要这个小配件 折叠车的车架前叉如果安装碟刹需要一个金属垫来垫高碟刹夹器 注意后轮一般不需要 刹车线管套保护套(非必须) 用来束缚刹车线和变速线管的保护套,起到一个美观和保护的作用,可不用 安装工具内六角扳手(必备) 内六角属于是必备的工具,无论是安装还是维护,是比较常用的工具 对于常用的工具,推荐买一套材质比较好的,避免滑丝或者断裂 苹果酱润滑脂(必备) 业内人士称它为苹果酱大概率是因为颜色吧 功效就如同图里说的,几乎所有金属零件的连接出处都可以来上一点,可以达到防水,润滑的作用 如果配件中还有碳纤维制品,还需要额外购买止滑剂增加摩擦力 打气筒(必备)无须解释,有压力表显示的更佳 链条截链器(必备) 购买的链条一般都是偏长的,需要链条截链器截断几节链条之后通过魔术扣连接 魔术扣钳子(非必备) 链条使用魔术扣连接之后,需要通过这个工具来夹紧 或者你也可以省略这个工具,通过大力踩踏脚踏让链条自然拉紧 卡飞套筒(必备) 安装飞轮的工具,需要配合扳手一起使用 活动扳手(必备) 这家伙应该不会有人不认识吧 卡飞扳手(非必备) 这个工具需要配合卡飞套筒一起使用 作用是用来固定飞轮.看到上面的链条了吗,使用链条卡住飞轮的齿轮以固定飞轮不转动 这个工具在拆卸飞轮时才需要,安装飞轮的时候必须 中轴扳手(必备) 中轴扳手顾名思义是用来安装中轴的 禧玛诺和浩盟的许多中轴都只需要这个工具就可以完成安装 小部分中轴还需要用到中轴套筒,关于这一点可以向中轴的店家咨询是否需要中轴套筒 碗组压入工具(非必备) 购买车架的时候可以询问店家是否可以帮忙压入碗组(当然,你也需要在同一家店铺购买碗组),同意帮忙的话可以省下这个工具 也有人大力出奇迹暴力压入碗组,只能说风险有点大,可能导致车架或者碗组报废 电工胶带(非必备) 电工胶带的作用有很多 在安装刹车把和变速车把的时候,可以现在车把上缠一两圈胶带再固定安装刹车把和变速车把会更佳 个人组装配置单 配置单具有时效性,可能当你看到时已经过时,或者已经有更合适的配置 同时也具有强主观性,选择某一个配件只因我的喜好 折叠自行车配置单 参考价格均来自 淘宝,时间为 2025.09.14 配件名称 规格 备注 参考价格 / ¥ 车架 风行FGD2018碟刹 蚂蚁架 黑金 1247 变速套件 TIAGRA 4700小套件 395 - 飞轮 - 指拨 - 后拨 28T飞轮短腿 - 链条 内外胎 数量 * 2 213.4 * 2 - 世文马拉松PLUS 20*1.35 - AV6美嘴内胎 坐管 MaxtroN美壮 Tower5 黑色 86.2 头管 黑色-双钉外折(31.5cm) 鹅颈头管 172.66 刹车 - 碟片 禧玛诺 RT56 160mm * 2 80 - 油刹 禧玛诺MT200油压碟刹 油管长100-170cm 购买前请咨询店家油管长度,避免不必要的截管 185 轮组 EIOSIX犀牛 中刀碟刹 406 全黑色 * 2 406 代表轮组的轮径,表示20英寸; 451 表示 21 英寸 466.98 车把 MaxtroN美壮Needle5 黑色 25.4*500mm短直把 车把长度建议线下体验之后解决 74.8 坐垫 SELLE ROYAL 自行车座垫 treking旅行款 77.4 脚踏 尼龙脚踏 三培林脚踏 橘色 62.74 中轴 + 曲柄 + 盘片 + 盘钉 prowheel浩盟 有没有盘钉需要执行咨询店家,没有需要额外购买 189.38 - 中轴 中空 - 曲柄 170曲柄 - 盘片 52T盘片 - 盘钉 碗组 MaxtroN美壮 红色 也可以换成景晔的碗组.买前咨询店家是否可以帮忙压入碗组再发货 68.6 把套 美国ODI橡胶肉球 红色 18.25 碟刹转换座 禧玛诺 160MM-B柱前转换座 10 脚撑 20寸黑色 非必须 48 折叠自行车磁铁 非必须,折叠之后避免车架散开 11.13 刹车线管套保护套 12mm红色 2米 非必须,固定,美观刹车和变速线管 24.1 合计 3644.04 折叠自行车组装工具配置单 参考价格均来自 淘宝,时间为 2025.09.14 配件名称 规格 备注 参考价格 / ¥ 内六角扳手 S2钢材 35 苹果酱润滑脂 50克 17.91 打气筒 气压表 + 多功能气针 脚踩式 30.9 中轴扳手 TL-BB01扳手 注意与中轴的适配,请在购买前咨询店家是否适配你所购买的中轴 25 魔术扣钳子 8.8 链条截链器 + 卡飞套筒 + 卡飞扳手 只是安装可以不需要卡飞扳手,卡飞扳手只在拆卸时有用 32 碗组压入工具 简约款 非必须.购买车架时,请咨询店家是否可以帮忙压入碗组.可省略此工具 28 活动扳手 10寸开口35mm 13.6 电工胶带 黑色1卷30米 非必须 3.9 总计 195.11 附表配件简图 配件 简图 车架 前叉 车把 把套 头管 碗组 坐管 坐垫 脚撑 脚踏 曲柄 链条 飞轮 后拨 指拨 油刹 刹车片 外胎 内胎 轮组 碟刹转换座 折叠自行车磁铁","link":"/bike/2025/09/bike/index.html"},{"title":"自组折叠自行车 - 实践篇","text":"从上一篇 自组折叠自行车 中, 我已经掌握了很多折叠车的理论知识 古语说, “纸上得来终觉浅, 绝知此事要躬行” 现在它来了 购买折叠车的配件都是基本都是单独购买的, 并没有局限于某一家店铺 不过由于配件巨多, 购买平台还是统一在淘宝, 部分配件在其它平台购买可能有些许优惠, 但为了方便, 选择了妥协 这里提几个配件的插曲 内胎与轮组不匹配首次购买的内胎气嘴是美嘴的, 但轮组的气嘴孔无法通过美嘴 和卖家商量之后, 卖家倒是也很爽快给我更换了适配轮组的法嘴内胎 脚撑不适配很多公路车是不安装脚撑的, 尤其是碳纤维车架的公路车 在我购买的所有配件中, 脚撑是最先到达的快递, 但脚撑又属于是可安装又不需安装的配件 就这样脚撑不适配的问题一直等到无法退货才被发现 万事俱备, 那就开干 组装 组装折叠车的主要难点在于变速微调 其它的只要有时间和蛮力 下图是我只靠自己组装的成果, 已经完成了九成的组装 后拨变速少购买了一把剪线钳, 无法完全安装调试 最后花了 100 大洋在车店完成了最后的组装 称重是 11.68kg 上路 是骡子是马, 拉出来溜溜 我定义折叠车主要是在城市道路休闲骑的交通工具 变速变速套件使用的是 禧玛诺 TIAGRA 4700, 变速很丝滑, 也基本够用 骑行时不需要刻意去关注档位, 凭感觉调整档位大多数时候都在中间的档位切换 避震由于我没有上碳纤维, 避震几乎是没有的 长时间骑行后, 手腕会有酸痛 坐垫也是偏硬的(坐垫比共享单车的还硬), 骑久了会屁股痛 坐管夹车架自带的坐管夹有坐管滑落的毛病 需要将坐管夹的螺丝拧紧才能避免, 但拧紧的弊端是, 坐管夹快拆扣需要很大的力气才能锁定 可能双层坐管夹可以解决这个问题, 但一个便宜的双层坐管夹也要一百多 线管大多数的折叠车线管会预留一部分长度, 这部分长度冗余给折叠使用 比如我的折叠车, 车头的线管呈现牛角的弧线 挡泥板挡泥板是很重要的配件 但这个配件在一些自行车(尤其是公路车)上是被歧视的 可以看到我的折叠车也是没有安装挡泥板的 这会导致路过水面时, 泥点子毫无阻拦地洒在骑车者的背上 蹬车折叠车的刚性不太好, 蹬车的感觉很软 从车架的结构上来看, 折叠车也不适合蹬车 加速或者上坡可以通过变速来达到, 蹬车不是一个好习惯 其它图片 总结整车组装下来, 零零散散也要个 4千 对于在家就在工作地的小伙伴, 不建议购入折叠车, 它虽然携带方便一些, 但性价比和安全性比不了山地车和公路车","link":"/bike/2026/01/bike/index.html"},{"title":"2024年年中消费简述","text":"简述简单总结一个自从三月来深圳的大体消费支出情况 基本情况 2024 年 3 月到 4 月中旬前，处于求职状态，这个时候暂无收入 2024 年 4 月下旬至今，由于有了工作，每月的开销都相对固定 消费类别个人的支出情况主要分为几大类 住宿 饮食 交通 零食 生活用品 其它购物 在求职阶段，主要消费类别集中在前三项 消费细项求职阶段住宿住宿是消费的大头，求职阶段我的住宿是先住酒店落脚，后面找民宿 / 公寓缓解住宿费用高的压力 如果是初来深圳没有工作，且没有熟人帮衬的情况，先住酒店，后面换民宿 / 公寓我认为是很好解决住宿问题的方案 如果确实没资金，还可以考虑青旅。青旅即便在南山区 / 福田区的繁华地段，也能找到 30 / 日的青旅。但我不推荐的青旅，因为青旅多是多人一间，这很考验人的睡眠质量。曾经我住过一次两天的青旅，较为常住的下铺室友貌似是跑外卖的，但他这个外卖员不是天天做的（我有天出去面试，正常人工作时间回来发现他还在床上）。无论你是想要正常休息，还是准备面试，青旅的环境都绝不是一个好地方 我记得我第一次来深圳，选在了深圳市宝安区的翻身地铁站附近（后来了解到这个站算是宝安区比较繁华的一个地方），因为有酒店信任优惠，大概 160 / 日，连续住宿了两三天 一方面，因为出来深圳，不清楚面试地点主要集中在那一个区域，所以这个时候我的住宿地点一方面受近期面试的公司地点影响；另一方面，也受到酒店 / 民宿 / 公寓的日租价格影响 我比较常用的住宿订购软件是美团，简单说说遇到的两个糟心事 前一天定好的民宿，到了之后被告知没空房了。所以一定要先在美团上提前和房东打招呼，询问好是否在入住的那天有空房 共享电动车有地区范围限制。扫码开锁之后没走一百米就提示超出了服务区域，被强制停电了。这个属于是我没注意看软件说明导致的 选择比较偏远的地方酒店 / 民宿，可以把短租的住宿费用降低到 100 / 日左右 这个价位的酒店环境可能就比较一般了，你住宿的酒店可能会有以下问题 酒店位置偏远。可能距离地铁比较远 酒店卫生一般 入住时房间的烟味甚至也没有散去 sex 方面的小广告，小玩具等 民宿的话 费用较低，比如 90 / 日，房东可能不会提供空调服务。如果有电风扇，看当天的天气是否能接受 蟑螂多。这个生物在广州，深圳都是很常见的，如果非常害怕这个的话，要么多花钱去卫生条件好的地段，要么就不要来广州深圳（认真脸） 饮食饮食消费主要看住宿的地方 早餐 早操因为深圳的地摊很多，如果你能接受炒面炒粉做早餐，我想早餐完全可以控制在 5 / 日以内，即便是在福田区，南山区这些地方 午餐，由于午餐有时候会处于两段面试中间，所以午餐需要在相对繁华的地段解决，这个时候我偏向于面食类午餐 午餐我一般吃面，一方面面食相对较为便宜，消费一般都能在 20 / 顿以内 或者能找到快餐店，消费也不会高出太多。大概在 (10 , 30) 这个范围浮动吧 晚餐，一般回到住处附近吃 一天三顿最好有一顿是大米饭的。如果硬是要给一个范围的话，30 / 顿我是可以保证的。如果去一些快餐店（比如什么木桶饭），一般能够保持在 20 / 顿以内 其他 一天如果上下午都有面试，我是不回住宿的地方。中午解决完毕午餐之后，还需要找个地方能让我等到面试时间。这个地点一般是各种奶茶店 我以前比较少喝什么喜茶，瑞幸。来了深圳之后这些奶茶和家里的三线小城市相比物价方面也还差不多吧（当然，不要选购贵的） 交通在深圳交通费用基本是地铁。我的消费主要是地铁 + 共享单车，以及几次来回深圳的高铁费用 关于交通，我可以很认真地说，住宿的地点一定要选择离地铁近的。这样无论的工作地点在哪里，至少你能到达（中间可能需要多次转路线） 关于地铁，如果是 iPhone 手机用户，建议开启深圳通（打95折），而且开卡不需要手续费；如果是安卓用户，开卡需要手续费，不打算长期或者暂时无法遇见是否能长期呆在深圳的话，建议使用微信小程序付费（没有优惠） 深圳地铁的费用我个人觉得不低，但确实很方便出行 然后就是，深圳地铁 11 号线真的多人，每天都要挤这条线的人感觉绝望又无奈 零食– 生活用品生活用品我大部分都是随行李箱自带，除了洗衣粉 其他购物这个阶段除了必须的物品，一律一毛不拔 有部分费用，比如简历的打印费用，在深圳无论是繁华地段还是偏远地段，都是 1 / 张，在广州的一家打印店我倒是遇到一次几毛钱一张的。简历的打印我一般推荐一次不要打印过多份，一方面可能有修改的需求；另一方面A4纸在身上很容易被弄出折痕 工作阶段住宿在深圳宝安区 11 号线附近找了一处 1000 / 月的一室一卫，每月加上水电网费，大概在 1200 / 月 关于住宿，如果可以离公司很近，那无所谓，如果离公司很远，那必须选择离地铁站 1 公里以内的 饮食早餐 一般 3 / 顿，两个包子。周末 5 / 顿，肠粉 中餐 工作提供了 500 / 月的餐补，勉强够工作日中餐的消费。大概 25 / 顿，不够的是否需要自己补上一部分 晚餐 如果当晚跑步，一般吃面食。住宿附近的面食比较便宜，可以控制在 13 / 顿以内 如果没有跑步，可能会选择米饭，各种快餐店（比如木桶饭），大概能控制在 15 / 顿左右 其他 周末可能和朋友一起在外面搓一顿，消费一般在 300 / 次以内 交通地铁 每个工作日的早晨都是在挤深圳 11 号线的路上，固定消费 7 / 次，一天一个来回，一共 14 / 天，会有一点点打折（95折） 如果不幸中途出站上厕所，可能会把当天的地铁消费提高 1 - 2 元 共享单车 有时候周末去图书馆骑共享单车，一个月下来共享单车的消费应该不到 20 / 月 零食 每个星期大概都会有 50 左右的消费在零食上面 生活用品 T恤，我比较喜欢纯白纯棉的，然后在深圳的几个月里，因为多雨的天气坏了三件 裤子，鞋子，洗衣粉，电风扇，电热锅，洗洁精等等 其他购物 我的衣服都是晚上购物的 近期购买了一个 小米平板（3200）送妹妹生日 建议 初来深圳先住酒店落脚，后面换成靠近地铁的民宿 / 公寓 只要没有住在繁华的地段，饮食的消费一般不会太过分，一般能在住宿附近解决就在住宿附近解决 地铁是绝大多数深圳打工人的出行方式，可以在线办理深圳通，有一点点优惠 碎碎念深圳虽然确实也没啥可玩的，但来了深圳。可以在面试或者工作之余去爬爬山，逛逛商场。生活不只有工作，一旦工作起来，下一次连续的工作日长假就要等离职了","link":"/chat/2024/09/cost/index.html"},{"title":"自建音乐流媒体的碎碎念","text":"前言首先谈谈我对音乐，以及流媒体的理解 我对音乐的狭义理解是：由乐器弹奏，或者人哼唱出来的歌。对于流媒体，我认为在网络上传播的影像记录就是流媒体 现在我们来看看维基百科给它们的定义 音乐，广义而言，就是指任何以声音组成的艺术。英文Music一词源于古希腊语的μουσική（mousike），意即缪斯（muse）女神的艺术。而中文的音乐二字，东汉许慎《说文解字》解释为“音，声也。生于心，有节于外，谓之音。”认为音乐和声音的区别，在于音乐需要透过人心去想像和创造。 流媒体（英语：Streaming media），指将一连串的多媒体资料压缩后，经过互联网分段发送资料，在互联网上即时传输影音以供观赏的一种技术与过程，此技术使得资料数据包得以像流水一样发送，如果不使用此技术，就必须在使用前下载整个媒体文件。 除此之外，我在 音流 这款软件的文档上，看到了一段这样的说明 网络附接存储（英语：Network Attached Storage，缩写：NAS）是一种文件级（与块级存储相对）的计算机数据存储服务器，它连接到计算机网络，并提供对异构网络用户的数据访问。它专门用于通过其硬件、软件或配置来提供文件服务。它通常作为专门制造的专用计算机设备制造。 NAS系统是包含一个或多个通常排列成逻辑存储器、冗余存储器或RAID存储驱动器的网络设备。NAS消除了从网络上的其他服务器提供文件服务的负担，它们通常使用网络文件共享协议（如NFS、SMB或AFP）提供对文件的访问。作为一种在多台计算机之间共享文件的便捷方法，NAS设备从1990年代中期开始开始流行起来。与同样提供文件服务的通用服务器相比，专用网络附加存储的潜在优势包括更快的数据访问、更简单的管理和简单的配置。 我知道自建音乐流媒体是一个小众的需求，不过搭建一个音乐流媒体平台不需要借助 NAS 也是可行的，因为我观察到较为主流的开源流媒体服务都是支持 Docker 部署的 需求起初我是没有想要自建流媒体服务的，那时我对一款音乐播放器的需求是这样的 功能单一，只需要有基本的播放，收藏，查看歌词，歌手封面 有云盘功能，支持移动客户端上传音乐 没有广告（不充会员的情况） 我还在用的两款音乐软件，酷狗，网易云音乐都是无法满足第一点和第三点的，而且我以前根本不敢想的弹窗广告和摇一摇广告竟然都出现了。以至于很长一段时间内，我都是使用Bilibili来听视频，反正我这个木耳朵也听不出来区别 24 年阮一峰的博客推荐了一个音乐流媒体服务，现在尝试找了找，没找到当时的那篇博客。当时我尝试安装了试用了一下，原本以为它能像是一个只拥有播放，网盘功能的酷狗音乐。结果登录Web页面之后，什么都没有。现在想起来，是需要我先把音乐文件喂给它，它只负责读取然后播放。当时觉得这个东西根本没法用，就放弃了 最近刷到了一篇文章，问大家都用的什么音乐播放器。里面有人回答，Apple Music，Spotify，网易云音乐。使用 Apple Music 的用户觉得没广告音质好，推荐Spotify的用户说曲库全还免费，还在用网易云音乐的用户说自己是永久会员 就我个人而言，Apple Music 我也用过，周杰伦的歌也有，但我搜索进击的巨人时，给出的列表没有网易云音乐那样丰富 Spotify 我记得是有广告的，它是在听歌的间隙插入音频广告。也因为是国外设计的软件，用起来不习惯就放弃了。因为我听歌一般只认歌名和歌手，什么专辑对我而言是多余的东西 至于网易云音乐，广告以及功能太杂了，我觉得音乐播放器不应该这么臃肿 在众多的评论中，我看到有些人回复了一些我没听过的名字，这个 音流 就是其中之一 搜索了一下 音流，提供四端应用（Mac，Windows，Android，iOS）。看官网给出的截图也挺纯净简洁，勾起了我自建音乐流媒体的兴趣 说明需要说明的是，我们经常说的网易云音乐，Apple Music它们只是一个音频播放器客户端，真正大量保存音乐资源的都是在服务器上，网易云音乐有网易云音乐的服务器，Apple Music也有Apple Music的服务器，这样做才能让我们在移动终端上不需要下载也能听到音乐传作者们发布的歌曲 那么需要自建音乐平台，至少也要有一个服务端和一个终端。如果把这个服务端类比成网易云音乐的服务器，终端类比成网易云音乐的手机 app 或者电脑端的 app，应该会更形象一些 对比先确定了终端使用 音流 这款软件，那么就要看看音流这款移动终端它适配了哪些服务。这是 音流 官网给的适配功能对比 第一行是各个流媒体服务的名称 功能 Subsonic Navidrome Audio Station Emby Jellyfin Plex 内嵌歌词 - √ √ √ 外置歌词 - √ √ √ √ 在线歌词 - √ √ √ √ 歌手简介 - √ √ √ √ 歌手头像 - √ √ √ 多艺术家 - - √ √ 回放增益 - √ √ √ 评分功能 √ √ √ √ 收藏功能 √ √ √ √ 文件夹 - √ √ √ 删除接口 - √ √ √ 这是各个流媒体服务的特性对比 特性/平台 Subsonic Navidrome Audio Station Emby Jellyfin Plex 定位 音乐流媒体服务器 轻量级音乐服务器 Synology 专属音频服务 多媒体服务器 多媒体服务器 多媒体服务器 开源 部分开源 开源（GPL-3.0） 闭源 部分开源 开源（GPL-2.0） 闭源 支持的媒体类型 仅音频 仅音频 仅音频 音频、视频、图片 音频、视频、图片 音频、视频、图片 跨平台支持 是 是 否（仅 Synology NAS） 是 是 是 客户端支持 多种（Web、手机应用） Web、手机应用 手机应用、浏览器 多种（Web、应用） 多种（Web、应用） 多种（Web、应用） 多用户支持 是 是 是 是 是 是 离线下载 是 是 否 是 是 是 流媒体转码支持 是 是 否 是 是 是 字幕支持 否 否 否 是 是 是 多设备同步 否 否 是 是 是 是 免费使用 部分功能 完全免费 包含于 Synology 部分功能免费 完全免费 部分功能免费 高级功能付费 是 否 不适用 是 否 是 活跃社区 较小 较小 专注于 Synology 用户群体 较大 较大 较大 硬件需求 较低 较低 依赖 Synology NAS 中等 中等 中等 Docker 支持 是 是 否 是 是 是 我说说我的看法 Subsonic 算是老牌的流媒体服务了，后续很多的流媒体服务都是基于 Subsonic 的 api 开发的，不过 Subsonic 基于 Java 开发的，资源消耗也会大一点，而且对我来说，它的功能有点复杂了 Navidrome 是使用 Go 开发的，完全开源，在网上也看见了不少朋友在推荐，也满足我的基本要求，除了上传音乐的功能 Audio Station 就不说了，我需要一个能 Docker 部署的服务 另外三个我没实际安装体验过就不发表意见了 除了上面的六个流媒体服务器之外，基于上传的需求，我找到了 airsonic-advanced，一个基于 Subsonic 开发的流媒体服务器，也支持上传 不知道大家会不会觉得功能太复杂了，实际安装之后我觉得有些复杂了，自用的系统不需要这么多功能 那么最终就选定 Navidrome 了，开干 部署基于 Navidrome 的安装文档，很快都能配置完毕，以下是我的 docker compose 配置，仅供参考 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647services: navidrome: image: deluan/navidrome:0.54.3 container_name: navidrome restart: unless-stopped user: 1000:1000 networks: - music environment: ND_SCANSCHEDULE: 1h ND_LOGLEVEL: info ND_SESSIONTIMEOUT: 24h ND_DEFAULTLANGUAGE: zh-Hans ND_ENABLETRANSCODINGCONFIG: true volumes: - ~/applications/music/data:/data - ~/applications/music/music:/music:ro lrcapi: image: hisatri/lrcapi:1.5.7 container_name: lrcapi restart: unless-stopped networks: - music depends_on: - navidrome environment: - API_AUTH=xxx volumes: - ~/applications/music/music:/music route-music: image: nginx:latest container_name: route-music restart: unless-stopped ports: - 3312:3312 networks: - music depends_on: - navidrome - lrcapi volumes: - ~/applications/music/nginx/nginx.conf:/etc/nginx/nginx.conf:ro - ~/applications/music/nginx/logs:/opt/nginx/logsnetworks: music: driver: bridge hisatri/lrcapi:1.5.7：是一个补充服务，用于弥补 Navidrome 无法提供歌词和封面的缺陷 上传音乐自建的流媒体是没有任何音乐的，需要手动上传到流媒体服务器上。这也是我期望有上传功能的原因，但上传并没有在 Subsonic 的 api 中 我目前是通过 SFTP 上传到服务器的。我的听歌范围变化不那么频繁，这一步虽然麻烦但还能接受 成果展示 体验首先 音流 确实很简洁，没有任何广告，基本功能除了一个上传也够我用 其它一些缺点在于，Navidrome 流媒体服务安装在我家里，网络延迟有点大，有时候会出现歌词加载不及时的问题。至于听歌本身我感觉下来还是没什么问题的，主要还是因为我的歌曲大都都是 128k 音质的，大小在 5MB 以内 如此如此~~","link":"/chat/2025/01/music-stram/index.html"},{"title":"浅谈 @EventListener 和 @TransactionalEventListener","text":"前言spring 有一个事件传播机制,可以较为方便地实现一个简单的广播模型 一般使用 ApplicationEventPublisher(org.springframework.context.ApplicationEventPublisher) 来发布一个事件,然后使用 @EventListener 或者 @TransactionalEventListener 标记的方法来接收指定的事件 这样做有什么好处呢?一方面是解耦,另一方面提高可拓展性 @EventListener 使用我一般会这样使用 事件定义123@Builderpublic record UserUpdateEvent(Long id) {} 事件发布者: xxxService 中123456789@Transactional(rollbackFor = Exception.class)public void updateUser() { // 其他业务逻辑 // 发布一个 用户修改 事件 publisher.publishEvent(UserUpdateEvent.builder() .id(userId) .build());} 监听器: xxxListener 中1234@EventListener(UserUpdateEvent.class)public void onUserUpdate(final UserUpdateEvent userUpdateEvent) { log.info(&quot;onUserUpdate listener: {}&quot;, userUpdateEvent.id());} 在这种的用法中,但发布者发布事件之后,就会触发该事件的所有监听器,并在同一个线程中,一个一个地执行监听器的动作,最后才返回到发布事件的位置继续往下执行 如果想要执行执行监听器而不影响主流程,可以配合 @Async 注解实现异步 @EventListener 不适用场景试想一下这样一个场景 如果事件发布时处于处于一个数据库事务当中,同步监听这个事件的监听器发出了一个异步通知给第三方系统,第三方系统接收到这个通知之后需要回查状态.那么使用上面的方案可能会遇到什么问题 第三方系统回查状态时,事件发布者所在的事务已经完成提交 第三方系统回查状态时,事件发布者所在的事务还未提交 回查时事件发布者所在的事务还未提交,那么就可能查询到旧数据,导致数据不一致 想要解决这个问题,有哪些处理方案呢? 使用手动事务,在提交之后再发布事件 使用 @TransactionalEventListener 注解,使监听器在事务提交后再执行 本地消息表.适用于最终一致性场景 方案1 我在实践中遇到一个问题 一般而言, spring 的事务传播机制是 Propagation.REQUIRED,通俗来讲就是有事务时加入,没事务时新建一个事务 在上面的例子中,如果 updateUser 的调用者已经处于一个事务之中,那么 updateUser 便会加入这个事务 也就是在这种场景下说这个事务的边界并非时我们在 方案1 预想的那样,等待 updateUser 执行完毕之后,事务就被提交了.那么监听器仍然是有可能在一个事务之中被执行 将事务传播机制换成 Propagation.REQUIRES_NEW 也能解决这个问题,但这个改动不一定符合实际使用场景是吧 方案2 我重点说说 把原来的监听器改成如下这样,和之前的区别就是方法上的注解,它从 @EventListener 变成了 @TransactionalEventListener 1234@TransactionalEventListener(value = UserUpdateEvent.class, phase = TransactionPhase.AFTER_COMMIT)public void onUserUpdate(final UserUpdateEvent userUpdateEvent) { log.info(&quot;onUserUpdate listener: {}&quot;, userUpdateEvent.id());} 来看看这个注解的定义 1234567891011121314151617181920212223@Target({ElementType.METHOD, ElementType.ANNOTATION_TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@EventListenerpublic @interface TransactionalEventListener { TransactionPhase phase() default TransactionPhase.AFTER_COMMIT; boolean fallbackExecution() default false; @AliasFor(annotation = EventListener.class, attribute = &quot;classes&quot;) Class&lt;?&gt;[] value() default {}; @AliasFor(annotation = EventListener.class, attribute = &quot;classes&quot;) Class&lt;?&gt;[] classes() default {}; @AliasFor(annotation = EventListener.class, attribute = &quot;condition&quot;) String condition() default &quot;&quot;; @AliasFor(annotation = EventListener.class, attribute = &quot;id&quot;) String id() default &quot;&quot;;} 这是个复合注解,它扩展了 @EventListener 比较重要的属性是 phase: 定义监听器的执行时机,可以配置成 BEFORE_COMMIT, AFTER_COMMIT, AFTER_ROLLBACK, AFTER_COMPLETION 四种.四个配置项基本都能见名知意,最后一个 AFTER_COMPLETION 的意思是无法事务执行成功与否都会执行监听器的动作 fallbackExecution: @TransactionalEventListener 比 @EventListener 更加特别的一个地方是,如果事件的发布者没有处于一个事务中,默认不会触发监听器的动作.但如果配置 fallbackExecution 为 true,那么在无事务时发布事件,也可以触发监听器 value 和 classes: 这两互为别名,和 @EventListener 中是一样的,表示监听的事件类型 方案3 就不讲了, spring 的事件机制不支持持久化,也不支持分布式,使用本地消息表方案建议配合 mq 食用 小结@EventListener 适合用在无事务,或者不关心监听器的执行与事件发布者处于同一个事务中的场景 @TransactionalEventListener 适合用在存在事务,且需要控制监听器在事务执行的某个阶段(BEFORE_COMMIT, AFTER_COMMIT, AFTER_ROLLBACK, AFTER_COMPLETION )执行的场景 问题解决,如果使用 @EventListener 和 @TransactionalEventListener 同时监听同一个事件,会怎么样呢 首先,需要明确一个前提,同时使用 @EventListener 和 @TransactionalEventListener 意味着发布事件的时机在事务中(@TransactionalEventListener 也可以用在非事务环境,但一般不会这样使用吧),那么可以确认了,@EventListener 的监听器会先于 @TransactionalEventListener 被执行 理由是 @EventListener 的监听器总是在发布事件之后就被触发,而此时事务仍然处于未提交状态 @EventListener 的原理注册 ApplicationListener在 spring 容器启动时,会将标记了 @EventListener 的方法包装成一个 ApplicationListener 对象存放到容器中 ApplicationListener 中包含了监听器被触发是需要执行的动作,也就是 onApplicationEvent 方法 在 ApplicationListenerMethodAdapter(org.springframework.context.event.ApplicationListenerMethodAdapter) 中,它的 onApplicationEvent 方法如下 1234567891011121314151617181920public void onApplicationEvent(ApplicationEvent event) { // 触发监听器动作 processEvent(event);}public void processEvent(ApplicationEvent event) { // 解析参数 Object[] args = resolveArguments(event); // 根据 condition 属性判断是否需要触发 if (shouldHandle(event, args)) { // 反射执行监听器动作 Object result = doInvoke(args); if (result != null) { handleResult(result); } else { logger.trace(&quot;No result object given - no result to handle&quot;); } }} 再看一下事件的发布者做了什么在 SimpleApplicationEventMulticaster(org.springframework.context.event.SimpleApplicationEventMulticaster) 中 1234567891011121314public void multicastEvent(ApplicationEvent event, @Nullable ResolvableType eventType) { ResolvableType type = (eventType != null ? eventType : ResolvableType.forInstance(event)); Executor executor = getTaskExecutor(); for (ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) { if (executor != null &amp;&amp; listener.supportsAsyncExecution()) { // 异步执行 executor.execute(() -&gt; invokeListener(listener, event)); } else { // 同步执行 invokeListener(listener, event); } }} 根据事件类型取出所有的监听器 这里的源码我也没太看明白,但逻辑就是根据事件类型取出监听器 123456789101112131415161718192021222324252627282930313233343536protected Collection&lt;ApplicationListener&lt;?&gt;&gt; getApplicationListeners( ApplicationEvent event, ResolvableType eventType) { Object source = event.getSource(); Class&lt;?&gt; sourceType = (source != null ? source.getClass() : null); ListenerCacheKey cacheKey = new ListenerCacheKey(eventType, sourceType); // Potential new retriever to populate CachedListenerRetriever newRetriever = null; // Quick check for existing entry on ConcurrentHashMap CachedListenerRetriever existingRetriever = this.retrieverCache.get(cacheKey); if (existingRetriever == null) { // Caching a new ListenerRetriever if possible if (this.beanClassLoader == null || (ClassUtils.isCacheSafe(event.getClass(), this.beanClassLoader) &amp;&amp; (sourceType == null || ClassUtils.isCacheSafe(sourceType, this.beanClassLoader)))) { newRetriever = new CachedListenerRetriever(); existingRetriever = this.retrieverCache.putIfAbsent(cacheKey, newRetriever); if (existingRetriever != null) { newRetriever = null; // no need to populate it in retrieveApplicationListeners } } } if (existingRetriever != null) { Collection&lt;ApplicationListener&lt;?&gt;&gt; result = existingRetriever.getApplicationListeners(); if (result != null) { return result; } // If result is null, the existing retriever is not fully populated yet by another thread. // Proceed like caching wasn't possible for this current local attempt. } return retrieveApplicationListeners(eventType, sourceType, newRetriever);} 然后循环挨个触发监听器动作一路追下去,可以在 invokeListener -&gt; doInvokeListener -&gt; listener.onApplicationEvent(event) 看到 onApplicationEvent 被触发了.完成了一个闭环 @TransactionalEventListener 的原理上面说的是 @EventListener 的原理,那么 @TransactionalEventListener 会怎么样呢 它们区别在于注册监听器类型不一样 @EventListener 使用了 ApplicationListenerMethodAdapter 包装它的执行动作 @TransactionalEventListener 使用了 TransactionalApplicationListenerMethodAdapter 包装它的执行动作 123// TransactionalApplicationListenerMethodAdapter 继承了 ApplicationListenerMethodAdapterpublic class TransactionalApplicationListenerMethodAdapter extends ApplicationListenerMethodAdapter implements TransactionalApplicationListener&lt;ApplicationEvent&gt; {} 此外, TransactionalApplicationListenerMethodAdapter 额外保存了 @TransactionalEventListener 注解特有的 transactionPhase 和 fallbackExecution 属性 TransactionalApplicationListenerMethodAdapter 还重写了 onApplicationEvent 方法 12345678910111213141516171819public void onApplicationEvent(ApplicationEvent event) { if (TransactionalApplicationListenerSynchronization.register(event, this, this.callbacks)) { if (logger.isDebugEnabled()) { logger.debug(&quot;Registered transaction synchronization for &quot; + event); } } else if (this.fallbackExecution) { if (getTransactionPhase() == TransactionPhase.AFTER_ROLLBACK &amp;&amp; logger.isWarnEnabled()) { logger.warn(&quot;Processing &quot; + event + &quot; as a fallback execution on AFTER_ROLLBACK phase&quot;); } processEvent(event); } else { // No transactional event execution at all if (logger.isDebugEnabled()) { logger.debug(&quot;No transaction is active - skipping &quot; + event); } }} 最重要的就是 TransactionalApplicationListenerSynchronization.register(event, this, this.callbacks) 它依赖了 spring 事务,注册了一个事件,方便在事务的生命周期内执行注册的动作. register 方法会判断当前是否处于一个事务中,如果不处于事务中,则会返回 false 如果事务未开启,则会进入第二个 if 判断,即 fallbackExecution 设置为 true 时,即便没有事务也会执行 否则,什么都不做 小结对比 @EventListener 和 @TransactionalEventListener,会发现,其实这两在底层代码上的区别并不是很大. @TransactionalEventListener 的主流程和 @EventListener 大差不差 spring 启动时注册 监听器 为 ApplicationListener 事件发布者发布事件,触发 onApplicationEvent 方法 反射调用 监听器方法 区别在于 @TransactionalEventListener 在触发 onApplicationEvent 时通过 TransactionalApplicationListenerSynchronization.register 将监听器事件注册给了事务管理器,之后的触发逻辑就由事务管理器来负责了 TransactionalApplicationListenerSynchronization最后我们来看看 TransactionalApplicationListenerSynchronization 的 register 方法 123456789101112131415161718public static &lt;E extends ApplicationEvent&gt; boolean register( E event, TransactionalApplicationListener&lt;E&gt; listener, List&lt;TransactionalApplicationListener.SynchronizationCallback&gt; callbacks) { // 事务处于执行状态 if (TransactionSynchronizationManager.isSynchronizationActive() &amp;&amp; TransactionSynchronizationManager.isActualTransactionActive()) { TransactionSynchronizationManager.registerSynchronization(new PlatformSynchronization&lt;&gt;(event, listener, callbacks)); return true; } // 和响应式事务有关 else if (event.getSource() instanceof TransactionContext txContext) { TransactionSynchronizationManager rtsm = new TransactionSynchronizationManager(txContext); if (rtsm.isSynchronizationActive() &amp;&amp; rtsm.isActualTransactionActive()) { rtsm.registerSynchronization(new ReactiveSynchronization&lt;&gt;(event, listener, callbacks)); return true; } } return false;} 我的环境使用的是普通的事务,走第一个 if 判断.如果事务处于执行状态,则将监听器事件包装起来并注册到事务管理器中 123456789public static void registerSynchronization(TransactionSynchronization synchronization) throws IllegalStateException { Assert.notNull(synchronization, &quot;TransactionSynchronization must not be null&quot;); Set&lt;TransactionSynchronization&gt; synchs = synchronizations.get(); if (synchs == null) { throw new IllegalStateException(&quot;Transaction synchronization is not active&quot;); } synchs.add(synchronization);} 后面的触发实际就有 spring 事务管理了 再看看提交给事务管理器的 PlatformSynchronization 1234567891011121314151617181920212223242526272829303132333435private static class PlatformSynchronization&lt;AE extends ApplicationEvent&gt; extends TransactionalApplicationListenerSynchronization&lt;AE&gt; implements org.springframework.transaction.support.TransactionSynchronization { public PlatformSynchronization(AE event, TransactionalApplicationListener&lt;AE&gt; listener, List&lt;TransactionalApplicationListener.SynchronizationCallback&gt; callbacks) { super(event, listener, callbacks); } @Override public void beforeCommit(boolean readOnly) { // 事务提交前 if (getTransactionPhase() == TransactionPhase.BEFORE_COMMIT) { processEventWithCallbacks(); } } @Override public void afterCompletion(int status) { TransactionPhase phase = getTransactionPhase(); // 事务提交后 if (phase == TransactionPhase.AFTER_COMMIT &amp;&amp; status == STATUS_COMMITTED) { processEventWithCallbacks(); } // 事务回滚 else if (phase == TransactionPhase.AFTER_ROLLBACK &amp;&amp; status == STATUS_ROLLED_BACK) { processEventWithCallbacks(); } // 无论如何都执行 else if (phase == TransactionPhase.AFTER_COMPLETION) { processEventWithCallbacks(); } }} 刚好处理了 @TransactionalEventListener 中 phase 的四个配置项 而它实现的接口 TransactionSynchronization 定义了事务的生命周期钩子方法 123456789101112131415161718192021222324252627282930313233343536public interface TransactionSynchronization extends Ordered, Flushable { int STATUS_COMMITTED = 0; int STATUS_ROLLED_BACK = 1; int STATUS_UNKNOWN = 2; @Override default int getOrder() { return Ordered.LOWEST_PRECEDENCE; } default void suspend() { } default void resume() { } @Override default void flush() { } default void beforeCommit(boolean readOnly) { } default void beforeCompletion() { } default void afterCommit() { } default void afterCompletion(int status) { }} 最后本文简单阐述了 @EventListener 和 @TransactionalEventListener 的使用场景,以及区别,尝试结合源码来说明事件的发布流程","link":"/chat/2025/08/event-listener/index.html"},{"title":"珂朵莉树","text":"前言不知道你是否听说过一个叫珂朵莉树(ODT, Old Driver Tree)的”数据结构“，它有个名称：“老司机树” 珂朵莉这个人物名称来源于日漫《末日时在做什么？有没有空？可以来拯救吗？》中的女主角，之所以叫珂朵莉树是因为被发布在 codeforces 算法题背景人物中有珂朵莉 在洛谷的题目CF896C中文翻译中，不仅把题目背景介绍移除了，连背景图也移除了，真是本末倒置 wiki 上对它的解释如下 老司机树，ODT(Old Driver Tree)，又名珂朵莉树（Chtholly Tree)。起源自 CF896C。 会用 STL 的 set 就行。 把值相同的区间合并成一个结点保存在 set 里面。 这是一个基于 TreeMap 的暴力算法，用来解决一些区间赋值操作的数据结构题。在相对随机的数据场合，也堪堪能通过 几年前我刚在网络上偶然刷到相关的文章时，那时候我能找到的相关题解都还是基于 cpp 语言的，对 cpp 不熟悉成了我放弃理解这个算法的拦路虎 在现在大语言模型的帮助下，我重新理解了这个算法，并按自己的理解实现了 Java 版本和 Go 版本的 ODT 我的理解先看题目要求 分别要求四个操作 区间加 区间赋值 区间排序 区间求和 我认为 ODT 树实现的核心就是，执行操作前，先按要执行的区间进行拆分 比如，区间 [left, right] 加操作，先以拆成以 left 起始的区间，再拆 right + 1 （以 right + 1为起始，就意味着必定有一个区间以 right 结尾）为起始的区间 拆完之后，就意味着在 [left, right] 这个大区间中，刚好会存在一个或者多个子区间在这个大区间内，而且这些子区间可以保证是大区间的子集 这样处理之后，只需要快速遍历这个大区间的所有子区间，就可以很方便地执行上面的四个操作 对于操作1，每个子区间都加上给定的 val 对于操作2，直接移除所有的子区间，并新增一个区间，范围就是 [left, right]，值为给定的值 对于操作3，稍稍复杂一些，需要把所有子区间按 val 排序，然后找第 k 个值所在的区间 对于操作4，同样是遍历整个子区间，使用快速幂计算模 对于查找子区间，由于 TreeMap 的特性，只需要$$O(log(n))$$的时间复杂度即可定位到起始区间 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132public class ODT { private static final class TreeNode implements Comparable&lt;TreeNode&gt; { private final int left; private final int right; private long val; private TreeNode(int left, int right, long val) { this.left = left; this.right = right; this.val = val; } private TreeNode(final int left) { this.left = left; this.right = Integer.MIN_VALUE; } @Override public int compareTo(final ODT.TreeNode o) { return Integer.compare(this.left, o.left); } @Override public boolean equals(final Object o) { if (o == null || getClass() != o.getClass()) { return false; } final TreeNode treeNode = (TreeNode) o; return left == treeNode.left &amp;&amp; right == treeNode.right &amp;&amp; val == treeNode.val; } @Override public int hashCode() { return Objects.hash(left, right, val); } } private final TreeSet&lt;TreeNode&gt; tree = new TreeSet&lt;&gt;(); public ODT(long[] data) { Objects.requireNonNull(data, &quot;input data is null&quot;); final int n = data.length; if (n == 0) { return; } var pre = 0; for (int i = 1; i &lt; n; i++) { if (data[i] != data[i - 1]) { this.tree.add(new TreeNode(pre, i - 1, data[i - 1])); pre = i; } } this.tree.add(new TreeNode(pre, n - 1, data[n - 1])); } private void split(final int index) { final var floor = tree.floor(new TreeNode(index)); if (floor == null || floor.left == index || floor.right &lt; index) { return; } tree.remove(floor); tree.add(new TreeNode(floor.left, index - 1, floor.val)); tree.add(new TreeNode(index, floor.right, floor.val)); } public void add(final int left, final int right, final long val) { this.split(right + 1); this.split(left); tree.subSet(new TreeNode(left), new TreeNode(right + 1)) .forEach(node -&gt; node.val += val); } public void assign(final int left, final int right, final long val) { this.split(right + 1); this.split(left); tree.subSet(new TreeNode(left), new TreeNode(right + 1)) .clear(); tree.add(new TreeNode(left, right, val)); } public long kth(final int left, final int right, final int k) { this.split(right + 1); this.split(left); final var treeNodes = tree.subSet(new TreeNode(left), new TreeNode(right + 1)); final var sortTreeNodes = new ArrayList&lt;TreeNode&gt;(treeNodes.size()); sortTreeNodes.addAll(treeNodes); sortTreeNodes.sort(Comparator.comparingLong(node -&gt; node.val)); var kth = k; for (final var sortTreeNode : sortTreeNodes) { final var length = sortTreeNode.right - sortTreeNode.left + 1; if (length &gt;= kth) { return sortTreeNode.val; } kth -= length; } return -1; } public long powerSum(final int left, final int right, final long x, final long y) { this.split(right + 1); this.split(left); var ans = 0L; for (final var treeNode : tree.subSet(new TreeNode(left), new TreeNode(right + 1))) { final var length = treeNode.right - treeNode.left + 1; ans = (ans + this.pow(treeNode.val, x, y) * (length % y) % y) % y; } return ans; } private long pow(long base, long pow, final long mod) { var ans = 1L; base %= mod; while (pow &gt; 0) { if ((pow &amp; 1) == 1) { ans = (ans * base) % mod; } base = (base * base) % mod; pow &gt;&gt;= 1; } return ans; }} 这里也有一份 Go 的实现，但 Go 的标准库中并没有 TreeMap 的实现，所以依赖了一个第三方库 gods，但 codeforces 不接受引入第三方库，以下代码提交后直接提示编译错误了，但实际应该是可行的实现（可以跑过给出的测试用例，虽然只有两个测试用例） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import ( &quot;fmt&quot; &quot;github.com/emirpasic/gods/trees/redblacktree&quot; &quot;slices&quot;)type ODT struct { tree *redblacktree.Tree}type treeNode struct { left int right int val int64}func NewODT(data []int64) *ODT { tree := redblacktree.NewWith(func(a, b interface{}) int { treeNode1 := a.(*treeNode) treeNode2 := b.(*treeNode) return treeNode1.left - treeNode2.left }) if len(data) == 0 { return &amp;ODT{tree} } pre := 0 n := len(data) for i := 1; i &lt; n; i++ { if data[i] != data[i-1] { tree.Put(&amp;treeNode{pre, i - 1, data[i-1]}, struct{}{}) pre = i } } tree.Put(&amp;treeNode{pre, n - 1, data[n-1]}, struct{}{}) return &amp;ODT{tree}}func (o *ODT) split(index int) { key, ok := o.tree.Floor(&amp;treeNode{left: index}) if !ok || key.Key.(*treeNode).left == index || key.Key.(*treeNode).right &lt; index { return } node := key.Key.(*treeNode) o.tree.Remove(node) o.tree.Put(&amp;treeNode{node.left, index - 1, node.val}, struct{}{}) o.tree.Put(&amp;treeNode{index, node.right, node.val}, struct{}{})}func (o *ODT) foreach(start, end int, handle func(*treeNode)) { cur, _ := o.tree.Floor(&amp;treeNode{left: start}) iterator := o.tree.IteratorAt(cur) for { node := iterator.Key().(*treeNode) if node.left &gt; end { break } handle(node) if !iterator.Next() { break } }}func (o *ODT) Add(left, right int, val int64) { o.split(right + 1) o.split(left) o.foreach(left, right, func(node *treeNode) { node.val += val })}func (o *ODT) Assign(left, right int, val int64) { o.split(right + 1) o.split(left) var deleteNodes []*treeNode o.foreach(left, right, func(node *treeNode) { deleteNodes = append(deleteNodes, node) }) for _, node := range deleteNodes { o.tree.Remove(node) } o.tree.Put(&amp;treeNode{left, right, val}, struct{}{})}func (o *ODT) Kth(left, right, kth int) int64 { o.split(right + 1) o.split(left) var sortNodes []*treeNode o.foreach(left, right, func(node *treeNode) { sortNodes = append(sortNodes, node) }) slices.SortFunc(sortNodes, func(a, b *treeNode) int { return int(a.val - b.val) }) for _, node := range sortNodes { length := node.right - node.left + 1 if kth &lt;= length { return node.val } kth -= length } return -1}func (o *ODT) PowerSum(left, right int, x, y int64) int64 { o.split(right + 1) o.split(left) ans := int64(0) o.foreach(left, right, func(node *treeNode) { ans = (ans + ((int64(node.right-node.left+1)%y)*pow(node.val, x, y))%y) % y }) return ans}func pow(base, pow, mod int64) int64 { ans := int64(1) base %= mod for pow &gt; 0 { if (pow &amp; 1) == 1 { ans = (ans * base) % mod } base = (base * base) % mod pow &gt;&gt;= 1 } return ans} 总结ODT 树不算是一个多高级的数据结构，甚至也不算一个数据结构，强依赖了 TreeMap 我之所以有想法去探讨这个算法，应该是因为我们都喜欢珂朵莉吧","link":"/chat/2025/08/odt/index.html"},{"title":"记一次域名证书自动续签","text":"前要我很早之前就使用了 acme.sh 脚本自动化更新域名 但我并不是先知道 acme.sh, 而是从某一个技术博主的推荐中使用的一个第三方提供免费域名签发的平台, 叫 freessl.cn 这个网站更新使用的是也是 acme.sh, 只不过证书的签发平台是它自己, 而不是我们熟知的 ZeroSSL, Let's Encrypt 问题当前的免费证书签发有效期都是三个月, 我经常发现我配置在服务的 acme.sh 自动续签不起作用 home.wuhunyu.top 域名关联了我许多服务, 这个域名的证书过期, 引发了许多问题 nginx 以改域名申请的证书的反向代理通通出错了 为又拍云 CDN 配置的回源证书过期导致博客的部分图片无法访问 解决今天抽了点时间, 准备研究一下 acme.sh 目标 证书到期前自动续签 续签之后证书相关文件复制到指定的路径 续签并复制后, 重载 nginx 服务以重新加载新证书 方案证书签发平台我的服务器(国内)无法访问 ZeroSSL, 换成了 Let's Encrypt 之后是没问题的 续签机制acme.sh 支持多种自动续签机制, 我是在阿里云购买的域名, 解析管理也在阿里云 可以通过 DNS 验证的方式来实现自动验证 操作步骤实际的操作步骤主要分为两步 申请证书. 此操作只需要操作一次即可, 后续通过 cron 定时任务定期更新即可, 无需人工干预 复制证书并重载 nginx. 此操作也只需要手动操作一次, acme.sh 会记住这个动作, 并在续签之后自动执行 实操安装 acme.sh123456# 安装命令$ curl https://get.acme.sh | sh -s email=my@example.com# 安装完毕之后, 可以查询 acme.sh 添加的定时任务$ crontab -l43 0 * * * &quot;/home/xxx/.acme.sh&quot;/acme.sh --cron --home &quot;/home/xxx/.acme.sh&quot; &gt; /dev/null 阿里云 ak 申请 &amp; 配置针对阿里云的域名, 具体操作如下 修改 ~/.acme.sh/account.conf, 添加以下环境变量 12export Ali_Key=&quot;&lt;key&gt;&quot;export Ali_Secret=&quot;&lt;secret&gt;&quot; 阿里云 ak 申请地址, 需要给 AliyunDNSFullAccess 权限 申请证书时, 额外指定 --dns dns_ali 即可 申请证书 我们以 baidu.com 这个域名为例, 当然我没有管理 baidu.com 这个域名的权限, 下面只是举例而已 12# 申请证书, 由于添加 TXT 记录到被感知需要时间, 所以有可能会失败几次, 慢慢等待它重试即可$ ~/.acme.sh/acme.sh --issue -d baidu.com --dns dns_ali --debug 复制证书 &amp; 重载 nginx 假设我们需要将证书复制到 ~/cert/baidu.com 目录下 1234$ ~/.acme.sh/acme.sh --install-cert -d baidu.com --ecc \\--key-file ~/cert/baidu.com/key.pem \\--fullchain-file ~/cert/baidu.com/fullchain.pem \\--reloadcmd &quot;service nginx reload&quot; 这样配置之后, 就可以全自动续签证书 &amp; 重载 nginx 了","link":"/chat/2025/12/acme-sh/index.html"},{"title":"2026 年第一次相亲","text":"今天是元旦节假日的第二天, 外面寒风凛冽, 气温还不到 10℃ 一天没出门了, 不过晚上有个约会, 这会儿我正在打理已经炸毛的头发 准备铃铃铃~, 是老妈打过来的微信电话 “现在可以出发了”, 电话那头有点急切的声音 今晚的约会是一个相亲饭局, 我了解到的相亲对象(下面统一叫“女方”)那边的信息不多 比我小 2 岁 元旦父母没空, 元旦第二天要上班, 所以约的是元旦第二天的晚上 在老家上班 从上面的消息中, 我猜测是在老家上的一个比较忙的工作, 元旦只有一天假期 老妈传递的信息缺失的很厉害, 不过也不是第一次了 父辈们安排的见面, 如果不太费时费钱的话, 就顺着他们的意思去做吧, 这样也能少些吵架 再加上, 我的年纪也不少了, 确实该考虑结婚的事情了 不过, 我是在外地上班, 元旦的第三天中午就要去赶火车, 女方假期这么少的话, 很难有后续, 本身也是异地恋 “走啊”, 老爸还在家里等我 “马上”, 我知道老妈交代老爸出门带一些现金, 肯定是要给女方那边红包的 出门饭局的馆子是我家定好的, 离我家不太远, 一公里的样子 有一瞬间想过要不要骑车去, 万一饭局完了之后需要带女方出去玩呢, 不过这个想法马上就被迎面的冷风吹散了 “到了没有?”, 老妈在我们赶路的时候又打了一遍老爸的电话 刚好我们到了饭馆的门口, 一对中年夫妇在门口等着, 我暗猜这就是女方的家属 等待定的饭局在二楼, 老爸和我上楼之后, 就把楼下站着的女方爸妈叫了上来 女方的爸妈都是很健谈的人 女方爸妈, 中介, 我爸妈聊天这会儿, 我把碗筷用热茶水冲洗了一遍, 然后挨个倒茶 等待女方本人来的时间, 基本都是这些老一辈的人在聊天, 我本身不算是一个话少的人, 但这个场合, 我还是安静一些比较合适, 过度地表现自我会让我怀疑这个正在表现自己的人还是自己吗 等待的时间很无聊, 把在场人的碗筷都洗了一遍之后, 看哪个长辈的被子快没茶了, 我就去满上 吃饭女方也到齐了, 老妈通知厨房开始做菜 虽然一开始就有预期了, 但全部的人都到齐之后, 人还真不少, 我家 3 个人, 两个中介, 女方家里那边来了 5 个人, 如果算上女方姐姐的两个婴儿, 一共 12 个人 一张大圆桌, 坐满了人, 还是挺壮观的, 这个热闹程度在我家这种四口之家不多见 女方来了之后, 就在帮她姐姐给婴儿喂奶 我和她是这场饭局的主要人物 我喵了女方一眼, 挺高的, 比起我的偏瘦的脸型来说, 她的脸型会显得更大一些, 脸上还有几颗痘痘印, 还在我的审美范围内 我本身长相很一般的人, 自我评价中下吧, 再加上裸身高 164 cm, 对自己相亲能找到的对象有一个基本的认知 我给女方姐妹俩倒了两杯白开水, 年轻人应该都不喜欢喝茶水 吃饭期间, 我从父辈们的谈话之间了解更多的信息 女方姐姐和我同岁 女方本人比我小 5 岁 女方在家里做会计 女方元旦有三天假 和我之前了解的信息有出路 差 5 岁的话, 老实说, 我有点想要退缩了 虽然网友们经常说要找 18 岁的妹妹, 但我自我怀疑: 自己比人家大这么多, 人家为什么会接受和一个老帮菜做男朋友甚至结婚呢 有钱 有颜 能提供情绪价值 这三点我都没有, 所以我很清楚自己的定位 如果想要找一个一起度过后半生的对象, 小 5 岁有些不太现实了 她等得起, 我也没多少时光就步入 30 大关了 我个人的期望是 ±2, 不过经历过社会磨砺的人, 对另一半的期望都会更偏向于现实条件, 感性会少一些 我为什么会想要步入结婚大军, 扪心自问 想要体验一下男女之事 看到别的情侣, 也会想如果不是一个人会不会过的更幸福 身边的同学朋友一个一个走进了婚姻 父母催婚 父母的压力其实是最小的, 所以我并不反对老妈安排的相亲, 因为我的圈子里基本遇不到适龄的未婚女性 我反对的是父母把这次相亲办成了两家人见面. 这没必要, 还费钱, 成功率也不高 我正犹豫着找个机会约女方出去单独聊一聊 聊中介站出来叫我约女方出去走一走 顺水推舟, 饭局上, 女方的爸爸出去了抽烟, 我向女方的妈妈说了声“那我带你女儿出去聊一会儿”, 就邀请女方出去玩了 我原本的计划是, 先自我介绍, 然后再问下是否有意向相处下去 女方回答的有点模糊, 但也表达了自己还早, 可能以后也不想结婚 我问, “你是第一次参加相亲吗?” 她答, “不算是第一次吧” 其实她说没想法的那一刻, 我也没啥好了解的了. 我不是一个死缠烂打的人, 如果对方没有一点想法的话, 即便我能通过契而不舍地追求得到这个婚姻, 但这样取得的婚姻到底会不会是幸福的呢 后面的聊天就有些尴尬了, 下来的时候长辈还起哄说要加微信, 然后第二天早上还能约一起见个面 我直接摊牌了, “既然你没有想法, 那我们就不用加微信了” 我觉得有时候决绝一点是对两边的负责 在外面转了一圈, 我们就准备回去了 进饭馆的门之前, 女方问我, “如果长辈们问起来没加微信怎么办?” 我也犹豫了, 我知道穿帮肯定是迟早的事 从口袋摸出来手机, 摆弄了一会儿终究还是没有说出来加微信 返回饭局上楼之后, 爸妈都不在. 此时我还没意识到他们去做什么了 我重新坐回座位, 女方的父亲和我聊起来他以前在深圳时候的一些事情 物是人非, 他说的很多地方名, 我是没听说过的, 不过和这些长辈聊一聊这些也挺好的, 如果话题又扯到了相亲上, 我也不一定能接得住 和女方的父亲聊了好一会儿, 我靠近女方, 和她说, “你爸爸还是很热情的, 加油抗争”, 不知道她听清楚了没 之后听见老妈的声音, 我就起身出去. 老妈直接塞了一个红包给我, 说是要给女方 边上除了老妈还有其他人在, 一时之间我没好意思说我们已经结束了 半推半就下, 我还是把红包给了女方 女方估计也是第一次见这种场面, 推脱了几次, 在身边人的起哄下接受了红包 这会儿没加到女方微信, 连悄悄话都没机会说了 女方的父母都没说什么, 也是很期望能成一家人吧 红包发完, 也差不多该结束了 女方的姐姐两个婴儿回去不方便, 饭局上有人提议我家开车送她们回去 于是我一个人回家拿钥匙去取车 取车路上我在想, 不能一错再错了, 一会儿回去, 问一下女方的想法, 要不要当场说明我们没想法继续下去 沉默取车回来之后, 女方正在和她姐姐逗小孩玩 我靠近她, 说了声, “要不要现在说出来” 她回了我一句, 但我没听清楚是什么 沉默 我想起来让子弹飞有句台词, “哭, 哭也算时间哦” 此刻, “沉默也算是做出了决定”, 是对我的写照 很快, 我们就各回各家了 责怪老妈说我为什么不和女方招呼说拜拜 这会儿只有中介和老妈在, 我也瞒不住, 直接说已经凉了 老妈的表情瞬间变了 中介说, “你在微信上和人家再聊一聊, 男生要脸皮厚一点” 我说没加微信, 那会儿中介和老妈觉得天都塌了 我和她们解释再多都没用, 中介说她问女方父母要女方的电话, 然后我再加她的微信 中介说她自己, 21 岁时她老公见面的第二天就直接跑她家里去找她了 我很想说现在的恋爱和她们那时候不一样了, 但无法说服她们 加微信已经是晚上九点半了 我加了女方的微信, 说我这边失守了 再加微信, 我的感受也是很复杂 再次询问了是否还有机会, 我还有一上午在老家 女方回复说, “她想要睡懒觉” 我觉得如果对一个人有一点想法, 即便说了要睡懒觉, 也会表达可以几点之后约见面吧, 更何况我已经表明了自己隔天下午就要去赶火车 事已至此, 我直接问, “那我就理解为拒绝了哈” 成年人之间沟通真的没必要扭扭捏捏, 这是在相亲, 而不是在谈恋爱, 我作为男方需要确认对方也有意向才能迈出下一步 老爸老妈老一辈的观念是, 人家女孩子不可能来追我, 需要我主动一些, 不要因为一点挫折就放弃, 我也不小了, 要抓住机会 我寻思着, 我并没有反对相亲, 也不是不想配合老爸老妈的安排 问题是在明确得知对方没有意向向后发展的前提下, 我还挂着一个异地恋的 debuff, 真没必要硬追 如果当时, 女方说有想法向后发展, 或者她说可以试试, 我都不可能不加人家微信 我希望我的婚姻不是单纯地因为物质条件而走到了一起 第二天我向老爸老妈表明了微信上的聊天结果, 他们还是希望能继续推进 老实说, 我已经不再去想打扰对方了 算上微信上的答复, 我算不算已经被拒绝了两次 强扭的瓜不甜, 虽然两方的家长都很积极想要推进关系, 我家里也没什么财产好继承, 不想给自己加以后离婚的风险","link":"/chat/2026/01/blind-date/index.html"},{"title":"VisualVM 远程连接","text":"VisualVM 远程连接简介 VisualVM is a visual tool integrating commandline JDK tools and lightweight profiling capabilities.Designed for both development and production time use.谷歌翻译：VisualVM 是一个集成了命令行 JDK 工具和轻量级分析功能的可视化工具。 专为开发和生产时间使用而设计。 服务端jstatd 授权文件 - jstatd.all.policyjdk8123grant codebase &quot;file:${java.home}/../lib/tools.jar&quot; { permission java.security.AllPermission;}; jdk9+1234567grant codebase &quot;jrt:/jdk.jstatd&quot; { permission java.security.AllPermission;};grant codebase &quot;jrt:/jdk.internal.jvmstat&quot; { permission java.security.AllPermission;}; 启动 jstatd tool12# -p 指定使用的端口，默认使用的是 1099 端口jstatd -J-Djava.security.policy=&lt;jstatd.all.policy文件的路径&gt;/jstatd.all.policy -J-Djava.rmi.server.hostname=&lt;对外暴露的主机ip&gt; -p 1099 开放端口 1099 端口：jstatd 工具默认使用的是 1099 端口 其他：jstatd 工具还会使用一个端口，需要在运行期间查询获的。可通过 netstat 指令查询获取 1234&gt; netstat -tanp | grep jstatdtcp6 0 0 :::1099 :::* LISTEN 4046836/jstatd tcp6 0 0 :::40499 :::* LISTEN 4046836/jstatd tcp6 0 0 xx.xx.xx.xx:40499 xx.xx.xx.xx:60082 ESTABLISHED 4046836/jstatd 那么除了 1099 端口以外，还需要开放 40499 端口 注意：40499 端口是在 jstatd 运行之后查询获得的，每次重启 jstatd 都可能会不一样 客户端安装 VisualVm12# mac 环境brew install visualvm 远程连接服务端 选择需要查看的 JVM 进程 查看 GC 情况 CPU 内存指标需要使用 JMX，使用 JMX 需要服务端的 JVM 应用开启相关配置","link":"/jvm/2024/02/VisualVM%20%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/index.html"},{"title":"面经 - 一叶智能一面技术面","text":"一叶智能 – 一面面经公司简介听面试官说，以前本部是在北京，然后在江苏也有分布，现在在深圳的发展挺好的，总部正在往深圳这边迁移 面试官问空窗期 saas服务发布是怎么做的 项目中的redis主要用来做什么 项目中的gateway主要用来做什么 表a有一个复合索引 (a, b, c)，假设查询语句是 select b from a where a = ? and c = ? group by b，索引使用情况 大表的冷数据要通过什么方式迁移 大表索引变更要怎么做 threadlocal在多线程中如何使用 mybatis的拦截器有没有用过 myabtis中如果想要把一个对象保存为json格式要怎么做 项目中遇到的难点 上一份工作除了技术上，你觉得还给你了什么提升 未来的发展方向 反问贵公司目前的项目上使用到了哪些技术 贵公司的关系型数据库为什么选择了 postgresql，而不是 mysql","link":"/mianjing/2024/02/yishuyiye/index.html"},{"title":"面经 - 深圳同行者","text":"深圳同行者 – 技术面面经情报 约的面试是下午 3 点，签到的时候就发现有好些人都是去面试这家公司的，有 Java，有测试，有实习。 到了之后公司门口的会客室一堆人在做笔试题，面试 Java 的人比较多，我笔试的时候对面坐的人也是面试 Java的 面试有两轮，第一轮问八股多一些，第二轮会结合场景问实际应用 笔试 三道算法题，一道sql题 算法题没有给出方法签名 有 1000 个数，取值取值范围是 [0, 999]，其中有两个数出现了两次，要求找出这个重复的数 sql 题，题目比较长，题目就不写了，需要用到 group by，having 以及 聚合函数 设计一个程序，可以让两个线程交替打印从0到n的数字 应该是 LeetCode 原题，这会儿没找到是哪个原题，但印象中我做过 打印26个小写字母的所有不重复排列组合，每个字母只能使用一次，类似于a，ab，abc这种都算是一个合法的组合 一轮技术面面试官问数据冷热分离是怎么做的 开源项目的 star 是多少 笔试第四题的结果数量有多少 二轮技术面 二轮面试的面试官就在面试的等待区面试，其他人是可以听见面试的过程 不过二轮面试的面试官对每个人问的问题不尽相同，我面试之前这个面试官对另一位面试者问了笔试的第 3 题，也稍微深入问了下 gc 对于公司批量通知十多个面试者同一时间段来笔试的行为我心里有点不爽，对于二轮技术面的面试官直接在面试等待区面试的行为我表示贵公司没有对面试者给到基本的尊重 面试官问谈谈你对 CAS 的理解 这里我回答，基于 CAS 的轻量级锁比重锁更节省系统资源。然后被面试官打脸了，后面面试官一步步引导我玩正确的方向回答 你是如何分析 jvm 的 dump 文件的 笔试第一题我使用数组计数的方式来做的，也可以换成使用 map 来计数，面试官要求使用 O(1) 空间复杂度求解，需要现场写，面试官就在旁边看着 有没有自己搞过一些开源的工具类 开源项目中使用的 netty 是做什么用的 有没有在 gateway 中进行限流 是否有独立负责的微服务项目 最近一个项目的架构图，需要现场画出来 有一个接口需要做限流，每天每个用户有额定的调用次数，然后随后系统还有一个总的调用次数，调用次数达到这个用户的调用次数或者达到总的调用次数上限都要做限流，你的方案是什么 进度二面的面试官说，差不多两天会给通知","link":"/mianjing/2024/03/tongxingzhe/index.html"},{"title":"面经 - 行云绽放","text":"行云绽放 – 技术面面经情报 公司有三个产品，按我的理解，是比较偏向安全类的产品。比如，数据库操作层面的安全，服务器操作层面的安全 对开发而言，要求会网络编程 面试流程上分为笔试 + 一面（技术面）+ 二面（偏向主管面，偶尔也问点技术问题） 笔试主要说说最后的三个大题 写一个延迟加载的单例 给一个byte数组，要求写入指定文件，如果文件已经存在，则采用追加写，否则创建该文件并写入字节数组 有一个程序运行有一些问题，具体的问题如下，分别写出排查和定位的过程 CPU占用过高 出现了OOM异常 出现了open too many files异常 其他笔试还有两个填写关键代码的算法，分别是冒泡排序和二分查找 笔试中除了第二题考了字节流，选择题中也有考字节流 一轮技术面Java 常用的数据结构，说说你对 List，Queue 的看法 线程池的参数 创建线程的方法 如果有一批任务被提交给线程池，线程池是怎么分排线程的 Linux 常用命令 查询指定时间范围内的日志的 Linux 命令 jvm 内存结构 对于这种文件最好和面试官说明是想要问 java内存模型 还是 jvm运行时内存区域 分布式锁是使用redis来实现的吗 jdk 原生自带了 nio 的一套 api，为什么还要用 netty 呢，好处是什么 二轮主管面如果和其他同事发生了冲突，不一定是字面意义上的冲突，也可能是观念上的不认同，具体的场景比如，测试反馈了一个bug，但你不认为这是一个bug，你会怎么做 工作中有没有遇到什么难题，又是怎么解决的 你觉得自己有什么优点 你觉得自己有什么缺点 你期望一个怎么样的工作环境","link":"/mianjing/2024/03/xingyunzhangfang/index.html"},{"title":"面经 - 一临云科技","text":"一临云科技 – 一面面经公司简介一家医疗行业的公司，在深圳和成都有分布，深圳这边的开发大概二十多人，测试都在成都，有交流通过线上会议的方式进行 面试官问自我介绍 数据冷热分离是怎么做的 对线程池的理解 redis 在项目的用处 redis 分布锁的实现 gateway 在项目中的用处 要怎么保证 mq 消息不丢失 sql 优化 索引失效的情况有哪些 仔细谈谈最左匹配原则 回表是什么 反问深圳这边的开发规模 开发团队 20 人左右 面试的流程，是否还有二面 线上一面，最好线下二面","link":"/mianjing/2024/03/yilinyun/index.html"},{"title":"面经 - 易世通达(跨境电商)技术面","text":"易世通达(跨境电商) – 技术面面经前言不知道其他人是怎么理解电商的，我以前一直以为电商这一行的互联网是自己搞购物平台，但这家公司和我说，公司主要做海外的电商业务，对商品的上下架，机器人客服啥的，这才是电商 公司简介强调了加班，周末有事情也要加班，不过公司提供住宿来着，就是不知道住宿的环境如何 一面面试官问线程池的参数有哪些 如果要实现核心线程也能被回收，要怎么做 es 是否了解 HashMap的put流程 链表变红黑树的条件 线程安全的map 简单说说ConcurrentHashMap是怎么保证线程安全的 你对b+树的了解 一道需要分组聚合查询的sql题，比较简单，面试官现场出题需要现场写查询sql 二面面试官问有没有遇到过tomcat线程阻塞的场景 gateway使用的是容器是什么，tomcat还是netty 举一个mysql死锁的例子 nacos一致性协议有哪些 gRPC是否了解 http2是否了解 quartz框架是否可以通过手动修改cron的方式不停机更新cron xxl-job是否有了解","link":"/mianjing/2024/03/yishitongda/index.html"},{"title":"面经 - 江西燃点","text":"江西燃点 – 面经情报面试是混面，技术面，主管面(一般针对个人经历，发展提问的面试我称之为主管面) 公司运营的项目目前趋于稳定，期望开发者拥有大数据方面的技能，可以给一定的时间给开发者来转这个方向 技术面面试官问二分法适用的场景 分布式事务中，有八个 rpc 修改类接口需要同步调用，现在发现整个事务执行效率较低，该如何优化 spring 中的循环依赖是如何解决的 有没有在网关层做过鉴权 ThreadLocal 使用场景，以及可能遇到的问题 有没有做过 sql 调优类的实践工作 有没有做过数据权限，要怎么实现 其他面试结果在本周内会给回复 这算是第一个打开了我个人博客的面试官，虽然我的个人博客并没有很么干货，感动ing，呜呜呜～～","link":"/mianjing/2024/03/jiang-xi-ran-dian/index.html"},{"title":"面经 - 卓讯信息技术","text":"卓讯信息技术 – 技术面面经公司简介公司简介上写的时候双休，每天工作7小时，但hr给我的表态是还是得一周工作至少40小时，估计多的5小时是不算加班费的 技术面的面试官给我的信息是，工作会很忙，周六也要加班，大概可以理解为 996 吧，hr给我的反馈是周末加班算调休，但我猜测有概率是不给任何薪水或者调休补偿的 面试前公司需要收集的信息有点越界了，除了要写每家公司的证明人和联系电话，还要求填写自己的优缺点，工作过公司的一些对标企业 公司目前做的项目是一个低代码平台，面向的客户也是政府机关 笔试 笔试题整体偏简单（除了问 Session 失效的 api 以外），题量也不大，主要考 sql 查询 如何使 Servlet 中的 Session 对象失效，然后有四个选项。笔试后我查询了一下，应该选第3个 session.clear() session.remove() session.invalidate() session.stop() 最后有一个编程题，大概的意思是一个对象，有三个属性（id，name，children），然后它是一个树状结构，给一个id给你，要你返回目标对象，题目给出了方法签名，需要补充方法体 123public Map test(List&lt;Map&gt; list, String id) { // TODO 需要面试者填充方法体} 这题我使用广度优先遍历来做的，面试官疑惑怎么还用上队列了，我解释了一番之后，他还是表示没这么复杂 事后我才想总不会是直接循环这个List，然后比较id，而不需要去遍历children来做的吧 技术面面试官问 问了一下我的开源项目以及工作项目，没啥干货 反问我脑海中的低代码平台还是停留在非开发者也能快速上手，但后期维护难的印象。如果客户有需要导入旧数据到低代码平台的需要，要怎么做 面试官的大概意思他们的低代码平台用户还是需要一点代码水平的，针对我说的数据迁移，他说这个业务不在考虑范围内 hr 面主要谈了谈薪水和工作时间，薪水压了我1k，现场给了意向 offer 总结自我感觉这家公司的业务可能并没有涉及分布式微服务之类的，给offer倒是挺快的，算是我来深圳的第一个offer","link":"/mianjing/2024/03/zhuoxun/index.html"},{"title":"基于 langchain4j 的简易 MCP Client","text":"前言这次的定语比较多，又是 简易 又是 client 的 如果你去翻阅 langchain4j 有关 MCP 的文档（点击这里），你会发现有关它的内容比起 RAG 少的可怜 在我看来，应该是以下几个原因有关 langchain4j 并没有实现完整的 MCP 协议。在目前版本（1.0.1）的 langchain4j 中是不存在 MCP Server 这个组件的 langchain4j 对于 MCP Client 的理解和 Function Calling 异曲同工，有一些逻辑在其他模块已经实现了 我其实是有些纳闷的，因为我最初就是想用 langchain4j 把现有的服务构建成一个 MCP Server，至于 MCP Client 我想许多客户端都可以充当这个角色 以上推论主观性很强，如果有错误，可以联系我的邮箱 langchain4j 中的 MCP Client在 langchain4j 中，McpClient 本质还是一个 tool 从官方文档中给的例子中可以看出来一些端倪 12345678910111213141516171819McpTransport transport = new HttpMcpTransport.Builder() .sseUrl(&quot;http://localhost:3001/sse&quot;) .logRequests(true) // if you want to see the traffic in the log .logResponses(true) .build(); McpClient mcpClient = new DefaultMcpClient.Builder() .key(&quot;MyMCPClient&quot;) .transport(transport) .build();McpToolProvider toolProvider = McpToolProvider.builder() .mcpClients(mcpClient) .build();Bot bot = AiServices.builder(Bot.class) .chatModel(model) .toolProvider(toolProvider) .build(); 先是构建了一个基于 SSE 的传输对象 transport，然后封装成一个 McpClient 客户端，再包装成一个工具供应商 McpToolProvider，最后交给 AiServices 构建一个服务组件 AiServices 除了有 toolProvider 属性，还有 tools 属性。而这个 tools 属性就是 Function Calling 用来配置 tool 的 看一下源码（dev.langchain4j.service.tool.ToolService#createContext） 123456789101112131415161718192021222324public ToolServiceContext createContext(Object memoryId, UserMessage userMessage) { if (this.toolProvider == null) { return this.toolSpecifications.isEmpty() ? new ToolServiceContext(null, null) : new ToolServiceContext(this.toolSpecifications, this.toolExecutors); } List&lt;ToolSpecification&gt; toolsSpecs = new ArrayList&lt;&gt;(this.toolSpecifications); Map&lt;String, ToolExecutor&gt; toolExecs = new HashMap&lt;&gt;(this.toolExecutors); ToolProviderRequest toolProviderRequest = new ToolProviderRequest(memoryId, userMessage); ToolProviderResult toolProviderResult = toolProvider.provideTools(toolProviderRequest); if (toolProviderResult != null) { for (Map.Entry&lt;ToolSpecification, ToolExecutor&gt; entry : toolProviderResult.tools().entrySet()) { if (toolExecs.putIfAbsent(entry.getKey().name(), entry.getValue()) == null) { toolsSpecs.add(entry.getKey()); } else { throw new IllegalConfigurationException( &quot;Duplicated definition for tool: &quot; + entry.getKey().name()); } } } return new ToolServiceContext(toolsSpecs, toolExecs);} 逻辑大概是 如果 toolProvider 为空，则只包装了 tools 来源的工具 toolSpecifications 和 toolExecutors 是 tools 属性处理得到的 否则，先将 toolSpecifications 和 toolExecutors 添加到容器中做准备 重点在于 provideTools 方法，它将 McpClient 的 tool 列举出来（通过 MCP 协议的实现 dev.langchain4j.mcp.client.McpClient#listTools）并返回了 ToolProviderResult dev.langchain4j.mcp.McpToolProvider#provideTools(dev.langchain4j.service.tool.ToolProviderRequest, java.util.function.BiPredicate&lt;dev.langchain4j.mcp.client.McpClient,dev.langchain4j.agent.tool.ToolSpecification&gt;) 源码如下 1234567891011121314151617181920protected ToolProviderResult provideTools(ToolProviderRequest request, BiPredicate&lt;McpClient, ToolSpecification&gt; mcpToolsFilter) { ToolProviderResult.Builder builder = ToolProviderResult.builder(); for (McpClient mcpClient : mcpClients) { try { mcpClient.listTools().stream().filter(tool -&gt; mcpToolsFilter.test(mcpClient, tool)) .forEach(toolSpecification -&gt; { builder.add(toolSpecification, (executionRequest, memoryId) -&gt; mcpClient.executeTool(executionRequest)); }); } catch (IllegalConfigurationException e) { throw e; } catch (Exception e) { if (failIfOneServerFails) { throw new RuntimeException(&quot;Failed to retrieve tools from MCP server&quot;, e); } else { log.warn(&quot;Failed to retrieve tools from MCP server&quot;, e); } } } return builder.build();} 这里就可以看出来，其实本质上和普通的 Function Calling 没有什么区别 将 ToolProviderResult 中的 tool 添加到第二步准备好的容器内 剩下就是和 Function Calling 一样，把 tool 的描述交给大模型，让大模型判断应该使用哪些工具来协助完成用户的提问 项目演示目标利用 langchain4j 的 MCP Client 能力，使用 高德地图 和 腾讯地图 的 MCP Server 来实现一个旅行规划 准备 deepseek api key：申请地址 高德地图 api key：申请地址 腾讯地图 api key：申请地址 大模型换用其它的国内模型也没问题，我习惯用 deepseek 了 其实百度地图也提供了它们的 MCP Server，不过它们的 key 申请需要认证开发者 编码pom 依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;top.wuhunyu.mcp&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-mcp-client-example&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;21&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;21&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;spring-boot.version&gt;3.4.0&lt;/spring-boot.version&gt; &lt;langchain4j.version&gt;1.0.1&lt;/langchain4j.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- springboot 版本锁定 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-boot.version}&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;!-- langchain4j 版本锁定 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-bom&lt;/artifactId&gt; &lt;version&gt;${langchain4j.version}&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- web --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j springboot 启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j 响应式编程 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-reactor&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j openai 接入 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-open-ai-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j mcp 接入 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-mcp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- lombok --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; application.yml 配置1234567891011121314151617181920212223242526272829303132spring: application: name: langchain4j-mcp-client-examplelangchain4j: # 大模型 open-ai: chat-model: base-url: https://api.deepseek.com/v1 api-key: [替换成你的 deepseek api key] model-name: deepseek-chat log-requests: true log-responses: true # 流式大模型 streaming-chat-model: base-url: ${langchain4j.open-ai.chat-model.base-url} api-key: ${langchain4j.open-ai.chat-model.api-key} model-name: deepseek-chat log-requests: ${langchain4j.open-ai.chat-model.log-requests} log-responses: ${langchain4j.open-ai.chat-model.log-responses}mcp: # 高德地图 amap: sse-url: https://mcp.amap.com/sse?key=[替换成你的 高德地图 api key] client-name: amap-client client-version: 0.0.1 # 腾讯地图 qq: sse-url: https://mcp.map.qq.com/sse?key=[替换成你的 腾讯地图 api key] client-name: qq-client client-version: 0.0.1 组件配置123456public interface MapClientService { @SystemMessage(&quot;你是一个旅行规划专家，你可以根据用户的需求，为用户规划出旅行路线。&quot;) Result&lt;String&gt; plan(@MemoryId Long userId, @UserMessage String userMessage);} 12345678910111213141516171819202122232425262728// 手动构建 AiService@Bean(&quot;mapService&quot;)public MapClientService mapService( ChatModel chatModel, StreamingChatModel streamingChatModel, ChatMemoryProvider chatMemoryProvider, RetrievalAugmentor retrievalAugmentor, @Qualifier(&quot;amapMcpClient&quot;) McpClient amapMcpClient, @Qualifier(&quot;qqMcpClient&quot;) McpClient qqMcpClient, DateTool dateTool) { final var mcpToolProvider = new McpToolProvider.Builder() .failIfOneServerFails(false) .mcpClients(List.of( amapMcpClient, qqMcpClient )) .build(); return AiServices.builder(MapClientService.class) .chatModel(chatModel) .streamingChatModel(streamingChatModel) .chatMemoryProvider(chatMemoryProvider) .retrievalAugmentor(retrievalAugmentor) .toolProvider(mcpToolProvider) .tools(dateTool) .build();} 只贴出了最关键的部分 注意除了 toolProvider 配置 MCP Client 以外，还添加了一个 tools。这个日期工具只提供了一个基本的当前日期时间功能 1234567891011@Component(&quot;dateTool&quot;)public class DateTool { private final DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;); @Tool(&quot;查询当前的日期时间，以 yyyy-MM-dd HH:mm:ss 格式返回&quot;) public String now() { return LocalDateTime.now().format(dateTimeFormatter); }} 执行结果让我们调用 MapClientService#plan 看看效果。我现在想要知道这个周末在深圳的旅游规划，提示词如下 1请帮我规划一下这周周末深圳的两天旅行规划，要求精确到小时的安排 这是最后的效果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697以下是深圳周末两天（2025年6月14日-15日）的详细旅行计划，具体到每小时的活动安排：---### **第一天：6月14日（星期六）**#### **上午**- **08:00-09:00**：早餐 推荐地点：深圳本地早茶店（如点都德、陶陶居） 活动：品尝广式早茶，如虾饺、叉烧包等。- **09:00-11:00**：深圳湾公园 活动：散步、骑行，欣赏深圳湾海景和红树林生态。- **11:00-12:00**：前往华侨城创意文化园 交通：打车或地铁（约20分钟） 活动：参观艺术展览、创意小店。#### **中午**- **12:00-13:30**：午餐 推荐地点：华侨城创意园内的特色餐厅（如“旧天堂书店”或“Gaga鲜语”） 活动：享用午餐，稍作休息。#### **下午**- **13:30-15:30**：世界之窗 交通：打车或地铁（约15分钟） 活动：参观世界著名景点的微缩模型，拍照打卡。- **15:30-17:00**：欢乐海岸 活动：购物、散步，欣赏水秀表演（如时间合适）。- **17:00-18:00**：前往深圳湾万象城 交通：步行或打车（约10分钟） 活动：购物或休闲。#### **晚上**- **18:00-19:30**：晚餐 推荐地点：深圳湾万象城的高端餐厅（如“炳胜品味”） 活动：享用晚餐。- **19:30-21:00**：深圳湾灯光秀（如开放） 活动：欣赏深圳湾夜景和灯光表演。- **21:00-22:00**：返回酒店 交通：打车或地铁 活动：休息。---### **第二天：6月15日（星期日）**#### **上午**- **08:00-09:00**：早餐 推荐地点：酒店早餐或附近咖啡馆。- **09:00-11:00**：大梅沙海滨公园 交通：打车或自驾（约40分钟） 活动：沙滩漫步、游泳（如天气允许）。- **11:00-12:00**：东部华侨城 活动：参观茶溪谷或大峡谷，体验自然风光。#### **中午**- **12:00-13:30**：午餐 推荐地点：东部华侨城内的餐厅 活动：享用午餐。#### **下午**- **13:30-15:30**：中英街 交通：打车（约30分钟） 活动：购物、了解历史。- **15:30-17:00**：盐田海滨栈道 活动：散步，欣赏海景。- **17:00-18:00**：返回市区 交通：打车或地铁 活动：休息。#### **晚上**- **18:00-19:30**：晚餐 推荐地点：市区内的海鲜餐厅（如“79号渔船”） 活动：享用晚餐。- **19:30-21:00**：深圳平安金融中心云际观景台 活动：俯瞰深圳夜景。- **21:00-22:00**：返回酒店或前往机场/车站 交通：打车或地铁 活动：结束行程。---### **注意事项**1. **交通**：深圳地铁覆盖广泛，建议使用地铁或打车。2. **天气**：提前查看天气，携带防晒或雨具。3. **门票**：部分景点（如世界之窗、东部华侨城）需提前购票。如果需要调整或补充，请告诉我！ 其他问题发现了一处 bug：okhttp 的响应流未关闭，控制台打印了警告日志 122025-06-10 17:37:01.957 [OkHttp ConnectionPool] WARN okhttp3.OkHttpClient - A connection to https://mcp.map.qq.com/ was leaked. Did you forget to close a response body? To see where this was allocated, set the OkHttpClient logger level to FINE: Logger.getLogger(OkHttpClient.class.getName()).setLevel(Level.FINE);2025-06-10 17:37:01.957 [OkHttp ConnectionPool] WARN okhttp3.OkHttpClient - A connection to https://mcp.map.qq.com/ was leaked. Did you forget to close a response body? To see where this was allocated, set the OkHttpClient logger level to FINE: Logger.getLogger(OkHttpClient.class.getName()).setLevel(Level.FINE); 问题源码（dev.langchain4j.mcp.client.transport.http.HttpMcpTransport#execute）如下 12345678910111213141516171819202122232425private CompletableFuture&lt;JsonNode&gt; execute(Request request, Long id) { CompletableFuture&lt;JsonNode&gt; future = new CompletableFuture&lt;&gt;(); if (id != null) { messageHandler.startOperation(id, future); } client.newCall(request).enqueue(new Callback() { @Override public void onFailure(Call call, IOException e) { future.completeExceptionally(e); } @Override public void onResponse(Call call, Response response) throws IOException { int statusCode = response.code(); if (!isExpectedStatusCode(statusCode)) { future.completeExceptionally(new RuntimeException(&quot;Unexpected status code: &quot; + statusCode)); } // For messages with null ID, we don't wait for a response in the SSE channel if (id == null) { future.complete(null); } } }); return future;} Response 对象未在使用后调用其 close 方法关闭连接 修复成如下应该就可以了 123456789101112131415161718192021222324252627private CompletableFuture&lt;JsonNode&gt; execute(Request request, Long id) { CompletableFuture&lt;JsonNode&gt; future = new CompletableFuture&lt;&gt;(); if (id != null) { messageHandler.startOperation(id, future); } client.newCall(request).enqueue(new Callback() { @Override public void onFailure(Call call, IOException e) { future.completeExceptionally(e); } @Override public void onResponse(Call call, Response response) throws IOException { try (response) { int statusCode = response.code(); if (!isExpectedStatusCode(statusCode)) { future.completeExceptionally(new RuntimeException(&quot;Unexpected status code: &quot; + statusCode)); } // For messages with null ID, we don't wait for a response in the SSE channel if (id == null) { future.complete(null); } } } }); return future;} 提了一个 issue，本想要提一个 pr，但 langchain4j 的源码在我本地没跑起来，这个机会让给其他有缘人吧 项目代码GitHub 参考langchain4j 官网","link":"/ai/rag/2025/06/langchain4j-mcp-example/index.html"},{"title":"基于 langchain4j 的简易 RAG","text":"RAG 是什么langchain4j 的官网给了一段通俗易懂的描述 简单来说，RAG 是一种在发送给 LLM 之前，从你的数据中找到并注入相关信息片段到提示中的方法 RAG 分为两个阶段，索引和检索 关于检索部分，下面的项目中将分别使用 langchain4j 的原生组件以及高级用法分别实现 简单说明一下我对这两个阶段的理解 索引 上图是 langchain4j 官网给出的示意图 大概的过程就是 原始的文档（可以是 word，pdf，markdown等格式的文件，即图中的 Document）经过切割之后，得到了一个个的片段（即图中的 Segments） 一般走图中的右侧的流程，将片段经过内嵌模型（即图中的 Embedding Model）计算后，得到内嵌对象（图中的 Embedding），最后将内嵌对象保存到专门的数据库（图中的 Embedding Store）中 过程中出现了几个名词 Document：理解为包含内容的文档，比如 word，pdf，markdown等格式的文件 Segment：由 Document 拆分而来，具体的拆分规则可以自定义 至于为什么需要拆分，因为大模型一次能处理的上下文窗口是有限制的，如果直接把一个包含有 1百万 token 的文档喂给大模型，大模型也吃不消 将文档进行拆分，然后把一个个小的片段保存到数据库中，在需要用到这个文档时，只查询出相关性比较高的 topN 片段给大模型，这样大模型才能处理地过来，也能节省 token 的消耗，以及加快大模型的运行速度。缺点是根据拆分的规则，可能会丢失一部分信息 Embedding Model：内嵌模型。我理解为把自然语言转换成向量的一个模型 为什么要转换成向量？我们需要计算两句话的相关性高不高，需要特殊的算法。比如需要判断土豆和马铃薯的相关性高还是土豆和黄豆的相关性比较高 Embedding：内嵌对象。也就是内嵌模型的输出，也是内嵌数据库的输入 Embedding Store：内嵌数据库。用来保存内嵌对象的地方 许多我们熟悉的数据库也支持保存向量，比如 redis，postgresql。但它们在向量数据的处理方面毕竟没那么专业 检索 在索引完成之后，内嵌数据库中已经保存了一些文档数据 而检索的过程就是将用户的提问在内嵌数据库中查询相关性最高的片段，然后将这些片段和用户的提问聚合，再喂给大模型，由大模型做整合 在 langchain4j 给的图示中，对部分内容进行了省略。我尝试以代码的角度对这个流程进行完善 用户的提问信息被称为一个 Query Query 也可以像文档一样，先进行转换。在 langchain4j 目前的实现中，给出了三种转换 DefaultQueryTransformer：什么都不做 CompressingQueryTransformer：通过大模型对用户的提问进行压缩。这是给大模型的提示词 123456789101112Read and understand the conversation between the User and the AI. Then, analyze the new query from the User. Identify all relevant details, terms, and context from both the conversation and the new query. Reformulate this query into a clear, concise, and self-contained format suitable for information retrieval. Conversation:{{chatMemory}}User query: {{query}}It is very important that you provide only reformulated query and nothing else! Do not prepend a query with anything! 翻译一下是 123456789101112阅读并理解用户与AI之间的对话。然后，分析用户的新查询。从对话和新查询中识别所有相关细节、术语和上下文。将此查询重新表述为清晰、简洁且适合信息检索的独立格式。对话：{{chatMemory}}用户查询：{{query}}非常重要，您只能提供重新表述的查询，不得提供其他内容！不要在查询前添加任何内容！ ExpandingQueryTransformer：和 CompressingQueryTransformer 一样，通过大模型对原有的提问进行扩展。可以指定扩展的数量，默认是 3 条。提示词如下 1234567Generate {{n}} different versions of a provided user query. Each version should be worded differently, using synonyms or alternative sentence structures, but they should all retain the original meaning. These versions will be used to retrieve relevant documents. It is very important to provide each query version on a separate line, without enumerations, hyphens, or any additional formatting!User query: {{query}} 翻译 1234567生成提供的用户查询的{{n}}个不同版本。每个版本应使用不同的措辞，使用同义词或不同的句子结构，但它们都应该保留原始意义。这些版本将用于检索相关文档。非常重要，每个查询版本应单独一行提供，不要使用列举、破折号或任何其他格式化！用户查询：{{query}} 经过转换之后，得到的一个 Query 列表。接下来需要的对每个 Query 进行路由，以选择每个 Query 的检索方式 同样的，路由也有多个实现。langchain4j 中有如下两种 DefaultQueryRouter：按顺序执行全部的检索方式 LanguageModelQueryRouter：问一下大模型选择哪些检索方式 关于检索方式，可以有很多很多。比如查询关系型数据库，web搜索，查询内嵌数据库，查询es数据库等等 相对来说，对于自然语言的处理，内嵌数据库和web搜索是我觉得比较合适的搜索方式 有了路由选择，每个 Query 都能被安排到对应的检索方式去处理。而处理的结果就是一个个 Content 列表 再将这些 Content 列表进行聚合，按相关性倒序排。在 langchain4j 中也提供了两种实现 DefaultContentAggregator：有一个特殊的算法，可以计算每个 Content 的相关性分数 ReRankingContentAggregator：用专门的排序模型（ScoringModel）评分 把排序好的 Content 注入到用户的提问中。可以自定义注入的模板 最后将注入好的消息交给大模型，让大模型完成最后的总结 简单总结一下整个流程 括号中是 langchain4j 的组件接口 1用户提问 -&gt; 问题压缩/扩展(QueryTransformer) -&gt; 问题路由(QueryRouter) -&gt; 检索(ContentRetriever) -&gt; 聚合(ContentAggregator) -&gt; 注入(ContentInjector) -&gt; 交给llm 环境准备 在动手实现之前，需要准备一些环境 大模型的 api key。我这里使用的是 deepseek 的 key 内嵌模型的 api key。我用的是阿里云的内嵌模型 text-embedding-v3 内嵌数据库。自建的 milvus 数据库 tavily 搜索引擎的访问 key。申请网站是 https://app.tavily.com 也可以换用 谷歌搜索，区别不是很大 这里给出 milvus 环境的 docker compose 配置 注意事项：由于没有挂载数据卷，容器被销毁后，数据也会被删除 端口 19530：milvus 对外提供服务的端口 3000：milvus web 服务的访问端口 访问 3000 端口可以在网页端操作 milvus 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263services: etcd: container_name: milvus-etcd image: quay.io/coreos/etcd:v3.5.18 environment: - ETCD_AUTO_COMPACTION_MODE=revision - ETCD_AUTO_COMPACTION_RETENTION=1000 - ETCD_QUOTA_BACKEND_BYTES=4294967296 - ETCD_SNAPSHOT_COUNT=50000 command: etcd -advertise-client-urls=http://etcd:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd healthcheck: test: [&quot;CMD&quot;, &quot;etcdctl&quot;, &quot;endpoint&quot;, &quot;health&quot;] interval: 30s timeout: 20s retries: 3 minio: container_name: milvus-minio image: minio:latest environment: MINIO_ACCESS_KEY: minioadmin MINIO_SECRET_KEY: minioadmin command: minio server /minio_data --console-address &quot;:9001&quot; healthcheck: test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9000/minio/health/live&quot;] interval: 30s timeout: 20s retries: 3 milvus: container_name: milvus image: milvusdb/milvus:v2.5.10 command: [&quot;milvus&quot;, &quot;run&quot;, &quot;standalone&quot;] security_opt: - seccomp:unconfined environment: ETCD_ENDPOINTS: etcd:2379 MINIO_ADDRESS: minio:9000 healthcheck: test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9091/healthz&quot;] interval: 30s start_period: 90s timeout: 20s retries: 3 ports: - &quot;19530:19530&quot; depends_on: - &quot;etcd&quot; - &quot;minio&quot; attu: container_name: milvus-attu image: zilliz/attu:v2.5.7 environment: MILVUS_URL: milvus:19530 ports: - &quot;3000:3000&quot; depends_on: - &quot;milvus&quot;networks: default: name: milvus 编码 要求 jdk 版本在 21 及以上，项目中有一处使用了虚拟线程 项目简介分别使用 langchain4j 的高级语法和低级组件来实现一个简单的 RAG 功能 外部知识库的来源有两个部分，也可自行扩展 内嵌数据库 tavily 搜索 其中，内嵌数据库的数据库将通过 function call 的方式写入。下面将提供一个简单的案列供参考 pom 依赖 maven 的中央仓库没有 langchain4j-milvus-spring-boot-starter 依赖 有两个处理方案 clone langchain4j-spring 仓库打包对应的版本(1.0.0-beta3)到本地仓库 修改成 langchain4j-milvus 依赖，自行编码构建 MilvusEmbeddingStore bean 对象 这里我选择的是方法一 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;top.wuhunyu.rag&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-rag-example&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;21&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;21&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;spring-boot.version&gt;3.4.0&lt;/spring-boot.version&gt; &lt;langchain4j.version&gt;1.0.0-beta3&lt;/langchain4j.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- springboot 版本锁定 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-boot.version}&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;!-- langchain4j 版本锁定 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-bom&lt;/artifactId&gt; &lt;version&gt;${langchain4j.version}&lt;/version&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- web --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j springboot 启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j 响应式编程 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-reactor&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j openai 接入 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-open-ai-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- langchain4j milvus 接入 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-milvus-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${langchain4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- langchain4j tavily 搜索引擎接入 --&gt; &lt;dependency&gt; &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt; &lt;artifactId&gt;langchain4j-web-search-engine-tavily&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- lombok --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; application.yml123456789101112131415161718192021222324252627282930313233343536373839404142spring: application: name: langchain4j-rag-examplelangchain4j: open-ai: # 大模型 chat-model: base-url: https://api.deepseek.com/v1 api-key: [deepseek api key] model-name: deepseek-chat log-requests: true log-responses: true # 流式大模型 streaming-chat-model: base-url: https://api.deepseek.com/v1 api-key: [deepseek api key] model-name: deepseek-chat log-requests: true log-responses: true # 内嵌模型 embedding-model: base-url: https://dashscope.aliyuncs.com/compatible-mode/v1 api-key: [embedding api key] model-name: text-embedding-v3 max-segments-per-batch: 5 dimensions: 1024 # 内嵌数据库 milvus: host: [自建 milvus 数据库主机地址] port: 19530 database-name: rag collection-name: java dimension: 1024 # 最大搜索结果数 max-results: 10# 网络搜索引擎tavily: # 最大搜索结果数 max-results: 10 api-key: [tavily api key] 监听器 langchain4j 提供了一个监听器接口(dev.langchain4j.model.chat.listener.ChatModelListener)，可以让开发者监听与大模型的交互过程 下面的代码在请求和响应的时候打印了请求的消息对象。我们可以从中观察整个 RAG 过程和大模型交互的次数，以及中间过程的处理结果 123456789101112131415161718192021222324252627282930/** * 大模型交互监听器 * * @author wuhunyu * @date 2025/06/08 11:21 **/@Component(&quot;myChatModelListener&quot;)@Slf4jpublic class MyChatModelListener implements ChatModelListener { @Override public void onRequest(final ChatModelRequestContext requestContext) { final var chatRequest = requestContext.chatRequest(); var messages = chatRequest.messages(); log.debug(&quot;onRequest: {}&quot;, messages); } @Override public void onResponse(final ChatModelResponseContext responseContext) { final var chatResponse = responseContext.chatResponse(); var aiMessage = chatResponse.aiMessage(); log.debug(&quot;onResponse: {}&quot;, aiMessage); } @Override public void onError(final ChatModelErrorContext errorContext) { ChatModelListener.super.onError(errorContext); }} bean 配置 这个配置类注入了一些 langchain4j 的 bean，在高级语法和低级组件中都有用到 具体 bean 的作用可以看注释 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120/** * rag bean 配置 * * @author wuhunyu * @date 2025/06/08 10:44 **/@Configuration@EnableConfigurationProperties(RAGBeanConfig.TavilyProperties.class)public class RAGBeanConfig { // 消息存储，此处为内存存储 @Bean(&quot;chatMemoryStore&quot;) public ChatMemoryStore chatMemoryStore() { return new InMemoryChatMemoryStore(); } // 用于隔离不同的用户会话消息。也可使用 MessageWindowChatMemory 实现用来替换 TokenWindowChatMemory @Bean(&quot;chatMemoryProvider&quot;) public ChatMemoryProvider chatMemoryProvider(ChatMemoryStore chatMemoryStore) { return memoryId -&gt; TokenWindowChatMemory.builder() .chatMemoryStore(chatMemoryStore) .maxTokens(10240, new OpenAiTokenizer(OpenAiChatModelName.GPT_4)) .build(); } // Query 转换器。也可以换用 ExpandingQueryTransformer 进行 Query 扩展 @Bean(&quot;queryTransformer&quot;) public QueryTransformer queryTransformer(ChatLanguageModel chatLanguageModel) { return new CompressingQueryTransformer(chatLanguageModel); } @Data @NoArgsConstructor @AllArgsConstructor @ConfigurationProperties(&quot;tavily&quot;) public static class TavilyProperties { private Integer maxResults; private String apiKey; } // web 搜索引擎，此处选择的是 tavily 搜索引擎 @Bean(&quot;webSearchContentRetriever&quot;) public WebSearchContentRetriever webSearchContentRetriever(TavilyProperties tavilyProperties) { final var tavilyWebSearchEngine = TavilyWebSearchEngine.builder() .apiKey(tavilyProperties.getApiKey()) .build(); return WebSearchContentRetriever.builder() .maxResults(tavilyProperties.getMaxResults()) .webSearchEngine(tavilyWebSearchEngine) .build(); } // 内嵌数据库检索器，此处选择的是 milvus 数据库 @Bean(&quot;embeddingStoreContentRetriever&quot;) public EmbeddingStoreContentRetriever embeddingStoreContentRetriever( @Value(&quot;${langchain4j.milvus.max-results:50}&quot;) Integer maxResults, MilvusEmbeddingStore milvusEmbeddingStore, EmbeddingModel embeddingModel ) { return EmbeddingStoreContentRetriever.builder() .embeddingStore(milvusEmbeddingStore) .embeddingModel(embeddingModel) .maxResults(maxResults) .build(); } // Query 路由，让大模型决定选择哪一个检索器或者哪几个检索器。retrieverToDescription 属性的 key 为检索器，value 为检索器的描述 @Bean(&quot;queryRouter&quot;) public QueryRouter queryRouter( ChatLanguageModel chatLanguageModel, WebSearchContentRetriever webSearchContentRetriever, EmbeddingStoreContentRetriever embeddingStoreContentRetriever ) { return LanguageModelQueryRouter.builder() .chatLanguageModel(chatLanguageModel) .retrieverToDescription(Map.of( webSearchContentRetriever, &quot;Web Search&quot;, embeddingStoreContentRetriever, &quot;Embedding Database&quot; )) .build(); } // content 聚合器，如果有 ReRanking 模型，也可以选择使用 ReRankingContentAggregator 对检索结果进行排序 @Bean(&quot;contentAggregator&quot;) public ContentAggregator contentAggregator() { return new DefaultContentAggregator(); } // content 注入器，可以自定义注入内容的模型 @Bean(&quot;contentInjector&quot;) public ContentInjector contentInjector() { return new DefaultContentInjector(); } // 检索增强器，简单理解为把上面的几个组件联系到一起协作处理 Query，最终返回增强后的 ChatMessage @Bean(&quot;retrievalAugmentor&quot;) public RetrievalAugmentor retrievalAugmentor( QueryTransformer queryTransformer, QueryRouter queryRouter, ContentAggregator contentAggregator, ContentInjector contentInjector ) { // 自定义线程池 final var threadFactory = new CustomizableThreadFactory(&quot;retrievalAugmentor-&quot;); final var executorService = Executors.newThreadPerTaskExecutor(threadFactory); return DefaultRetrievalAugmentor.builder() .queryTransformer(queryTransformer) .queryRouter(queryRouter) .contentAggregator(contentAggregator) .contentInjector(contentInjector) .executor(executorService) .build(); }} 高级语法1234567891011121314151617181920/** * 高级 langchain4j 编程 * * @author wuhunyu * @date 2025/06/08 11:23 **/@AiService( wiringMode = AiServiceWiringMode.EXPLICIT, chatModel = &quot;openAiChatModel&quot;, streamingChatModel = &quot;openAiStreamingChatModel&quot;, chatMemoryProvider = &quot;chatMemoryProvider&quot;, retrievalAugmentor = &quot;retrievalAugmentor&quot;)public interface AdvanceLLMService { @SystemMessage(&quot;你是一个知识渊博的助手，请通俗易懂地回答我的问题。&quot;) TokenStream streamChat(@MemoryId Long userId, @UserMessage String userMessage);} 高级语法主要的配置在 @AiService 这个注解上 chatModel 和 streamingChatModel 分别表示非流式大模型对象和流式大模型对象。这么说可能会有一些歧义。大模型都是支持流式返回的，可以称为同步返回和异步一点一点返回，带 streaming 前缀的就是一步一点一点返回的 chatMemoryProvider 用来区别不同的会话 retrievalAugmentor 检索增强，RAG 的核心实现就是这个组件，后续低级写法中，也是对这个组件进行分解成多个基础组件并手动编码实现同样的效果 userId 参数是用来区别不同的会话使用的，通过同一个 id 值会讲历史的对话记录一并发送给大模型 userMessage 参数是实际用户的提问 方法顶上的 @SystemMessage 注解表示一个系统消息，在优先级上要高于普通的用户消息 TokenStream 类型的响应值表示这是一个流式的返回 那么，让我们试一下这个高级语法实现的 RAG 效果如何 我们新增一个请求路由来调用这个方法 请求路由123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * rag * * @author wuhunyu * @date 2025/06/08 11:35 **/@RestController@RequestMapping(&quot;/rag&quot;)@RequiredArgsConstructor@Slf4jpublic class RAGController { private final AdvanceLLMService advanceLLMService; @Data @NoArgsConstructor @AllArgsConstructor public static class UserMessageRequest implements Serializable { @Serial private static final long serialVersionUID = -8569299218617670092L; /** * 用户id */ private Long userId; /** * 消息 */ private String message; } @PostMapping(value = &quot;/advance&quot;, produces = MediaType.TEXT_EVENT_STREAM_VALUE) public SseEmitter advance(@RequestBody UserMessageRequest userMessageRequest) { final var tokenStream = advanceLLMService.streamChat( userMessageRequest.getUserId(), userMessageRequest.getMessage() ); // 结果返回 final var sseEmitter = new SseEmitter(-1L); tokenStream.onPartialResponse(s -&gt; { try { sseEmitter.send( SseEmitter.event() .data(s) .build() ); } catch (IOException e) { throw new RuntimeException(e); } }) .onCompleteResponse(chatResponse -&gt; { log.info(&quot;chatResponse: {}&quot;, chatResponse.aiMessage().text()); sseEmitter.complete(); }) .onError(throwable -&gt; { sseEmitter.complete(); log.error(&quot;&quot;, throwable); }); tokenStream.start(); return sseEmitter; }} 这时候内嵌数据库还是没数据的状态，尝试询问大模型一个有关 jvm 的问题 发起一个 POST 请求，如下 123456curl --location --request POST 'http://localhost:8080/rag/advance' \\--header 'Content-Type: application/json' \\--data-raw '{ &quot;userId&quot;: 1, &quot;message&quot;: &quot;请简单地描述一下 jvm 是什么&quot;}' 此前我们配置了一个监听器，让我们来看看都和大模型交互了一些什么信息 问题路由 可以看到，发送了一个选择给大模型，选择的条件是我的问题本身 123452025-06-08 20:27:07.224 [http-nio-8080-exec-2] DEBUG top.wuhunyu.rag.listener.MyChatModelListener - onRequest: [UserMessage { name = null contents = [TextContent { text = &quot;Based on the user query, determine the most suitable data source(s) to retrieve relevant information from the following options:1: Web Search2: Embedding DatabaseIt is very important that your answer consists of either a single number or multiple numbers separated by commas and nothing else!User query: 请简单地描述一下 jvm 是什么&quot; }] }] 而此次询问大模型返回的结果是 12025-06-08 20:27:11.492 [http-nio-8080-exec-2] DEBUG top.wuhunyu.rag.listener.MyChatModelListener - onResponse: AiMessage { text = &quot;1,2&quot; toolExecutionRequests = null } 也就是大模型角色有必要从 Web Search 和 Embedding Database 两个数据来源中获取数据 接下来，便是分别从两个数据来源获取响应的结果，然后进行排序，注入，最后再发送给大模型，由大模型进行总结 最后的请求和响应日志都太长了，这里就不贴出来了 在请求调试工具中， SSE 的返回结果形如 1234567891011121314151617181920212223242526data:Jdata:VMdata:（data:Javadata: Virtualdata: Machinedata:，data:Javadata: data:虚拟机data:）data:是data: Java... function call 上面这次问题，在内嵌数据库中，并没有查询到任何数据。这点可以通过日志观察得出来，最后一次提问中，发送给大模型的提示词中，并没有包含 web 的数据来源，而来源于 web 搜索的数据都会有一个网站的来源。比如 12345深入理解HotSpot JVM 基本原理 - 腾讯云它保存所有被JVM加载的类和接口的运行时常量池，成员变量以及方法的信息，静态变量以及方法的字节码。JVM的提供者可以通过不同的方式来实现方法区。在Oracle 的HotSpot JVM里，方法区被称为永久区或者永久代（PermGen）。doocs/jvm: JVM 底层原理最全知识总结 - GitHub🤗 JVM 底层原理最全知识总结. Contribute to doocs/jvm development by creating an account on GitHub. 表明了这两条数据分别来源于 腾讯云 和 GitHub 现在让我们利用 tool 将文档向量化并写入内嵌数据库中，不过在使用 tool 之前，我们需要先声明 本地工具 LocalFileHandlerTool 这个工具属于 langchain4j 的高级语法，低级写法太麻烦了，个人也推荐使用注解的方式描述工具 只有两个功能 列出服务器指定的路径下的所有文件 将指定的文件索引到内嵌数据库中。这里的内嵌数据库指的是环境准备中的 milvus 数据库 实现中，代码有一个 PATH 常量，需要替换成自己机器上的路径，需要这个路径下有一些文档文件（比如 word，pdf，markdown等格式的文件），便于向量化然后写入到内嵌数据库中 其中标注了 @Tool 注解的便是一个 tool，可以通过 @P 注解对入参进行描述 最后给到大模型的是一个 json 格式的工具描述，可以自行断点观察 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/* * 本地文件处理 * * @author wuhunyu * @date 2025/06/08 11:18 **/@Component(&quot;localFileHandlerTool&quot;)@RequiredArgsConstructor@Slf4jpublic class LocalFileHandlerTool { public static final String PATH = &quot;/Users/wuhunyu/WorkSpace/learn&quot;; private final EmbeddingModel embeddingModel; private final MilvusEmbeddingStore milvusEmbeddingStore; @Tool(&quot;列出目录下的所有文件&quot;) @SuppressWarnings(&quot;all&quot;) public List&lt;String&gt; listDir() { return Arrays.stream( new File(PATH).listFiles() ) .filter(File::isFile) .map(File::getName) .toList(); } @Tool(&quot;获取指定文件名的大小，单位：字节。当文件不存在或者不可读时，返回 -1&quot;) public Long fileSize(@P(&quot;文件名称&quot;) String fileName) { if (!this.isFile(fileName)) { return -1L; } return new File(this.getCompleteFileName(fileName)).length(); } private boolean isFile(String fileName) { fileName = this.getCompleteFileName(fileName); if (StringUtils.isBlank(fileName)) { return false; } final var file = new File(fileName); return file.exists() &amp;&amp; file.isFile() &amp;&amp; file.canRead(); } private String getCompleteFileName(String fileName) { return PATH + File.separator + fileName; } @Tool(&quot;将指定文件的内容存放至 embedding 数据库中。操作成功时，返回 success，其他返回都为错误信息&quot;) public String toEmbedding(@P(&quot;文件名称&quot;) String fileName) { if (!this.isFile(fileName)) { return &quot;文件不存在&quot;; } try { // 加载文档 final var document = FileSystemDocumentLoader.loadDocument( this.getCompleteFileName(fileName), new TextDocumentParser() ); // 文档拆分器 DocumentBySentenceSplitter documentBySentenceSplitter = new DocumentBySentenceSplitter( 300, 100, new OpenAiTokenizer(OpenAiEmbeddingModelName.TEXT_EMBEDDING_3_SMALL)); // 文档拆分，并添加 source 元数据 var textSegments = documentBySentenceSplitter.split(document) .stream() .peek(textSegment -&gt; { Metadata metadata = textSegment.metadata(); metadata.put(&quot;source&quot;, fileName); }) .toList(); // 添加到 内嵌数据库 中 var listResponse = embeddingModel.embedAll(textSegments); milvusEmbeddingStore.addAll(listResponse.content(), textSegments); // 返回成功 return &quot;success&quot;; } catch (Exception e) { log.error(&quot;读取本地文档 {} 到内嵌数据库失败: &quot;, fileName, e); return e.getMessage(); } }} 新增一个内嵌数据服务 EmbeddingService 不知道你看出来和 AdvanceLLMService 的区别了吗 本质区别就是少了 retrievalAugmentor 属性，多了 tools 属性 为什么要这么做呢，不能合并成一个吗？ 由于 retrievalAugmentor 内部会做各种动作对最初的提问进行增强，很可能会干扰大模型的判断，所以我们需要一个比较干净的上下文环境减少大模型的误判断 123456789101112131415161718192021/** * 内嵌数据准备 * * @author gongzhiqiang * @date 2025/06/08 20:45 **/@AiService( wiringMode = AiServiceWiringMode.EXPLICIT, chatModel = &quot;openAiChatModel&quot;, streamingChatModel = &quot;openAiStreamingChatModel&quot;, chatMemoryProvider = &quot;chatMemoryProvider&quot;, tools = { &quot;localFileHandlerTool&quot; })public interface EmbeddingService { Result&lt;String&gt; chat(@MemoryId Long userId, @UserMessage String userMessage);} 在请求路由上添加一个调用 12345@PostMapping(&quot;/embedding-tool&quot;)public String embeddingTool(@RequestBody UserMessageRequest userMessageRequest) { final var result = embeddingService.chat(userMessageRequest.getUserId(), userMessageRequest.getMessage()); return result.content();} 先要大模型列出路径下的全部文件 123456curl --location --request POST 'http://localhost:8080/rag/embedding-tool' \\--header 'Content-Type: application/json' \\--data-raw '{ &quot;userId&quot;: 2, &quot;message&quot;: &quot;列出全部的文件&quot;}' 结果如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748当前目录下的文件列表如下：1. Kafka.md2. MyBatis.md3. ArchLinux.md4. Docker操作命令.md5. JVM类的加载过程.md6. Go.md7. Java程序员-xxx.md8. SpringMVC.md9. Zookeeper2.md10. Spring5.md11. JVM字节码与类的加载篇.md12. JUC.md13. 面试积累.md14. Redis.md15. SpringBoot.md16. 二叉树前中后序遍历.md17. nginx.md18. Java工程师-xxx.md19. JVM内存与垃圾回收篇.md20. Java8新特性.md21. SpringSecurity.md22. MySQL.md23. ITX主机计划.md24. JVM.md25. 面试小笔记.md26. Linux.md27. JVM性能调优.md28. Maven.md29. C语言.md30. JVM运行时参数.md31. Git.md32. 工作经历-xxx.md33. ElasticSearch.md34. RabbiMQ.md35. Docker.md36. SpringCloud.md37. 反射.md38. 面试学习.md39. 实战开发.md40. NIO.md41. Spring简单整合MyBatis(基于xml方式).md42. MySQL调优.md43. Zookeeper.md44. 个人笔记.md如果需要进一步操作（如获取文件大小或嵌入内容），请告诉我！ 将 19 号文件加载到内嵌数据库中 123456curl --location --request POST 'http://localhost:8080/rag/embedding-tool' \\--header 'Content-Type: application/json' \\--data-raw '{ &quot;userId&quot;: 2, &quot;message&quot;: &quot;将 19 号文件加载到内嵌数据库中&quot;}' 结果如下 再次询问 请简单地描述一下 jvm 是什么 123456curl --location --request POST 'http://localhost:8080/rag/advance' \\--header 'Content-Type: application/json' \\--data-raw '{ &quot;userId&quot;: 1, &quot;message&quot;: &quot;请简单地描述一下 jvm 是什么&quot;}' 可以在控制台观察到最后发送给大模型的消息中，除了标记由来源的消息，也有不携带消息来源的消息，这些不携带消息来源的消息就是来源于内嵌数据库中 低级写法 所谓低级写法，就是将 @AiService 注解实现的功能用编码的方式手动实现一遍 此处只给出代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * langchain4j 低级写法 * * @author wuhunyu * @date 2025/06/08 12:23 **/@Component(&quot;basicLLMService&quot;)@RequiredArgsConstructor@Slf4jpublic class BasicLLMService { private final ChatLanguageModel chatLanguageModel; private final QueryTransformer queryTransformer; private final ChatMemoryProvider chatMemoryProvider; private final QueryRouter queryRouter; private final ContentAggregator contentAggregator; private final ContentInjector contentInjector; public String chat(Long userId, String message) { Query query = Query.from( message, Metadata.from( UserMessage.from(message), userId, chatMemoryProvider.get(userId) .messages() ) ); // transform Collection&lt;Query&gt; queries = queryTransformer.transform(query); // query router Map&lt;Query, Collection&lt;List&lt;Content&gt;&gt;&gt; query2Contents = new HashMap&lt;&gt;(); for (Query curQuery : queries) { Collection&lt;ContentRetriever&gt; contentRetrievers = queryRouter.route(curQuery); List&lt;List&lt;Content&gt;&gt; contents = new ArrayList&lt;&gt;(); for (ContentRetriever contentRetriever : contentRetrievers) { contents.add(contentRetriever.retrieve(curQuery)); } query2Contents.put(curQuery, contents); } // content aggregator List&lt;Content&gt; finalContents = contentAggregator.aggregate(query2Contents); // content injector ChatMessage userMessage = contentInjector.inject( finalContents, UserMessage.from(message) ); // 系统消息 var systemMessage = SystemMessage.from( &quot;&quot;&quot; 你是一个知识渊博的助手，请通俗易懂地回答我的问题。 &quot;&quot;&quot; ); // 历史消息 final var messages = chatMemoryProvider.get(userId) .messages(); if (CollectionUtils.isEmpty(messages)) { // 第一次提问 messages.add(systemMessage); } messages.add(userMessage); // chat ChatRequest chatRequest = ChatRequest.builder() .messages(messages) .build(); // response return chatLanguageModel.chat(chatRequest) .aiMessage() .text(); }} 总结本文先按作者自己的理解讲述了对 langchain4j 框架的 RAG 实现流程以及各个组件的作用 然后通过编码的方式实现了一个简单的 RAG 例子，基本用上了 langchain4j 的大多组件 遇到的问题 超出最大窗口大小 项目中使用的对话记忆方案是 TokenWindowChatMemory，最大的 token 被设置为了 10240。如果把 langchain4j.milvus.max-results 和 tavily.max-results 都调整成 50，可以观察到一个很诡异的事情是明明网络搜索和内嵌数据库搜索一共 100 条记录，最后却没有发送给大模型 其实问题出在上图中的 chatMemory.add(userMessage) 代码(图中 175 行)。由于 100 条记录的长度超出了 10240 长度，因此被丢弃了这部分消息 在生产开发过程中，还是需要注意一下最大窗口问题 适配 OpenAI API OpenAI API 的官方文档地址是 https://platform.openai.com/docs/api-reference/chat/create 在 langchain4j-open-ai-spring-boot-starter 依赖中，提供了 dev.langchain4j.model.openai.internal.chat.ChatCompletionRequest 和 dev.langchain4j.model.openai.internal.chat.ChatCompletionResponse 作为请求体类型和响应类型 一切都很完美是不是，我们需要实现 OpenAI API 接口的话，可以直接利用这两个类而不需要自己编写又臭又长还可能出错的 VO 类 但我现在要给你拨一瓢冷水了，dev.langchain4j.model.openai.internal.chat.ChatCompletionRequest#messages 的类型是一个接口，这意味着网络请求在反序列化时不清楚到底应该使用哪一个实现，因此不对它进行改造是会出错的 你可以将这个接口替换为某个实现类以修复这个问题 高级语法无法传递 ChatRequestParameters 参数 12345678910111213141516171819202122232425262728293031@Experimentalpublic interface ChatRequestParameters { String modelName(); Double temperature(); Double topP(); Integer topK(); Double frequencyPenalty(); Double presencePenalty(); Integer maxOutputTokens(); List&lt;String&gt; stopSequences(); List&lt;ToolSpecification&gt; toolSpecifications(); ToolChoice toolChoice(); ResponseFormat responseFormat(); static DefaultChatRequestParameters.Builder&lt;?&gt; builder() { return new DefaultChatRequestParameters.Builder&lt;&gt;(); } ChatRequestParameters overrideWith(ChatRequestParameters parameters);} 如果你使用过 Gemini 网页版或者像是 Cherry Studio 这样的客户端，应该能注意到在和大模型对话时，是可以调整一些参数的，比如 temperature，maxOutputTokens 等 langchain4j 所谓的高级语法主要是基于 @AiService 这个注解。而这个注解的处理类是 dev.langchain4j.service.DefaultAiServices 以非流式对话为例（代码片段为 dev.langchain4j.service.DefaultAiServices 的 214 行到 224 行） 1234567891011ChatRequestParameters parameters = ChatRequestParameters.builder() .toolSpecifications(toolExecutionContext.toolSpecifications()) .responseFormat(responseFormat) .build();ChatRequest chatRequest = ChatRequest.builder() .messages(messages) .parameters(parameters) .build();ChatResponse chatResponse = context.chatModel.chat(chatRequest); 看出来了没，ChatRequestParameters 构建时就没有注入 temperature，maxOutputTokens 等这些参数 也就是说，使用高级语法时，是无法实现对这些参数进行调整的 同样，我在 langchain4j 仓库中找到了一个 issue 有一个评论是这样的 1Response&lt;AiMessage&gt; chat(@UserMessage String userMessage, @ChatRequestParams ChatRequestParameters params); @ChatRequestParams 在 langchain4j 中是不存在的，这位兄弟想要通过注解的方式实现自定义 ChatRequestParameters 但目前这个 issue 还是 open 状态，目前如果有微调参数的需要，还是只能使用低级语法 不过值得注意的是，并不是所有大模型都支持这些参数的调整 项目代码GitHub 参考langchain4j 官网","link":"/ai/rag/2025/06/langchain4j-rag-example/index.html"},{"title":"Mermaid 简单使用与实践","text":"Mermaid 是什么?Mermaid 是一个基于 JavaScript 的图表和制图工具，使用 Markdown 风格的文本定义和渲染器来创建和修改复杂的图表。Mermaid 的主要目的是帮助文档跟上开发进度 Mermaid 能画哪些图? 流程图 时序图 类图 状态图 实体关系图 用户旅程图 甘特图 饼图 象限图 需求图 Git 图表 C4 图表 思维导图 时间线图 ZenUML Sankey XY 图表 块图 数据包图 Kanban 架构图 语法介绍 Mermaid 的图有很多, 这里只介绍流程图, 状态图和饼图三种图的常用语法 更多的语法和其它图请参考官方文档 以下语法介绍基于 11.12.1 版本 让我们开始吧 流程图先看一个完整的列子 12345678910---title: 流程图演示---graph TD %% 注释 node1[矩形节点] -- 普通剪头 --&gt; node2(圆角矩形节点) node2 == 加粗箭头 ==&gt; node4{菱形节点} node4 -. 虚线剪头 .-&gt; node5[(圆柱)] node4 &lt;-- 双向箭头 --&gt; node6((圆形)) node4 -- 自指向剪头 --&gt; node4 --- title: 流程图演示 --- graph TD %% 注释 node1[矩形节点] -- 普通剪头 --> node2(圆角矩形节点) node2 == 加粗箭头 ==> node4{菱形节点} node4 -. 虚线剪头 .-> node5[(圆柱)] node4 node6((圆形)) node4 -- 自指向剪头 --> node4 图声明和方向流程图可以使用 flowchart 或者 graph 来声明, 其后的 TD 表示流程图的方向 TB - 从上到下 TD - 从上到下/与从上到下相同 BT - 从下到上 RL - 从右到左 LR - 从左到右 1graph TD 节点形状语法形如 id&lt;形状标识&gt; 默认为矩形 123456graph TD 默认形状 node1[矩形] node2((圆角矩形)) node3[(圆柱)] node4{菱形} graph TD 默认形状 node1[矩形] node2((圆角矩形)) node3[(圆柱)] node4{菱形} 箭头语法形如 ----&gt; 12345678graph TD node1 -- 普通箭头 --&gt; node2 node3 &lt;-- 双向箭头 --&gt; node4 node5 -. 虚线无向箭头 .- node6 node7 -. 虚线箭头 .-&gt; node8 node9 == 加粗箭头 ==&gt; node10 node11 ---- 长箭头 ----&gt; node12 node13 ---- node14 graph TD node1 -- 普通箭头 --> node2 node3 node4 node5 -. 虚线无向箭头 .- node6 node7 -. 虚线箭头 .-> node8 node9 == 加粗箭头 ==> node10 node11 ---- 长箭头 ----> node12 node13 ---- node14 子图需要有一对 subgraph ... end 的标签 1234567graph TB node1[节点1] ---&gt; sub sub ---&gt; node2[节点2] subgraph sub[子图] sub_node1[子图节点1] --&gt; sub_node2[子图节点2] end graph TB node1[节点1] ---> sub sub ---> node2[节点2] subgraph sub[子图] sub_node1[子图节点1] --> sub_node2[子图节点2] end 状态图同样先看一个例子 1234567891011121314151617181920212223242526272829303132---title: 状态图演示---stateDiagram-v2 direction TB state shipped &lt;&lt;choice&gt;&gt; state pay { direction LR [*] --&gt; init init --&gt; paying: 用户发起支付 state &quot;初始化中&quot; as init state &quot;支付中&quot; as paying } [*] --&gt; pay: 提交订单 pay --&gt; processing: 支付成功 pay --&gt; cancelled: 用户主动取消 / 支付超时 processing --&gt; shipped: 商家 / 仓库发货 shipped --&gt; refunded: 用户申请取消 shipped --&gt; completed: 用户收货 cancelled --&gt; [*] refunded --&gt; [*] completed --&gt; [*] pay: 待支付 note right of pay: 待支付子状态 processing: 待发货 refunded: 已退款 cancelled: 已取消 completed: 已完成 --- title: 状态图演示 --- stateDiagram-v2 direction TB state shipped state pay { direction LR [*] --> init init --> paying: 用户发起支付 state \"初始化中\" as init state \"支付中\" as paying } [*] --> pay: 提交订单 pay --> processing: 支付成功 pay --> cancelled: 用户主动取消 / 支付超时 processing --> shipped: 商家 / 仓库发货 shipped --> refunded: 用户申请取消 shipped --> completed: 用户收货 cancelled --> [*] refunded --> [*] completed --> [*] pay: 待支付 note right of pay: 待支付子状态 processing: 待发货 refunded: 已退款 cancelled: 已取消 completed: 已完成 图声明状态图的声明关键字是 stateDiagram-v2 节点形状状态图并没有像是流程图那种丰富的节点形状 1234567891011121314stateDiagram-v2 %% 起始节点 和 结束节点 direction LR [*] --&gt; [*] %% 选择节点: 无法添加注释,添加注释之后会变成圆角矩形 state choice &lt;&lt;choice&gt;&gt; %% 子状态图 state sub_state_diagram { direction LR [*] --&gt; [*] } sub_state_diagram: 子状态图 stateDiagram-v2 %% 起始节点 和 结束节点 direction LR [*] --> [*] %% 选择节点: 无法添加注释,添加注释之后会变成圆角矩形 state choice %% 子状态图 state sub_state_diagram { direction LR [*] --> [*] } sub_state_diagram: 子状态图 关系描述状态流转的关系描述可以直接使用 : &lt;关系描述&gt; 12345stateDiagram-v2 [*] --&gt; node: 关系描述1 node --&gt; [*]: 关系描述2 node: 节点 stateDiagram-v2 [*] --> node: 关系描述1 node --> [*]: 关系描述2 node: 节点 箭头状态图中的箭头只能是 --&gt; 状态描述状态图中有两种添加状态描述的方式 形如 id: &lt;状态描述&gt; 形如 state &quot;&lt;状态描述&gt;&quot; as id 1234567stateDiagram-v2 [*] --&gt; node1 node1 --&gt; node2 node2 --&gt; [*] node1: 节点1 state &quot;节点2&quot; as node2 stateDiagram-v2 [*] --> node1 node1 --> node2 node2 --> [*] node1: 节点1 state \"节点2\" as node2 子图子图的声明和选择节点有点像 12345678910111213stateDiagram-v2 state sub_state_diagram { direction LR [*] --&gt; sub_node sub_node --&gt; [*] sub_node: 子节点 } [*] --&gt; sub_state_diagram sub_state_diagram --&gt; [*] sub_state_diagram: 子图 stateDiagram-v2 state sub_state_diagram { direction LR [*] --> sub_node sub_node --> [*] sub_node: 子节点 } [*] --> sub_state_diagram sub_state_diagram --> [*] sub_state_diagram: 子图 饼图饼图相对来说比较简单 123456pie showData title 饼图演示 &quot;Apple&quot;: 5 &quot;Banana&quot;: 10 &quot;Orange&quot;: 3 &quot;Strawberry&quot;: 2 pie showData title 饼图演示 \"Apple\": 5 \"Banana\": 10 \"Orange\": 3 \"Strawberry\": 2 图声明饼图的声明以 pie 来标识, 可以使用 title 为饼图自定义标题 123pie title 饼图 &quot;full&quot;: 1 pie title 饼图 \"full\": 1 比例每个项目的占比使用 k-v 的形式声明, 要求 k 必须要使用双引号包裹, v 为正数(支持小数) 也可以使用 showData 来展示具体的占比的实际值 123456pie showData title 饼图 &quot;one&quot;: 1 &quot;two&quot;: 2 &quot;three&quot;: 3.00 &quot;four&quot;: 4.0 pie showData title 饼图 \"one\": 1 \"two\": 2 \"three\": 3.00 \"four\": 4.0 写在最后还有许多其它常用的图, 比如 时序图, XY 图表, 思维导图, 它们的定义就会比较复杂一些了 我觉得应该在 Mermaid 这种编码图和二进制图之间做一个取舍, 编码图固然方便, 既没有二进制图片存储的负担, 修改也方便, 实时渲染且样式统一简洁; 但面对一些复杂的图形(比如流程节点众多的流程图), 有一些网站(比如 draw.io)提供的在线绘图能力在作图成本上还是占优的,而且不需要记忆编码图的语法 引用 Mermaid 英文官方文档 Mermaid 中文文档","link":"/chart/2025/12/01/mermaid/index.html"},{"title":"jackson mixin 特性简单应用","text":"前言前段时间遇到一个问题 场景 项目中有一个手机号脱敏的需求 项目中涉及脱敏的 VO 类也挺多. 粗略看了一下有几十个,用法就更多了 当时选择的脱敏方案是使用 jackson 的序列化注解方案,形如 1@JsonSerialize(using = DesensitizedPhoneSerializer.class) 其中的 DesensitizedPhoneSerializer 是一个自定义的序列化器, 需要继承 com.fasterxml.jackson.databind.JsonSerializer 抽象类, 主要需要重写它的一个方法 12345678910/** * Core method for serializing a value of the type this serializer handles. * * @param value The value to serialize; cannot be {@code null}. * @param gen The generator used to output the resulting JSON content. * @param serializers The provider that can be used to get serializers for * any objects that the value contains. * @throws IOException If an I/O error occurs during the generation process. */public abstract void serialize(T value, JsonGenerator gen, SerializerProvider serializers) throws IOException; 问题项目中微服务之间调用使用的是 OpenFeign, 这个框架基于 http 请求并进行动态代理封装的 依旧会使用 jackson 来进行序列化与反序列化, 网络传输的响应体也就会被 DesensitizedPhoneSerializer 脱敏手机号 但微服务之间的调用有些时候需要获取到手机号明文 处理方案方案一需要进行 rpc 内部调用的接口应该和对普通用户的接口职责分离, 这样可以做到接口级别的隔离 在对内的接口中传输明文, 在对外的接口中响应脱敏的密文 方案二如果有一个方案能在进行序列化时得知接口是否为对内, 也可以在 DesensitizedPhoneSerializer 内部决定是否进行脱敏处理 比较方案一涉及的范围就比较大了, 而且现有的系统对旧接口的依赖已经是一团乱麻了, 分离也不是一件容易的事情 程序界有伟人曾经说过, “能跑就行” 关于方案二, 可以利用 jackson 的 Mixin 特性来实现, 这样改动也是会比较小的 总结使用方案二, 改动成本和风险都是较小的 实践Mixin 特性 demo原本需要响应的 UserVO 类 1234567891011121314151617@Builder@Data@NoArgsConstructor@AllArgsConstructorpublic class UserVO implements Serializable { private static final long serialVersionUID = 7799431123917020619L; @JsonSerialize(using = ToStringSerializer.class) private Long id; @JsonSerialize(using = DesensitizedPhoneSerializer.class) private String phone; private String desc;} 添加一个用来替换 DesensitizedPhoneSerializer 序列化器的 UserVOMixin 类 1234567891011@Data@NoArgsConstructor@AllArgsConstructorpublic abstract class UserVOMixin implements Serializable { private static final long serialVersionUID = -793190335567089250L; @JsonSerialize public String phone;} 测试 123456789101112131415final var objectMapper1 = new ObjectMapper();UserVO userVO = UserVO.builder() .id(100L) .phone(&quot;18603216630&quot;) .desc(&quot;My name's Zhangsan&quot;) .build();System.out.println(&quot;=============================脱敏==============================&quot;);System.out.println(objectMapper1.writeValueAsString(userVO));final var objectMapper2 = new ObjectMapper();// 替换掉原有的 UserVOobjectMapper2.addMixIn(UserVO.class, UserVOMixin.class);System.out.println(&quot;============================未脱敏==============================&quot;);System.out.println(objectMapper2.writeValueAsString(userVO)); 输出 1234=============================脱敏=============================={&quot;id&quot;:&quot;100&quot;,&quot;phone&quot;:&quot;186****6630&quot;,&quot;desc&quot;:&quot;My name's Zhangsan&quot;}============================未脱敏=============================={&quot;id&quot;:&quot;100&quot;,&quot;phone&quot;:&quot;18603216630&quot;,&quot;desc&quot;:&quot;My name's Zhangsan&quot;} 应用有了这个特性的帮助, 为 OpenFeign 创建一个单独的 ObjectMapper 对象用于序列化, 并添加需要进行 mixin 的类就大功告成了 或许你会觉得, 不实用 jackson, 换成 fastjson2 或者 gson 作为 OpenFeign 的序列化方案, 这样也可以忽略 jackson 的 @JsonSerialize 注解 这样也可以, 但奥卡姆剃刀有句名言说道”如无必要, 勿增实体”","link":"/java/2025/11/17/jackson-mixin/index.html"},{"title":"MultipartFile 输入流可重复读吗","text":"结论在前每次调用 org.springframework.web.multipart.MultipartFile#getInputStream 都会返回一个全新的输入流 所以每次 MultipartFile 输入流可重复读 前言通常而言,流是单向读取,且不可重复读的 上面指的流包括 jdk 中的 java.io 和 java.util.stream 都遵守这个规则 这个例子中,第二次读取同一个流出现了异常 java1234567891011121314var inputStream = new FileInputStream(&quot;/Users/wuhunyu/Desktop/image.jpeg&quot;);// 第一次读取try { var read1 = inputStream.read(); var read2 = inputStream.read(); System.out.println(read1 + &quot; / &quot; + read2);} finally { // 关闭流 inputStream.close();}// 第二次读取var read3 = inputStream.read(); 控制台打印如下 shell1234255 / 216Exception in thread &quot;main&quot; java.io.IOException: Stream Closed at java.base/java.io.FileInputStream.read0(Native Method) at java.base/java.io.FileInputStream.read(FileInputStream.java:231) 换成 java.util.stream 也是一样的 java1234567final var intStream = IntStream.range(1, 4);// 第一次读取intStream.boxed() .forEach(System.out::println);// 第二次读取intStream.boxed() .forEach(System.out::println); 控制台打印如下 shell12345678910123Exception in thread &quot;main&quot; java.lang.IllegalStateException: stream has already been operated upon or closed at java.base/java.util.stream.AbstractPipeline.&lt;init&gt;(AbstractPipeline.java:203) at java.base/java.util.stream.ReferencePipeline.&lt;init&gt;(ReferencePipeline.java:96) at java.base/java.util.stream.ReferencePipeline$StatelessOp.&lt;init&gt;(ReferencePipeline.java:800) at java.base/java.util.stream.IntPipeline$1.&lt;init&gt;(IntPipeline.java:174) at java.base/java.util.stream.IntPipeline.mapToObj(IntPipeline.java:174) at java.base/java.util.stream.IntPipeline.boxed(IntPipeline.java:233) 探索假设你需要开发一个日志切面,这个日志切面需要记录每次请求的 uri ,请求明细数据以及响应数据 这里我们不探讨需求是否合理,只研究如何实现 比较常见的做法是,利用 spring 的 aop 特性,编写一个切面,来记录网络请求的各项参数 这里比较棘手的问题在于,请求体中的数据如果不事先缓存起来,那么在这个日志切面中读取一次之后,在业务层就无法再次读取请求体 原因在于请求体是 java.io.InputStream 类型,遵守单向读取,且不可重复读 为了处理这个问题,一般会缓存原始的请求体数据,每次要读取时,就从缓存中读取即可 那么,接下来,再看看这个例子 这个例子的功能很简单,把上传的文件复制了两份,分别叫做 image1.jpeg 和 image2.jpeg,并保存在 /Users/wuhunyu/Desktop 目录下 可以看到复制过程中使用了 try-with-resources 语法,也就说, input 流使用完毕之后就会被关闭 java1234567891011121314151617181920@PostMapping(&quot;/upload&quot;)public String inputStream( @RequestParam(&quot;file&quot;) final MultipartFile file) { final Consumer&lt;String&gt; task = fileName -&gt; { try ( final var output = new FileOutputStream(&quot;/Users/wuhunyu/Desktop/&quot; + fileName); final var input = file.getInputStream() ) { input.transferTo(output); } catch (IOException e) { throw new RuntimeException(e); } }; task.accept(&quot;image1.jpeg&quot;); task.accept(&quot;image2.jpeg&quot;); return &quot;ok&quot;;} MultipartFile 是 spring 中对网络文件对象的抽象, 我使用的是 tomcat 内置容器,它的实现是 org.springframework.web.multipart.support.StandardMultipartHttpServletRequest.StandardMultipartFile 在上面的例子中, file.getInputStream() 被执行了两次,如果 org.springframework.web.multipart.MultipartFile#getInputStream 返回的是同一个流对象,那么必然会出现前两个例子中的异常 但很幸运的是,代码正常运行,且在 /Users/wuhunyu/Desktop 目录下,也像我们预期的那样多了两个文件( image1.jpeg 和 image2.jpeg) 这就令人疑惑了,难道 org.springframework.web.multipart.MultipartFile#getInputStream 返回的不是同一个流对象吗 翻看源码,来到了 org.apache.tomcat.util.http.fileupload.disk.DiskFileItem#getInputStream java123456789101112131415161718192021/** * Returns an {@link java.io.InputStream InputStream} that can be * used to retrieve the contents of the file. * * @return An {@link java.io.InputStream InputStream} that can be * used to retrieve the contents of the file. * * @throws IOException if an error occurs. */@Overridepublic InputStream getInputStream() throws IOException { if (!isInMemory()) { return Files.newInputStream(dfos.getFile().toPath()); } if (cachedContent == null) { cachedContent = dfos.getData(); } return new ByteArrayInputStream(cachedContent);} 别的不看,两个 return 都是创建了新的流.那就说明每次 getInputStream 时,都是一个全新的 InputStream 对象,这样就不奇怪为什么 org.springframework.web.multipart.MultipartFile#getInputStream 获取的流可以被多次使用了 那么, spring 是怎么做到的呢,莫非也是缓存? 简单 debug 之后,发现在 org.apache.tomcat.util.http.fileupload.FileUploadBase#parseRequest 中 java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Processes an &lt;a href=&quot;http://www.ietf.org/rfc/rfc1867.txt&quot;&gt;RFC 1867&lt;/a&gt; * compliant {@code multipart/form-data} stream. * * @param ctx The context for the request to be parsed. * * @return A list of {@code FileItem} instances parsed from the * request, in the order that they were transmitted. * * @throws FileUploadException if there are problems reading/parsing * the request or storing files. */public List&lt;FileItem&gt; parseRequest(final RequestContext ctx) throws FileUploadException { final List&lt;FileItem&gt; items = new ArrayList&lt;&gt;(); boolean successful = false; try { final FileItemIterator iter = getItemIterator(ctx); final FileItemFactory fileItemFactory = Objects.requireNonNull(getFileItemFactory(), &quot;No FileItemFactory has been set.&quot;); final byte[] buffer = new byte[Streams.DEFAULT_BUFFER_SIZE]; while (iter.hasNext()) { if (items.size() == fileCountMax) { // The next item will exceed the limit. throw new FileCountLimitExceededException(ATTACHMENT, getFileCountMax()); } final FileItemStream item = iter.next(); // Don't use getName() here to prevent an InvalidFileNameException. final String fileName = item.getName(); final FileItem fileItem = fileItemFactory.createItem(item.getFieldName(), item.getContentType(), item.isFormField(), fileName); items.add(fileItem); try { Streams.copy(item.openStream(), fileItem.getOutputStream(), true, buffer); } catch (final FileUploadIOException e) { throw (FileUploadException) e.getCause(); } catch (final IOException e) { throw new IOFileUploadException(String.format(&quot;Processing of %s request failed. %s&quot;, MULTIPART_FORM_DATA, e.getMessage()), e); } final FileItemHeaders fih = item.getHeaders(); fileItem.setHeaders(fih); } successful = true; return items; } catch (final FileUploadException e) { throw e; } catch (final IOException e) { throw new FileUploadException(e.getMessage(), e); } finally { if (!successful) { for (final FileItem fileItem : items) { try { fileItem.delete(); } catch (final Exception ignored) { // ignored TODO perhaps add to tracker delete failure list somehow? } } } }} 第 34 行,出现了一个流复制.其中,输入流来源于请求,输出流如下源码.创建了一个临时文件 java12345678910111213141516/** * Returns an {@link java.io.OutputStream OutputStream} that can * be used for storing the contents of the file. * * @return An {@link java.io.OutputStream OutputStream} that can be used * for storing the contents of the file. * */@Overridepublic OutputStream getOutputStream() { if (dfos == null) { final File outputFile = getTempFile(); dfos = new DeferredFileOutputStream(sizeThreshold, outputFile); } return dfos;} 并且在 org.apache.tomcat.util.http.fileupload.FileUploadBase#parseRequest 的 finally 块中,对 items 进行了清理 这样说来,spring 在处理文件上传时,会先将上传的文件写入到临时文件中,而后续开发者对 MultipartFile 对象的操作本质都是针对这个已经缓存在服务器上的临时文件,即便是多次创建新的输入流,成本也并没有网络io那么高 可为什么要这么做呢?spring 大可以告知开发者们,文件流只允许读取一次,如果有多次读取的需求,请自行实现 带着这个问题,我想看看在 go 中,它的一个热门 web 框架 gin 是怎么看待这个问题的 示例代码如下,省略了很多错误处理 go123456789101112131415161718192021222324252627282930313233343536373839404142func main() { r := gin.Default() r.POST(&quot;/upload&quot;, func(c *gin.Context) { formFile, _ := c.FormFile(&quot;file&quot;) task := func(targetName string) bool { file, err := formFile.Open() if err != nil { return false } targetFile, _ := os.OpenFile(targetName, os.O_WRONLY|os.O_CREATE, 0666) defer func() { _ = targetFile.Close() }() _, _ = io.Copy(targetFile, file) if err := file.Close(); err != nil { return false } return true } if !task(&quot;/Users/wuhunyu/Desktop/image1.jpeg&quot;) { c.JSON(http.StatusBadRequest, gin.H{ &quot;message&quot;: fmt.Sprintf(&quot;upload file to %s failed&quot;, &quot;image1&quot;), }) return } if !task(&quot;/Users/wuhunyu/Desktop/image2.jpeg&quot;) { c.JSON(http.StatusBadRequest, gin.H{ &quot;message&quot;: fmt.Sprintf(&quot;upload file to %s failed&quot;, &quot;image2&quot;), }) return } c.JSON(http.StatusOK, gin.H{ &quot;message&quot;: &quot;upload success&quot;, }) c.Done() }) err := r.Run(&quot;:8080&quot;) if err != nil { panic(err) }} 代码中, formFile 被 Open 了两次,最后的结果也是正常,且成功复制了两个文件( image1.jpeg 和 image2.jpeg) 也就是说, gin 的处理和 spring 是一样的,都是支持流被重复读取 至于为什么 StandardMultipartFile 被设计成可以被重复读取, ai 给我的回答如下 HTTP 请求在本质上是一个原子性的、一次性的事务。服务器必须读取完整个请求的输入流，才能确认请求已经结束并且没有损坏。一旦这个底层的网络输入流被读取完毕，它就被消费掉了，无法“倒带”或重读 将文件缓存下来，最大的好处就是文件内容可以被多次、独立地访问 将文件的接收阶段和处理阶段解耦，可以极大地提高系统的可靠性 框架（如 Spring MVC）和 Servlet 容器的设计目标之一就是简化开发者的工作 主要能说服我的是第一点 响应式编程传统的HTTP请求是请求-响应式的,如果换成响应式编程呢 特性 Spring MVC (MultipartFile) Spring WebFlux (FilePart) 核心机制 先缓存，后处理 (Buffer-then-process) 真正的流式处理 (True Streaming) 调用时机 控制器方法在整个文件接收并缓存完毕后才被调用。 控制器方法在请求头到达后立即被调用，文件内容以流的形式后续提供。 资源占用 内存或磁盘占用与上传文件大小成正比。一个 1GB 的文件会占用 1GB 的临时存储。 内存占用非常小且基本恒定，与文件大小无关。只占用一小块缓冲区来处理当前的数据块。 编程模型 命令式、阻塞式。transferTo() 是一个阻塞操作，直到文件写完才返回。 响应式、非阻塞。transferTo() 返回一个 Mono，立即返回，文件写入在后台异步进行。 可扩展性 处理大量并发大文件上传时，会因临时存储耗尽或线程阻塞而遇到瓶颈。 高度可扩展。由于资源占用低且非阻塞，可以用少量线程处理大量并发上传。 响应式编程默认情况下文件流不会被完整缓存到服务器内存或磁盘后才交给代码处理,它采用了真正的流式处理","link":"/java/2025/10/10/multipart-file/index.html"},{"title":"记一次个人服务器操作系统切换","text":"前言早年买了一台 mac mini,后面因为又购入了 mac book pro 而闲置了 一直没有找到合理利用它的场景 探索前面说到一直没有找到好好利用这台闲置的 mac mini 的场景 我同事有使用自己的 mac mini 办公,但我已经有 mac book pro 了,有些数据如果要在多个机器上传输还是很麻烦的.如果能做到工作和生活完全分离,那么工作用 mac mini,生活用 mac book pro 也是不错的.但对于我这种手机卡都只习惯使用一张的人来说,多机器还是有点麻烦了 mac os 作为一个个人桌面操作系统,相对于 windows 操作系统,我体验下来还是很不错的,具体体现在 类 linux 操作系统.虽然命令不怎么兼容各类 linux 发行版,但比起 windows 操作系统的终端来说,要好很多.如果直接使用 linux 桌面,针对个人来说,软件源太匮乏应该算是它最大的原罪 硬件也很精致. mac os 的流畅和它硬件强大也有很多关系, windows 平台这边,我没发现能与之匹敌的笔记本 品牌效应. 我得承认使用 mac 也受到了网络上一些群友,自媒体的引导, mac os 对我而言,除了不能打游戏以外, 以前在 windows 上能做的事情,现在在 mac os 上一样也能做(当然,我不会认为换成了 mac os 就会效果提升) 虽然有以上的这些特点,但其实我一直想要将闲置的 mac mini 用作个人的服务器,用于替换我的 x86 笔记本 原因在于 性能. 家里的 x86 笔记本硬件上不如 mac mini 能耗. mac mini 是 arm 架构,它的能耗控制的更好 电池. 服务器需要长期开机运行,我一直想要拆卸 x86 笔记本上的电池.我经常不在家,我还是比较害怕电池鼓包着火的,但一直没有动手拆卸它 网口. x86 笔记本用的是 type-c 转 rj45 转接头,我一直怀疑这个接口限制了局域网的速度上限.不过我家的带宽是 300M,作为互联网使用来说,网络的没有瓶颈,方便的地方就在少一个转接头 既然有了充足的理由,那就开干 第一次尝试最初我尝试直接使用 mac os 作为服务器 我的主要使用场景还是容器,只要能安装 docker,一般也不需要在宿主机上做多少准备工作 但我发现 mac os 不适合作为服务器操作系统 来回想一下服务器的一些特性 长时间运行无异常 终端远程登录 第一个特性我不好说,有可能没问题 第二个特性有比较大的问题 mac os 启动之后需要本地登录之后才允许远程登录.如果没有登录一个本地账户,我怀疑 mac os 的 sshd 进程根本就不会运行 mac os 需要一个显示器,否则远程桌面也不行.mac os 的远程和 windows 还不一样,它是共享,需要一个主,远程的画面算是从.这个可以通过一些配件来欺骗 mac os,实践时可以不需要真的连接一个显示器,那样功耗太大了,不过我没有实践过 第一个问题我没有找到可行的解决方案 所以第一次尝试就搁置了 第二次尝试那么,如果在 mac mini 的硬件上安装 一个 linux 发行版,是否可行呢? 答案是,早就有人在实践了,而且有个 release 版本 现在如果你这样问 Gemini 2.5 pro 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104Q: 我用一台闲置的mac mini,它是 m2 芯片的.现在我想要为它安装一个 linux 操作系统,请问有哪些 linux 发行版推荐?A: 太棒了！M2 Mac Mini 是一台性能强劲、功耗极低的小主机，用来运行 Linux 非常合适。对于 Apple Silicon (M1/M2/M3 芯片) 的 Mac，情况比较特殊，你不能像在普通 x86 电脑上那样随意选择任何 Linux 发行版。因为苹果的硬件是自研的，需要专门的驱动程序。目前，在这个领域有一个绝对的核心项目，那就是 Asahi Linux。核心概念： Asahi Linux 项目的目标是逆向工程苹果芯片，并为主线 Linux 内核开发开源的、完全功能的驱动程序。它本身不是一个独立的发行版，而是一个提供底层支持（内核、驱动、引导程序）的项目。其他的 Linux 发行版可以基于 Asahi 的工作成果来适配苹果芯片。因此，你的选择基本上就是“集成了 Asahi 项目成果的 Linux 发行版”。推荐列表 (从最推荐到备选)1. Fedora Asahi Remix (⭐⭐⭐⭐⭐ - 首选推荐)这是 Asahi Linux 项目官方的旗舰发行版，也是目前最推荐、最成熟、开箱即用体验最好的选择。为什么推荐？官方合作： Asahi Linux 团队与 Fedora 项目紧密合作，这意味着你能最快获得最新的硬件驱动和系统更新，集成度最高。开箱即用： 安装完成后，Wi-Fi、蓝牙、USB、GPU 硬件加速（OpenGL 3.3）、扬声器、电源管理等基本都已正常工作。M2 Mac Mini 的硬件支持度非常好。桌面环境： 默认使用非常精致的 KDE Plasma 桌面，对 Wayland 支持良好，完美利用了 Mac 的 Retina 显示屏。你也可以选择安装 GNOME 或其他桌面环境。安装简单： 官方提供一个非常简单的命令行安装脚本，在 macOS 的终端里运行一行命令即可，它会自动处理分区、下载和安装，非常安全，不会破坏你现有的 macOS 系统。适合谁？几乎所有人，特别是希望获得一个稳定、美观、功能完整的桌面 Linux 体验的用户。Linux 新手和老手都适用。2. Asahi Arch Linux ARM (⭐⭐⭐ - 进阶选择)这是 Asahi Linux 项目最初的参考发行版，基于 Arch Linux ARM。为什么推荐？滚动更新： 如果你喜欢永远使用最新的软件和内核，Arch Linux 的滚动更新模式非常适合你。高度可定制： Arch Linux 允许你从一个最小化的系统开始，完全按照自己的需求来构建整个系统和桌面环境。强大的社区和文档： Arch Wiki 是 Linux 世界里最详尽的文档库之一。需要注意什么？相比 Fedora Asahi Remix，它的配置过程更手动，需要你对 Linux 有一定的了解。更适合喜欢“折腾”、想要完全掌控系统的资深用户。适合谁？经验丰富的 Linux 用户，开发者，以及 Arch Linux 的爱好者。3. 其他选择 (需要更多手动配置)虽然官方主推以上两种，但你也可以在其他 ARM64 发行版上手动安装 Asahi 的内核和脚本，但这通常比较复杂，不推荐新手尝试。例如：Ubuntu Server / Debian (ARM64): 你可以安装一个基础的 ARM64 服务器系统，然后手动添加 Asahi 的软件源并安装其内核和支持包。这适合想把 Mac Mini 当作一个纯粹的、无图形界面的服务器（比如跑 Docker、做 Web 服务器）的用户。在你开始之前，需要了解的关键点硬件支持状态： 尽管 Asahi 项目进展神速，但仍有部分硬件功能尚未完美支持。对于 M2 Mac Mini 来说，绝大部分功能都已工作，但你需要注意：Thunderbolt / USB4: 端口可以作为普通的 USB-C 3.2 端口使用，但完整的 Thunderbolt 功能仍在开发中。HDMI 2.1: 部分高级功能（如高刷新率、VRR）可能还在完善中。内置扬声器： 已经有了重大突破并基本可用，但音质优化仍在进行中。麦克风： 目前还不支持。你可以随时在官方网站查看最新的硬件支持列表：Asahi Linux Wiki - Feature Support安装过程：双系统： Asahi 的标准安装过程是双系统。它会智能地缩减你当前的 macOS 分区，然后创建新的分区来安装 Linux。你的 macOS 系统和数据都是安全的。需要 macOS： 安装必须从最新的 macOS 系统下启动。你需要保持你的 Mac Mini 系统是最新版本。引导切换： 安装后，你可以在开机时按住电源键，进入启动选项菜单，自由选择启动到 macOS 还是 Asahi Linux。总结与最终建议对于你的 M2 Mac Mini，我强烈推荐你首先尝试 Fedora Asahi Remix。它的安装过程最简单，体验最完整，可以让你以最快的速度在 M2 Mac Mini 上拥有一个功能齐全的 Linux 桌面环境。它几乎就是为你的设备量身定做的。如何开始？将你的 Mac Mini 更新到最新的 macOS 版本。打开 macOS 的“终端” (Terminal) 应用。复制并粘贴以下命令并回车，然后按照屏幕提示操作即可：codeBashdownloadcontent_copyexpand_lesscurl https://fedora-asahi-remix.org/install | sh安装程序会引导你完成所有步骤，包括分区大小选择等，整个过程非常自动化。祝你玩得开心！ 在这一次回答中, 大模型给出的前两个选项都是 asahi 操作系统 其中, Asahi Arch Linux ARM 是早期的版本,现在比较推荐 Fedora Asahi Remix 考虑到 Fedora Asahi Remix 专门针对 mac 的硬件做了优化,那就没有其它的道理选择其它的发行版,也没其他选择了 再上官网上一看,也是明确支持 mac mini m2 的 再看看官方这么吹它的 Fedora Asahi Remix With Fedora’s excellent 64-bit ARM support and mature development process, you can expect a solid and high-quality experience without any unwanted surprises. 翻译: 凭借 Fedora 出色的 64 位 ARM 支持和成熟的开发流程，您可以期待获得稳定、高品质的体验，而不会有任何意外问题。 Fedora Asahi Remix ❤️ KDE Plasma Whether you’re a KDE enthusiast or a GNOME lover, Fedora Asahi Remix comes right out of the box with a 100% Wayland environment, bringing you the newest desktop and display server technologies, which are a perfect match for Apple hardware. 翻译: 无论您是 KDE 爱好者还是 GNOME 爱好者，Fedora Asahi Remix 都开箱即为您提供了一个 100% 的 Wayland 环境。其最新的桌面和显示服务器技术，与苹果硬件堪称绝配。 The best Linux laptop audio you’ve ever heard 翻译: 您所听过的最佳 Linux 笔记本音质。 对于我的用途来说 音质无所谓 fedora 并不是一个很稳定的操作系统,你了解的话,就知道 fedora 是 centos stream 的上游,但 centos stream 都已经很少企业会使用了 但咱不是没得挑吗,硬着头皮上喽 安装命令也就一行命令 1curl https://alx.sh | sh 开整 成果1234567891011121314151617 /:-------------:\\ wuhunyu@wuhunyu :-------------------:: --------------- :-----------/shhOHbmp---:\\ OS: Fedora Linux Asahi Remix 42 (Forty Two [Adams]) aarch64 /-----------omMMMNNNMMD ---: Host: Apple Mac mini (M2, 2023) :-----------sMMMMNMNMP. ---: Kernel: 6.16.8-400.asahi.fc42.aarch64+16k :-----------:MMMdP------- ---\\ Uptime: 1 hour, 10 mins ,------------:MMMd-------- ---: Packages: 2070 (rpm), 24 (brew) :------------:MMMd------- .---: Shell: bash 5.2.37 :---- oNMMMMMMMMMNho .----: Resolution: 3840x2160 :-- .+shhhMMMmhhy++ .------/ Terminal: /dev/pts/0 :- -------:MMMd--------------: CPU: (8) @ 2.424GHz :- --------/MMMd-------------; Memory: 1559MiB / 15705MiB :- ------/hMMMy------------::-- :dMNdhhdNMMNo------------; :---:sdNMMMMNds:------------: :------:://:-------------:::---------------------:// 使用体验你别说,这个 linux 发行版还真是让我大开眼界了 这个系统第一个我用着用着不是因为资源不足直接崩溃死机的 除此之外,还出现过一次找不到磁盘的错误.重新安装之后就好了 arm64 架构带来的问题是,要开始考虑软件适配问题了 好处是 性能强劲,还基本听不到风扇转,还是不错的 后手上面说到新系统不太稳定 那么,旧的 x86 笔记本还是得运行着,需要一些时间来观察新系统的稳定性","link":"/linux/2025/10/04/mac-mini/index.html"},{"title":"迁移 minio 到 rustfs","text":"前言minio 在今年 5 月份时, 控制台 ui 出现了一个很大的变化 – 删减了绝大多数 ui 特性 现状下图是社区版前后的对比 可以看到 Access Keys 和 Policies 的菜单都没了 讨论在 github 上,有讨论这个问题的相关 issue 官方给出的答复如下图 我的看法minio 在 oss 的开源圈中小有名气,我自己的图床也是基于 minio 搭建的 minio 项目的开源协议是 AGPL-3.0. 这个协议最大限制是它的传染性,就是说如果你修改了源代码,那么你必须要公开你修改之后的版本 就协议来说,对于我这种没有修改需求的用户而言,没有什么影响 这次的主要问题在于,我一个社区用户不能再通过控制台 ui 来管理访问 key,也不能通过控制台 ui 来管理访问策略 选择如果想要在 minio 的控制台 ui 上用上以上的特性,有两个选择 回退 minio 版本到 RELEASE.2025-04-22T22-12-26Z, 这是拥有完整 ui 特性的最后版本 升级到商用版本 我最近刷到的有关 rustfs 的广告真的有点多,那不然就试试呗 rustfs 现在最新的版本是 1.0.0-alpha.66,都还没发布正式版本,也不知道是哪些人在吹 不过 rustfs 是一个国产项目,文档有中文版本,对国人还是很友好的 开搞部署 rustfs直接部署一个新环境肯定没什么问题,但我之前使用的是 minio, 数据也要迁移过来才行 另外就是,我在 minio 中配置的是多磁盘,那肯定也是想要能直接换到 rustfs 就能用的 我的环境虽然说是多磁盘,但实际上是伪多磁盘,实际磁盘也只有一块 之前的部署使用的是 docker compose 配置 123456789101112131415161718192021222324252627282930313233343536373839services: minio: image: minio/minio:RELEASE.2025-04-22T22-12-26Z container_name: minio restart: always networks: - minio environment: MINIO_ROOT_USER: xxx MINIO_ROOT_PASSWORD: xxx volumes: - ~/applications/minio/data/disk1:/mnt/data/disk1 - ~/applications/minio/data/disk2:/mnt/data/disk2 - ~/applications/minio/data/disk3:/mnt/data/disk3 - ~/applications/minio/data/disk4:/mnt/data/disk4 command: | server /mnt/data/disk{1...4} --console-address &quot;:9001&quot; nginx: image: nginx:1.29.1 container_name: route-minio restart: always depends_on: - minio networks: - minio volumes: - ~/applications/minio/nginx/nginx.conf:/etc/nginx/nginx.conf:ro - ~/applications/cert/home.wuhunyu.top:/etc/cert:ro - ~/applications/minio/nginx/logs:/opt/nginx/logs ports: - 9000:9000 - 9001:9001networks: minio: driver: bridge 简单修改一下 docker compose 的配置,换成 rustfs 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647services: rustfs: image: rustfs/rustfs:1.0.0-alpha.66 container_name: rustfs restart: unless-stopped networks: - rustfs environment: TZ: Asia/Shanghai RUSTFS_ACCESS_KEY: xxx RUSTFS_SECRET_KEY: xxx RUST_LOG: error RUSTFS_OBS_LOG_DIRECTORY: &quot;/var/logs/rustfs/&quot; RUSTFS_ADDRESS: &quot;:9000&quot; RUSTFS_CONSOLE_ENABLE: true volumes: - ~/applications/rustfs/data/disk1:/mnt/data/disk1 - ~/applications/rustfs/data/disk2:/mnt/data/disk2 - ~/applications/rustfs/data/disk3:/mnt/data/disk3 - ~/applications/rustfs/data/disk4:/mnt/data/disk4 - ~/applications/rustfs/logs:/var/logs/rustfs - /etc/localtime:/etc/localtime:ro command: | /mnt/data/disk{1...4} nginx: image: nginx:1.29.1 container_name: route-rustfs restart: unless-stopped environment: TZ: Asia/Shanghai depends_on: - rustfs networks: - rustfs volumes: - ~/applications/rustfs/nginx/nginx.conf:/etc/nginx/nginx.conf:ro - ~/applications/cert/home.wuhunyu.top:/etc/cert:ro - ~/applications/rustfs/nginx/logs:/opt/nginx/logs - /etc/localtime:/etc/localtime:ro ports: - 9000:9000 - 9001:9001networks: rustfs: driver: bridge 启动, 登录控制台 ui 数据迁移部署的任务是完成了,但之前在 minio 的数据也需要迁移过来 如果要一个个地走 S3 的标准 API 那就有点麻烦了.看官方文档也支持 mc 工具,那不妨拿来用用 mc 工具是 minio 官方开发的一个命令行工具,它的功能要比控制台 ui 上的特性更丰富 但对于一般用户来说用的还是比较少 要用 mc 工具,那就得把两个 oss 服务都跑起来才行. 简单地修改修改一下原来的 minio 服务的端口就行了,避免端口冲突 比方说,我修改 minio 的 api 端口为 9002, 控制台 ui 端口为 9003 真实操作之前,还等登录控制台 ui 生成一个 Access Keys, minio 和 rustfs 都需要 12345678# 配置 minio$ mc alias set minio https://[访问地址]:9002 [Access Key] [Secret Key]# 配置 rustfs$ mc alias set rustfs https://[访问地址]:9000 [Access Key] [Secret Key]# 开始全量迁移$ mc mirror --overwrite --remove --preserve minio/ rustfs/ 等结束就好了 最后我的 images bucket 是需要匿名可访问的 1$ mc anonymous set download rustfs/images 注意事项超时如果像我一样配置了 nginx 反向代理,迁移过程中可能会因为迁移的文件太大而超时 这时候需要修改 nginx 配置,调大请求体,调大超时时间 1234proxy_send_timeout 3000s;proxy_read_timeout 3000s;proxy_connect_timeout 3000s;client_max_body_size 100G;","link":"/oss/2025/10/31/migration-minio-to-rustfs/index.html"},{"title":"记第一次半程马拉松","text":"这篇博客比较偏向于随想的碎碎念, 没有主题 比赛基本信息我参加的第一次半程马拉松是 2025 年的惠州马拉松, 具体的比赛日期是 2025年12月28日, 具体的开赛时间是 07:30 赛衣我是第一次参加马拉松比赛, 理所当然地被分配到了最后的一个分区. 在惠州马拉松中, 我是在 D 区 特意选择一个 1024 的号码, 只不过是倒过来的 1024 完赛证书 赛前计划打算 05:30 起, 洗漱后去附近的公交车接驳车到比赛现场 我距离最近的公交接驳点挺近的, 前一天晚上我走了一遍, 从出门到目标地点大约 10 分钟的路程 洗漱完毕 赛前物资准备 能量胶 x 3 盐丸 x 1 氨糖涂抹膏(惠马送的) x 1. 这个我是在开赛的半个小时前使用的, 不想打广告, 但真有点用 少量饮水 纸巾 x 4. 纸巾是因为我跑步时不时会流鼻涕(可能和气温, 湿度有关系, 但我本身也有鼻炎, 带上几张以防万一) 一次性雨衣 提前使用 能量胶 x 1 盐丸 x 1 氨糖涂抹膏 x 1 提前少量饮水. 这是我个人的经验, 喝水过多导致尿意, 或者提前岔气. 我个人的做法是, 赛中非必要不喝水, 不科学但我这次比赛我很想跑个好成绩, 尽量避免取水喝水这个动作 一次性雨衣 具体也可以通过食用香蕉来代替能量胶, 本来我也想买点香蕉的, 但前一天晚上玩嗨了, 忘记这回事了 盐丸是用来补充电解质的, 需要陪和白水一起食用, 切勿混合运动饮料一起食用 雨衣可以在寄存行李的时候穿上, 可以减少失温, 在赛场上, 我可以看到了使用锡箔纸做”雨衣”的, 锡箔纸的保温效果更佳, 但成本没那么好看, 在一个就是外形不如五颜六色的雨衣 提前上厕所重要事件 不要等到了比赛场地才想着上厕所, 无论是大便还是小便 从我观察的现象来看, 男厕的小便池姑且都是爆满状态, 不敢想像女厕那边是什么状态 提前在酒店如厕完毕, 之后尽量只少量饮水 存包一般马拉松比赛的参赛人数都是比较多的, 所以现场虽然有很多引导牌和工作人员, 但还是挺难找对地方 起初, 我以为是随便找个地方放就行, 但看到存包的地方还有号码牌的时候, 我就知道我放错地方了 惠州马拉松 2025 年这届比赛参赛人数据官方介绍有 13,000 名运动员, 这里面包含了全马和半马运动员 第一次参赛, 真给我整懵逼了, 我所在的 D 区 离公交接驳车下车的位置很远, 一度让我产生了是不是错过存包位置的想法 存包之后, 工作人员会在你的号码牌上用水笔做标记 之后, 就是可以入场准备了 入场入场会检查是否正确佩戴纸质手环 纸质手环有什么作用我也不清楚, 但领取参赛包的时候被交代一定要在比赛时戴上它 我所在的 D 区 前排, 每个区域内, 可以自由选择靠前还是靠后, 靠前可以提前开始比赛, 靠后枪响成绩会更差一些, 本质没什么区别, 开跑之后不久人与人之间的间隙还是有的, 不太会出现特别影响速度的情况. 你要是跑得快, 还想要争取一个好成绩, 也可以尽量往前靠 准赛前准备我大概是在 07:10 左右入场的, 尽量往 D 区 的前排靠 比赛开始前, 可以听到主持人在不停地预热气氛, 但位于 D 区 其实什么都看不见, 只能听见声音. 还有偶尔飞过来的无人机 赛中等到 07:30, 比赛真是开始, 但由于我在 D 区, 前排的人群没有什么动静, 估计不同的区域也是按批次放行通过比赛拱门 赛场垃圾这个问题我在参加比赛之前就刷到过相关的视频, 评论区偶尔能看到有几个道德感比较高的网友批评这种垃圾乱丢的行为 我从我个人的角度上来讲讲我看到的样子 垃圾比较多的位置出现在比赛开始的位置和每个补给点, 反而在终点, 我没有看到什么垃圾 比赛开始的位置开始位置的垃圾主要是能量胶, 盐丸的包装袋, 以及一次性雨衣 其中, 我认为比较影响其它参赛选手的垃圾是 一次性雨衣, 一次性雨衣很容易散开, 踩到容易绊住脚 比赛正式开始前, 我看到有部分参赛选手会将雨衣揉成一团之后, 丢到赛场的两侧 赛场的两侧是没有参赛人员的, 属于非赛事规划路段, 如果没有被风吹到赛场, 是不会影响其他参赛选手的 至于赛场的垃圾不应该乱丢这个问题, 我没有读各个马拉松比赛的推荐规范中, 有没有推荐参赛运动员如何处理这些垃圾, 但在一个万人级别的比赛项目中, 想要引导全部的参赛选手应该在哪里丢垃圾是一件很困难的事情, 更何况比赛的路线是固定的, 干垃圾收集起来相对还是更加省力的 对于想要在比赛中赢得好成绩的选手来说, 能争取一秒也是宝贵的 我个人的看法是, 允许在不影响其他参赛选手的情况丢垃圾, 但一般应该丢在比较明显的位置, 降低后勤人员清理的难度 明显的位置, 可以是比赛开始前的场地两侧, 这里会有大量的参赛选手丢弃雨衣, 能量胶等垃圾, 垃圾比较集中的话, 清理起来也会省力不少 补给点补给点分两种, 一种是官方提供的补给点, 另一种是私补(观众在赛道旁提供的补给) 这次惠州马拉松提供的补给有什么我没太关注 赛前怕喝水容易侧腹痛, 给自己的目标是非必要不喝水, 即便喝水也只能抿一口 这种做法不可取, 但每个人有自己的策略, 喝水属于赛场中人群比较集中的地方, 这意味着为了喝水冲进这个区域, 你需要降低配速 降低配速可能也争取不了多少秒, 但不同的人对成绩有不同的执念 但每个有补给的地方, 必然会出现很多乱丢的纸杯, 湿水海绵, 能量胶包装袋, 以及没有喝完的水 补给点附近有专门的垃圾回收区, 但纸杯乱丢的现象还是挺严重的 关于纸杯乱丢的问题, 我持中立态度 就像是很多国家在发展过程中, 都伴随着破坏环境, 发展过程中还要保护环境对很多企业来说意味着高成本 这并不是意味着我提倡不爱护环境, 而是在其中取舍, 如果像德国弃核一样一味环保, 就变成了废寝忘食, 那马拉松还是为了追求人类极限的目的而存在吗 另外说说我担心的地方没喝完的水倒在地上 普通的矿泉水 / 纯净水没什么, 但部分运动饮料中包含部分糖浆, 这部分饮料晒地上可能会有一点影响 拉拉队这届惠州马拉松虽然也是A类赛事, 但知名度很一般, 拉拉队除了围观的群众, 多数还是上了年纪的阿姨 说阿姨不知道会不会很没有礼貌, 但确实没有看到很年轻的 哦, 对了. 除了阿姨们, 也还有小孩子合唱团 路人也是很热情的, 我还听见了小孩子喊加油, “哥哥加油, 姐姐加油” 赛道 惠马的赛道前半段基本是下坡, 很少上坡路段, 我以为全程都是这种赛道, 心里还想着是不是能破二 是的, 后半段教我做人了. 后半段很多段路程是间歇性地上坡和下坡, 真是搞死人 赛道路过了惠州西湖, 此外一些路边的商家也有自发为比赛喊加油的 还有一个 cos 八戒的 我没认出来是八戒, 听其他参赛选手喊的 赛道觉大部分路段都是平整的路线, 部分路过商业街的路段有些小的坑洼, 需要注意⚠️ 配速安排原本, 我想要在最初的 5 公里以 6 分配的速度缓步前行的, 但我使用的 keep 只设置了目标里程是半马, 并没有设置目标配速, 导致 keep 只会在每公里达成的时候报平均的配速, 所以我是在抓瞎跑 再加上, keep 在我播放音乐的时候播报的声音, 我经常是刻意去听也听不清 最后只能稳住心率在 150 ~ 160 之间去跑 不过话说回来, 虽然我的起步速度确实太快了, 以至于我在 4 公里处, 就开始有点乏力了, 但前半程有很多下坡路段, 跑起来心率还是稳在了 160 以下 --- title: 2025 届惠州马拉松分段用时 --- graph TD start((枪响)) --> five[5公里用时 00:28:17] five -- \"5-10公里用时 00:26:12\" --> ten[10公里用时 00:54:29] ten -- \"10-15公里用时 00:26:58\" --> fifteen[15公里用时 01:21:27] fifteen -- \"15-20公里用时 00:30:28\" --> twenty[20公里用时 01:51:55] twenty -- \"净计时成绩: 01:58:10\" --> finish((完赛)) 令人意外的是, 速度最快的分段竟然是 10 - 15 公里处 比赛的平均配速是 05’36’’ 说实话这个配速我是很意外的, 我平时的最好成绩是 2 小时 09 分, 也就是 06’07’’ 真实比赛我是悲观态度, 期望能在 2 小时 20 分 以内完赛 之所以是 2 小时 20 分 是因为惠马的分区是按配速来的, 在 2 小时 20 分 以内, 我就可以进入 C 区 每个马拉松比赛有不同的分区标准, 甚至可以同一个城市不同举办年份的分区标准也会有区别, 并非简单地按照统一的配速标准来分区 比如, 惠州马拉松只有 ABCD 四个分区, 但像是北京马拉松, 无锡马拉松这种更知名的马拉松比赛, 它们的分区到了 E 区, 甚至无锡马拉松还有 S 区 但破二是很多人的目标, 也是一道坎, 破二意味着我需要保证平均配速不慢于 05’41’’ 这对与我来说是一个不小的挑战 此前, 我经常跑步的距离是 8 公里, 配速快的时候可以达到 5’20’’, 但 8 公里 和 半马差别挺大的, 跑 8 公里我不需要任何补给, 保持心率在 170 甚至 180 也能完赛 但半马维持这个速率的代价肯定会在比赛的后半程体现出来的, 很容易心肺维持不住, 提前撞墙 总之, 能在第一次半程马拉松中就获取破二的成绩, 我是很满足的 跑崩在正式参加这次马拉松之前, 我有过两次半马的经历, 都是在公园里面绕圈, 一圈 8 公里, 需要跑两圈半 我的策略是每 8 公里补给一次电解质水, 但两次的半马经历让我确信不能多喝水, 不然在第一个 8 公里之后, 两边的侧腹必然会痛 后来我查了一下, 侧腹痛的俗称是岔气, 是一个挺正常的现象 常发生在 膈肌痉挛（最常见原因） 内脏震动牵拉 腹膜摩擦 核心力量不足 在 14 公里处尤为明显, 此前, 在 4 公里处有一点点隐约的痛疼, 我通过收腹缓解了这个症状 但在 14 公里处, 我需要用手按压辅助来缓解侧腹的痛疼 可以断定从这个时候我就已经跑崩了 10 公里处我补充了第一支能量胶 14 公里处, 因为侧腹痛, 我开始补充第二支能量胶试图缓解症状, 也是在这个时候我喝了全程的第一次水 补充能量胶并没有缓解我的痛疼, 虽然知道能量胶需要一点时间来被吸收 但事实证明, 后半程, 尤其是 18 公里之后, 我不得不停下来走一段. 一方面是侧腹痛, 另一方面是抬腿都觉得费劲 惠州马拉松全程我走了三段, 说起来没面子, 身边路过的无论是男是女, 都没看见有停下来走的 包括我和其他刚开始接触跑步的朋友, 也是说, 不能停下来, 慢跑都比停下来好 但到自己身上, 身临其境了, 真的是那段时间, 想用手来代替脚跑步 不过拇指也因为按压侧腹脱力了, 赛后的一天内还没有完全恢复过来 最后两公里, 开始出现了干呕的症状, 这个症状我熟悉, 在网上看到过, 属于是正常的现象, 并非健康问题, 但总归还是有些难受的 硬撑着身体, 尽量在摄像头面前提点速, 坚持到了终点 到达终点后, 呕了一口能量胶出来, 估计 14 公里处补充的能量胶没吸收多少 如果在科学一点规划能量补给, 或许能再减慢撞墙的时间, 可以取得更好一些的成绩 说这句话倒不是后悔, 人总需要一点能提升的空间来让自己进步不是 赛后 惠州马拉松的终点在体育馆里面, 我在里面休息了大约二十分钟 同时在现场也见证了2025年的惠州马拉松全马冠军冲线, 时间是 2 小时 18 分, 大约比我慢了 20 分钟, 但比我多跑了一倍的距离, 确实敬佩 赛后的拱门后, 挺多选手在做简单的拉伸 终点处也提供了免费的百岁山矿泉水可以饮用 流水席出了体育馆, 除了完赛奖牌以外, 还有一个完赛包, 里面塞了一根香蕉 完赛奖牌可以从中间打开, 也就是奖牌是双层的 我个人不太喜欢这种设计 领完完赛包, 外面还有很多赛事赞助方的补给可以蹭一蹭 为了冲成绩, 赛道两边的补给我不敢接, 完赛之后的流水席那肯定是要品尝品尝的 我看见有人吃了牛肉丸子, 圆圆的像是绿豆饼的饼, 绿豆汤, 姜茶, 酸梅汤 这个姜汤让我眼前一亮, 本来想要问多少钱一包的, 但社恐没敢问, 拍了一个疑似她们商店的二维码 公交接驳车吃饱喝足, 就准备撤离了 还是坐官方的公交接驳车, 只不过回去的路线变了, 导致大半个小时才到酒店附近 路上看到了惠州的双鹅亲吻大桥 不过在车上, 还是了解到了一点点其他跑者的信息 跑团刚开始在公交车上, 我还很纳闷, 为什么有两个女跑总是和一个男跑说话 我身边很少有人爱好跑步, 更何况是半程马拉松, 我一直认为跑半程马拉松对于大多数人的身体来说是一个负担大于健康的运动 健康的运动不应该让身体透支 一直以为他们是夫妻, 但第二个女跑的出现打消我的猜想 后面询问之后, 才知道他们是从长沙过来的, 都是一个跑团的跑者 这下就明白了, 我说我都是独狼, 一直是一个人跑 想想有其他同伴一起比赛, 会是一件很有幸福感的事情 其他惠州西湖夜景","link":"/marathon/2025/12/29/huizhou/index.html"}],"tags":[{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"入门教程","slug":"入门教程","link":"/tags/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"},{"name":"GitHub Actions","slug":"GitHub-Actions","link":"/tags/GitHub-Actions/"},{"name":"MyBatis","slug":"MyBatis","link":"/tags/MyBatis/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"JetBrains 全家桶","slug":"JetBrains-全家桶","link":"/tags/JetBrains-%E5%85%A8%E5%AE%B6%E6%A1%B6/"},{"name":"IntelliJ IDEA","slug":"IntelliJ-IDEA","link":"/tags/IntelliJ-IDEA/"},{"name":"WebStorm","slug":"WebStorm","link":"/tags/WebStorm/"},{"name":"GoLand","slug":"GoLand","link":"/tags/GoLand/"},{"name":"DataGrip","slug":"DataGrip","link":"/tags/DataGrip/"},{"name":"Docker Hub","slug":"Docker-Hub","link":"/tags/Docker-Hub/"},{"name":"自组","slug":"自组","link":"/tags/%E8%87%AA%E7%BB%84/"},{"name":"组装","slug":"组装","link":"/tags/%E7%BB%84%E8%A3%85/"},{"name":"消费","slug":"消费","link":"/tags/%E6%B6%88%E8%B4%B9/"},{"name":"个人总结","slug":"个人总结","link":"/tags/%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/"},{"name":"音乐流媒体","slug":"音乐流媒体","link":"/tags/%E9%9F%B3%E4%B9%90%E6%B5%81%E5%AA%92%E4%BD%93/"},{"name":"自建","slug":"自建","link":"/tags/%E8%87%AA%E5%BB%BA/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"event","slug":"event","link":"/tags/event/"},{"name":"珂朵莉树","slug":"珂朵莉树","link":"/tags/%E7%8F%82%E6%9C%B5%E8%8E%89%E6%A0%91/"},{"name":"老司机树","slug":"老司机树","link":"/tags/%E8%80%81%E5%8F%B8%E6%9C%BA%E6%A0%91/"},{"name":"acme.sh","slug":"acme-sh","link":"/tags/acme-sh/"},{"name":"相亲","slug":"相亲","link":"/tags/%E7%9B%B8%E4%BA%B2/"},{"name":"VisualVM","slug":"VisualVM","link":"/tags/VisualVM/"},{"name":"一叶智能","slug":"一叶智能","link":"/tags/%E4%B8%80%E5%8F%B6%E6%99%BA%E8%83%BD/"},{"name":"技术面","slug":"技术面","link":"/tags/%E6%8A%80%E6%9C%AF%E9%9D%A2/"},{"name":"一面","slug":"一面","link":"/tags/%E4%B8%80%E9%9D%A2/"},{"name":"深圳同行者","slug":"深圳同行者","link":"/tags/%E6%B7%B1%E5%9C%B3%E5%90%8C%E8%A1%8C%E8%80%85/"},{"name":"行云绽放","slug":"行云绽放","link":"/tags/%E8%A1%8C%E4%BA%91%E7%BB%BD%E6%94%BE/"},{"name":"一临云科技","slug":"一临云科技","link":"/tags/%E4%B8%80%E4%B8%B4%E4%BA%91%E7%A7%91%E6%8A%80/"},{"name":"易世通达(跨境电商)","slug":"易世通达-跨境电商","link":"/tags/%E6%98%93%E4%B8%96%E9%80%9A%E8%BE%BE-%E8%B7%A8%E5%A2%83%E7%94%B5%E5%95%86/"},{"name":"江西燃点","slug":"江西燃点","link":"/tags/%E6%B1%9F%E8%A5%BF%E7%87%83%E7%82%B9/"},{"name":"卓讯信息技术","slug":"卓讯信息技术","link":"/tags/%E5%8D%93%E8%AE%AF%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF/"},{"name":"langchain4j","slug":"langchain4j","link":"/tags/langchain4j/"},{"name":"mcp","slug":"mcp","link":"/tags/mcp/"},{"name":"mcp-client","slug":"mcp-client","link":"/tags/mcp-client/"},{"name":"rag","slug":"rag","link":"/tags/rag/"},{"name":"mermaid","slug":"mermaid","link":"/tags/mermaid/"},{"name":"jackson","slug":"jackson","link":"/tags/jackson/"},{"name":"mixin","slug":"mixin","link":"/tags/mixin/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"mac mini","slug":"mac-mini","link":"/tags/mac-mini/"},{"name":"asahi","slug":"asahi","link":"/tags/asahi/"},{"name":"minio","slug":"minio","link":"/tags/minio/"},{"name":"rustfs","slug":"rustfs","link":"/tags/rustfs/"},{"name":"惠州马拉松","slug":"惠州马拉松","link":"/tags/%E6%83%A0%E5%B7%9E%E9%A9%AC%E6%8B%89%E6%9D%BE/"}],"categories":[{"name":"mongodb","slug":"mongodb","link":"/categories/mongodb/"},{"name":"GitHub Actions","slug":"GitHub-Actions","link":"/categories/GitHub-Actions/"},{"name":"MyBatis","slug":"MyBatis","link":"/categories/MyBatis/"},{"name":"破解","slug":"破解","link":"/categories/%E7%A0%B4%E8%A7%A3/"},{"name":"Docker Hub","slug":"Docker-Hub","link":"/categories/Docker-Hub/"},{"name":"折叠自行车","slug":"折叠自行车","link":"/categories/%E6%8A%98%E5%8F%A0%E8%87%AA%E8%A1%8C%E8%BD%A6/"},{"name":"碎碎念","slug":"碎碎念","link":"/categories/%E7%A2%8E%E7%A2%8E%E5%BF%B5/"},{"name":"spring event","slug":"spring-event","link":"/categories/spring-event/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"domain","slug":"domain","link":"/categories/domain/"},{"name":"相亲","slug":"相亲","link":"/categories/%E7%9B%B8%E4%BA%B2/"},{"name":"jvm","slug":"jvm","link":"/categories/jvm/"},{"name":"面经","slug":"面经","link":"/categories/%E9%9D%A2%E7%BB%8F/"},{"name":"spring transaction","slug":"spring-event/spring-transaction","link":"/categories/spring-event/spring-transaction/"},{"name":"ssl cert","slug":"domain/ssl-cert","link":"/categories/domain/ssl-cert/"},{"name":"ai","slug":"ai","link":"/categories/ai/"},{"name":"chart","slug":"chart","link":"/categories/chart/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"MultipartFile","slug":"MultipartFile","link":"/categories/MultipartFile/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"oss","slug":"oss","link":"/categories/oss/"},{"name":"马拉松","slug":"马拉松","link":"/categories/%E9%A9%AC%E6%8B%89%E6%9D%BE/"},{"name":"流","slug":"MultipartFile/流","link":"/categories/MultipartFile/%E6%B5%81/"},{"name":"半程马拉松","slug":"马拉松/半程马拉松","link":"/categories/%E9%A9%AC%E6%8B%89%E6%9D%BE/%E5%8D%8A%E7%A8%8B%E9%A9%AC%E6%8B%89%E6%9D%BE/"}],"pages":[{"title":"wuhunyuの个人简历","text":"联系方式 手机：15687225042 (归属地为云南大理) Email：wuhunyu@gmail.com 微信号：wuhunyu3312 个人信息 龚志强 / 男 / 26岁 / 1997年12月出生 全日制统招本科 / 大理大学(2016.9 ~ 2020.7) / 计算机科学与技术 实际工作年限：4 年 期望职位：Java 工程师 力扣：https://leetcode.cn/u/wuhunyu 个人博客：https://wuhunyu.top 技能清单 熟练使用 MySQL，了解 sql 优化，开发过程中善于分析并优化 sql 熟练使用 Redis，了解 Redis 各个数据结构和持久化策略，并能根据不同的应用场景选择合适的数据结构 熟练使用 RocketMQ，善于使用 MQ 用于服务异步通信，延时队列等 熟练使用 Linux，Nginx，Docker 常用命令，能够编写简单的脚本并部署项目 熟练使用 Netty 框架，能够独立开发 RPC 框架 能够整合 SpringCloud Alibaba 微服务全家桶，能够独立开发微服务项目 阅读过 Java 常用集合的源码，在开发中会注意根据源码优化代码 了解 JUC 并发工具的使用，阅读过 CAS，AQS 源码 了解 JVM 运行原理，能够简单分析 dump 文件 工作经历江西绿萌科技控股有限公司（2022.10 ~ 2023.10） 绿萌云（2022.10 ~ 2023.10）Java 中级开发项目简介绿萌云是一个水果分选加工管理平台，涵盖了水果种植数据监控，采购入库，分选加工，仓储管理，销售出库，水果溯源等一整套水果管理流程 担任职责我在项目中主要负责水果种植期的数据监控模块和采购入库模块的开发工作 主要成果 水果的数据采集设备上报数据的频率大约为每 5 秒一次，我采用数据冷热分离的方式，保留最近一个月的实时数据以及最近三年的统计结果，过期的实时数据保存在冷库中，保证了监控数据的实时查询效率 监控数据异常或者采购入库成功时，需要在云平台实时通知管理者。由于平台是微服务架构集群的部署方式，我是通过 RocketMQ 广播的方式来解决WebSocket 分布式 Session 的问题 技术架构SpringCloudAlibaba MyBatis-Plus Redis Gateway Nacos Dubbo Quartz RocketMQ WebSocket 江西憶源多媒体科技有限公司（2021.2 ~ 2022.8） 校园资产管理平台（2022.2 ~ 2022.8）Java 初级开发项目简介对校园内的贵重资产进行管理记录，包含资产数字化管理，资产报表以及资产数据分析等模块 担任职责我负责该管理平台后端开发的领导人角色，主要负责的模块有资产数字化管理，资产报表 主要成果开发了一些方便的工具帮助快速开发，如：代码生成，Excel 导出工具，Cron 表达式转换工具，Java Bean 转 Map 工具等，得到了项目组其他同事的认可 技术架构SpringCloud MyBatis-Plus Redis Gateway Eureka OpenFeign Quartz 智慧警务管理平台（2021.8 ~ 2022.1）Java 初级开发项目简介统一管理派出所内部有所有装备柜设备，包括装备状态监控，装备数据报表，警员管理等模块 担任职责我主要负责装备状态监控模块的开发 主要成果每个装备的状态信息不会主动上报，需要系统主动去获取。我采用定时更新+增量更新的方式来获取每个装备的状态信息，每 5 分钟轮询一次，以及在用主动查询主动更新。保证了装备状态数据的最终一致性 技术架构SpringCloud MyBatis-Plus Redis Gateway Eureka OpenFeign 随手拍（2021.2 ~ 2021.7）Java 初级开发项目简介居民不文明或隐患行为拍照上传的微信小程序 担任职责我主要负责微信小程序后台的授权登录以及举报奖励模块的开发 主要成果举报奖励模块对接了微信的公众服务 企业付款到零钱，加入了幂等，重试等机制保证可靠性，支付失败会有告警提示。小程序交付运行一年以来，除了部分未实名认证无法收款的微信用户外，未出现异常 技术架构SpringBoot MyBatis-Plus Redis 宇信鸿泰科技有限公司（2020.3 ~ 2020.12） 中国进出口网上银行（2020.7 ~ 2020.12）Java 初级开发项目简介 &amp; 担任职责我在此项目中负责了电子对账业务，用户电子账号管理，账单提交以及账单复核等模块的开发 主要成果最后在同事的帮助下成功通过线上测试并发布到线上 技术框架SSM 手机银行（2020.3 ~ 2020.6）Java 实习项目简介 &amp; 担任职责在此项目中我使用 MUI 框架开发网上银行移动端页面，包含用户登录与注册和网上银行转账业务两个模块 主要成果通过这两个模块的开发，快速学习了移动端 H5 的开发过程，为今后接手简单的前端开发任务打下了基础 技术框架MUI VUE 个人开源项目rpc 框架 简介项目分为服务端和客户端两个模块，服务端在项目启动时将自己注册到 Nacos 服务中，客户端读取配置在 Nacos 中的配置后读取 Nacos 注册中心中服务端各个实例的访问地址和端口等配置信息，从而远程调用服务端的实例 解决问题了解 Dubbo，gRPC 底层通信原理，拥有独立开发 rpc 框架的能力 技术栈Nacos Netty 分布式序列ID 源码地址本体项目地址：https://github.com/wuhunyu/rpc.git demo项目地址：https://github.com/wuhunyu/rpc-demo.git 代码生成 简介这是一个在线代码生成器，核心使用 druid 框架和 freemarker 模板引擎。其中使用 druid 解析 MySQL 的 DDL 语句，使用 freemarker 生成模板，也就是在线解析 DDL 语句，并根据解析结果生成 Java 模板代码的一款 web 应用 解决问题通过 DDL 语句快速生成可用的 Java 后端模板代码 技术栈SpringBoot druid FreeMarker 源码地址https://github.com/wuhunyu/code-gen-by-ddl.git 演示地址https://wuhunyu.top/code-gen 证书软考中级软件设计师 云南省蓝桥杯C/C++程序设计大赛大学B组三等奖 英语(CET-4) 计算机等级考试四级-数据库工程师 个人总结擅长 Java 开发，对算法有浓厚兴趣，能够独立解决力扣简单，中等难度的算法题 乐于学习新技术，并应用到实际的开发中来 闲暇之余，有阅读源码的习惯 喜欢和同事讨论各种技术问题 致谢感谢您花时间阅读我的简历，期待能有机会和您共事","link":"/resume.html"}]}